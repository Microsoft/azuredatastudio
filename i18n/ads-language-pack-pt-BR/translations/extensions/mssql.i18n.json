{
	"": [
		"--------------------------------------------------------------------------------------------",
		"Copyright (c) Microsoft Corporation. All rights reserved.",
		"Licensed under the Source EULA. See License.txt in the project root for license information.",
		"--------------------------------------------------------------------------------------------",
		"Do not edit this file. It is machine generated."
	],
	"version": "1.0.0",
	"contents": {
		"package": {
			"json.schemas.desc": "Esquemas associados a arquivos JSON no projeto atual",
			"json.schemas.url.desc": "Uma URL para um esquema ou um caminho relativo a um esquema no diretório atual",
			"json.schemas.fileMatch.desc": "Uma matriz de padrões de arquivos para fazer correspondência ao resolver arquivos JSON para esquemas.",
			"json.schemas.fileMatch.item.desc": "Um padrão de arquivos que pode conter '*' para fazer a correspondência ao resolver arquivos JSON para esquemas.",
			"json.schemas.schema.desc": "A definição de esquema para a URL fornecida. O esquema precisa ser fornecido apenas para evitar acessos à URL do esquema.",
			"json.format.enable.desc": "Habilitar/desabilitar o formatador JSON padrão (requer reinicialização)",
			"mssqlCluster.uploadFiles": "Carregar arquivos",
			"mssqlCluster.mkdir": "Novo diretório",
			"mssqlCluster.deleteFiles": "Excluir",
			"mssqlCluster.previewFile": "Visualizar",
			"mssqlCluster.saveFile": "Salvar",
			"mssqlCluster.copyPath": "Copiar Caminho",
			"mssqlCluster.manageAccess": "Gerenciar Acesso",
			"notebook.command.new": "Novo Notebook",
			"notebook.command.open": "Abrir o Notebook",
			"tab.bigDataClusterDescription": "Tarefas e informações sobre o cluster de Big Data do SQL Server",
			"title.bigDataCluster": "Cluster de Big Data do SQL Server",
			"title.submitSparkJob": "Enviar o Trabalho do Spark",
			"title.newSparkJob": "Novo Trabalho do Spark",
			"title.openSparkHistory": "Exibir o Histórico do Spark",
			"title.openYarnHistory": "Exibir o Histórico do Yarn",
			"title.tasks": "Tarefas",
			"title.installPackages": "Instalar Pacotes",
			"title.configurePython": "Configurar o Python para Notebooks",
			"title.searchServers": "Pesquisar: Servidores",
			"title.clearSearchServerResult": "Pesquisar: Limpar os Resultados do Search Server",
			"title.endpoints": "Pontos de Extremidade de Serviço",
			"title.books": "Notebooks",
			"title.showLogFile": "Mostrar o Arquivo de Log",
			"mssql.configuration.title": "Configuração do MSSQL",
			"mssql.query.displayBitAsNumber": "Colunas do tipo BIT devem ser exibidas como números (1 ou 0)? Se false, colunas do tipo BIT serão exibidas como 'true' ou 'false'",
			"mssql.format.alignColumnDefinitionsInColumns": "Definições de coluna devem ser alinhadas?",
			"mssql.format.datatypeCasing": "Tipos de dados devem ser formatados como letras MAIÚSCULAS, minúsculas ou nenhum (não formatado)?",
			"mssql.format.keywordCasing": "Palavras-chave devem ser formatadas como letras MAIÚSCULAS, minúsculas ou nenhum (não formatado)?",
			"mssql.format.placeCommasBeforeNextStatement": "vírgulas devem ser colocadas no início de cada instrução em uma lista? Por exemplo, ', minhacoluna2' em vez de no final, por exemplo, 'minhacoluna1,'?",
			"mssql.format.placeSelectStatementReferencesOnNewLine": "Referências a objetos em uma instrução select devem ser divididas em linhas separadas? Por exemplo, para 'SELECT C1, C2 FROM T1', em que C1 e C2 deverão estar em linhas separadas?",
			"mssql.logDebugInfo": "[Opcional] Registre a saída da depuração no console (Exibir -> Saída) e, em seguida, selecione o canal de saída apropriado no menu suspenso",
			"mssql.tracingLevel": "[Opcional] Registre o nível para serviços de back-end. O Azure Data Studio gera um nome de arquivo sempre que é iniciado e, quando o arquivo já existe, as entradas de logs são acrescentadas a esse arquivo. Para a limpeza de arquivos de log antigos, confira as configurações logRetentionMinutes e logFilesRemovalLimit. O tracingLevel padrão não registra uma grande quantidade de log. A alteração de detalhamento pode levar ao aumento dos requisitos de log e de espaço em disco para os logs. Erro inclui Crítico, Aviso inclui Erro, informações incluem Aviso e Detalhado inclui Informações",
			"mssql.logRetentionMinutes": "O número de minutos para reter os arquivos de log dos serviços de back-end. O padrão é uma semana.",
			"mssql.logFilesRemovalLimit": "Número máximo de arquivos antigos a serem removidos na inicialização com mssql.logRetentionMinutes expirado. Os arquivos que não forem limpos devido a essa limitação serão limpos na próxima vez em que o Azure Data Studio for iniciado.",
			"ignorePlatformWarning": "[Opcional] Não mostrar os avisos de plataforma sem suporte",
			"onprem.databaseProperties.recoveryModel": "Modo de Recuperação",
			"onprem.databaseProperties.lastBackupDate": "Último Backup de Banco de Dados",
			"onprem.databaseProperties.lastLogBackupDate": "Último Backup de Log",
			"onprem.databaseProperties.compatibilityLevel": "Nível de Compatibilidade",
			"onprem.databaseProperties.owner": "Proprietário",
			"onprem.serverProperties.serverVersion": "Versão",
			"onprem.serverProperties.serverEdition": "Edição",
			"onprem.serverProperties.machineName": "Nome do Computador",
			"onprem.serverProperties.osVersion": "Versão do Sistema Operacional",
			"cloud.databaseProperties.azureEdition": "Edição",
			"cloud.databaseProperties.serviceLevelObjective": "Tipo de Preço",
			"cloud.databaseProperties.compatibilityLevel": "Nível de Compatibilidade",
			"cloud.databaseProperties.owner": "Proprietário",
			"cloud.serverProperties.serverVersion": "Versão",
			"cloud.serverProperties.serverEdition": "Tipo",
			"mssql.provider.displayName": "Microsoft SQL Server",
			"mssql.connectionOptions.connectionName.displayName": "Nome (opcional)",
			"mssql.connectionOptions.connectionName.description": "Nome personalizado da conexão",
			"mssql.connectionOptions.serverName.displayName": "Servidor",
			"mssql.connectionOptions.serverName.description": "Nome da instância do SQL Server",
			"mssql.connectionOptions.databaseName.displayName": "Banco de dados",
			"mssql.connectionOptions.databaseName.description": "O nome do catálogo ou do banco de dados inicial na fonte de dados",
			"mssql.connectionOptions.authType.displayName": "Tipo de autenticação",
			"mssql.connectionOptions.authType.description": "Especifica o método de autenticação com o SQL Server",
			"mssql.connectionOptions.authType.categoryValues.sqlLogin": "Login do SQL",
			"mssql.connectionOptions.authType.categoryValues.integrated": "Autenticação do Windows",
			"mssql.connectionOptions.authType.categoryValues.azureMFA": "Azure Active Directory – Universal com suporte para MFA",
			"mssql.connectionOptions.userName.displayName": "Nome do usuário",
			"mssql.connectionOptions.userName.description": "Indica a ID de usuário a ser usada ao conectar-se à fonte de dados",
			"mssql.connectionOptions.password.displayName": "Senha",
			"mssql.connectionOptions.password.description": "Indica a senha a ser usada ao conectar-se à fonte de dados",
			"mssql.connectionOptions.applicationIntent.displayName": "Intenção do aplicativo",
			"mssql.connectionOptions.applicationIntent.description": "Declara o tipo de carga de trabalho do aplicativo ao conectar-se a um servidor",
			"mssql.connectionOptions.asynchronousProcessing.displayName": "Processamento assíncrono",
			"mssql.connectionOptions.asynchronousProcessing.description": "Quando true, permite o uso da funcionalidade assíncrona no provedor de dados do .NET Framework",
			"mssql.connectionOptions.connectTimeout.displayName": "Tempo limite de conexão",
			"mssql.connectionOptions.connectTimeout.description": "O período de tempo (em segundos) para aguardar uma conexão com o servidor antes de encerrar a tentativa e gerar um erro",
			"mssql.connectionOptions.currentLanguage.displayName": "Idioma atual",
			"mssql.connectionOptions.currentLanguage.description": "O nome do registro de idioma do SQL Server",
			"mssql.connectionOptions.columnEncryptionSetting.displayName": "Criptografia de coluna",
			"mssql.connectionOptions.columnEncryptionSetting.description": "A configuração de criptografia de coluna padrão para todos os comandos na conexão",
			"mssql.connectionOptions.encrypt.displayName": "Criptografar",
			"mssql.connectionOptions.encrypt.description": "Quando true, o SQL Server usa a criptografia SSL para todos os dados enviados entre o cliente e o servidor quando o servidor tem um certificado instalado",
			"mssql.connectionOptions.persistSecurityInfo.displayName": "Persistir as informações de segurança",
			"mssql.connectionOptions.persistSecurityInfo.description": "Quando false, as informações confidenciais de segurança, como a senha, não são retornadas como parte da conexão",
			"mssql.connectionOptions.trustServerCertificate.displayName": "Certificado do servidor de confiança",
			"mssql.connectionOptions.trustServerCertificate.description": "Quando true (e encrypt=true), o SQL Server usa a criptografia SSL para todos os dados enviados entre o cliente e o servidor sem validar o certificado do servidor",
			"mssql.connectionOptions.attachedDBFileName.displayName": "Nome do arquivo de BD anexado",
			"mssql.connectionOptions.attachedDBFileName.description": "O nome do arquivo principal, incluindo o nome do caminho completo, de um banco de dados anexável",
			"mssql.connectionOptions.contextConnection.displayName": "Conexão de contexto",
			"mssql.connectionOptions.contextConnection.description": "Quando true, indica que a conexão deve ser do contexto do SQL Server. Disponível somente quando executado no processo do SQL Server",
			"mssql.connectionOptions.port.displayName": "Porta",
			"mssql.connectionOptions.connectRetryCount.displayName": "Contagem de nova tentativa de conexão",
			"mssql.connectionOptions.connectRetryCount.description": "Número de tentativas para restaurar a conexão",
			"mssql.connectionOptions.connectRetryInterval.displayName": "Intervalo de nova tentativa de conexão",
			"mssql.connectionOptions.connectRetryInterval.description": "Atraso entre as tentativas de restauração de conexão",
			"mssql.connectionOptions.applicationName.displayName": "Nome do aplicativo",
			"mssql.connectionOptions.applicationName.description": "O nome do aplicativo",
			"mssql.connectionOptions.workstationId.displayName": "ID da estação de trabalho",
			"mssql.connectionOptions.workstationId.description": "O nome da estação de trabalho que se conecta ao SQL Server",
			"mssql.connectionOptions.pooling.displayName": "Pooling",
			"mssql.connectionOptions.pooling.description": "Quando true, o objeto de conexão é extraído do pool apropriado ou, se necessário, é criado e adicionado ao pool apropriado",
			"mssql.connectionOptions.maxPoolSize.displayName": "Tamanho máximo do pool",
			"mssql.connectionOptions.maxPoolSize.description": "O número máximo de conexões permitidas no pool",
			"mssql.connectionOptions.minPoolSize.displayName": "Tamanho mínimo do pool",
			"mssql.connectionOptions.minPoolSize.description": "O número mínimo de conexões permitidas no pool",
			"mssql.connectionOptions.loadBalanceTimeout.displayName": "Tempo limite de balanceamento de carga",
			"mssql.connectionOptions.loadBalanceTimeout.description": "O período mínimo de tempo (em segundos) para que essa conexão exista no pool antes de ser destruída",
			"mssql.connectionOptions.replication.displayName": "Replicação",
			"mssql.connectionOptions.replication.description": "Usado pelo SQL Server na replicação",
			"mssql.connectionOptions.attachDbFilename.displayName": "Anexar o nome do arquivo de BD",
			"mssql.connectionOptions.failoverPartner.displayName": "Parceiro de failover",
			"mssql.connectionOptions.failoverPartner.description": "O nome ou o endereço de rede da instância do SQL Server que atua como um parceiro de failover",
			"mssql.connectionOptions.multiSubnetFailover.displayName": "Failover de várias sub-redes",
			"mssql.connectionOptions.multipleActiveResultSets.displayName": "Conjuntos de resultados ativos múltiplos",
			"mssql.connectionOptions.multipleActiveResultSets.description": "Quando true, conjuntos de resultados múltiplos podem ser retornados e lidos de uma conexão",
			"mssql.connectionOptions.packetSize.displayName": "Tamanho do pacote",
			"mssql.connectionOptions.packetSize.description": "Tamanho em bytes dos pacotes de rede usados para comunicar-se com uma instância do SQL Server",
			"mssql.connectionOptions.typeSystemVersion.displayName": "Versão do sistema de tipos",
			"mssql.connectionOptions.typeSystemVersion.description": "Indica qual sistema de tipo de servidor o provedor poderá expor por meio do DataReader"
		},
		"dist/localizedConstants": {
			"msgMissingNodeContext": "Comando de nó chamado sem nenhum nó passado",
			"mssql.manageAccessTitle": "Gerenciar Acesso",
			"mssql.locationTitle": "Localização: ",
			"mssql.permissionsTitle": "Permissões",
			"mssql.ownerPostfix": "– Proprietário",
			"mssql.owningGroupPostfix": "– Grupo Proprietário",
			"mssql.everyone": "Todos os outros",
			"mssql.userLabel": "Usuário",
			"mssql.groupLabel": "Grupo",
			"mssql.accessHeader": "Acesso",
			"mssql.defaultHeader": "Padrão",
			"mssql.delete": "Excluir",
			"mssql.stickyHeader": "Autoadesivo",
			"mssql.inheritDefaultsLabel": "Herdar Padrões",
			"mssql.readHeader": "Ler",
			"mssql.writeHeader": "Escrever",
			"mssql.executeHeader": "Executar",
			"mssql.addUserOrGroup": "Adicionar Usuário ou Grupo",
			"mssql.enterNamePlaceholder": "Inserir o nome",
			"mssql.addLabel": "Adicionar",
			"mssql.namedUsersAndGroups": "Usuários e Grupos Nomeados",
			"mssql.apply": "Aplicar",
			"mssql.applyRecursively": "Aplicar Recursivamente",
			"mssql.errorApplyingAclChanges": "Erro inesperado ao aplicar as alterações: {0}",
			"sparkJobSubmission_LocalFileDestinationHint": "O arquivo local será carregado no HDFS. ",
			"sparkJobSubmission_SubmissionEndMessage": ".......................... Término do Trabalho de Envio do Spark ............................",
			"sparkJobSubmission_PrepareUploadingFile": "Carregando o arquivo da pasta local {0} para HDFS: {1}",
			"sparkJobSubmission_UploadingFileSucceeded": "O upload do arquivo para o cluster foi bem-sucedido.",
			"sparkJobSubmission_UploadingFileFailed": "Falha ao carregar o arquivo para o cluster. {0}",
			"sparkJobSubmission_PrepareSubmitJob": "Enviando o trabalho {0}... ",
			"sparkJobSubmission_SubmitJobFinished": "O Trabalho do Spark foi enviado.",
			"sparkJobSubmission_SubmitJobFailed": "Falha no Envio do Trabalho do Spark. {0} ",
			"sparkJobSubmission_YarnUIMessage": "URL do YarnUI: {0} ",
			"sparkJobSubmission_SparkHistoryLinkMessage": "URL do Histórico do Spark: {0} ",
			"sparkJobSubmission_GetApplicationIdFailed": "Falha ao Obter a ID do Aplicativo. {0}",
			"sparkJobSubmission_LocalFileNotExisted": "O arquivo local {0} não existia. ",
			"sparkJobSubmission_NoSqlBigDataClusterFound": "Não foi encontrado nenhum cluster de Big Data do SQL Server."
		},
		"dist/objectExplorerNodeProvider/fileSources": {
			"maxSizeNotice": "AVISO: este arquivo foi truncado em {0} para visualização. ",
			"maxSizeReached": "O arquivo foi truncado em {0} para visualização."
		},
		"dist/objectExplorerNodeProvider/command": {
			"progress": "$(sync~spin) {0}...",
			"cancelTooltip": "Cancelar",
			"cancel": "Cancelar operação?",
			"mssql.searchServers": "Nomes de Servidores de Pesquisa"
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkJobSubmissionService": {
			"sparkJobSubmission_LivyNoBatchIdReturned": "Não foi retornada nenhuma ID de lote de trabalho do Spark da resposta.{0}[Erro]{1}",
			"sparkJobSubmission_LivyNoLogReturned": "Nenhum log foi retornado na resposta.{0}[Error] {1}"
		},
		"dist/objectExplorerNodeProvider/hdfsCommands": {
			"allFiles": "Todos os Arquivos",
			"lblUploadFiles": "Carregar",
			"uploading": "Carregando arquivos para o HDFS",
			"uploadCanceled": "A operação de upload foi cancelada",
			"uploadError": "Erro ao carregar arquivos: {0}",
			"makingDir": "Criando diretório",
			"mkdirCanceled": "A operação foi cancelada",
			"mkDirError": "Erro ao criar o diretório: {0}",
			"enterDirName": "Insira o nome do diretório",
			"deleteError": "Erro ao excluir arquivos: {0}",
			"msgDeleteFolder": "Tem certeza de que deseja excluir esta pasta e o respectivo conteúdo?",
			"msgDeleteFile": "Tem certeza de que deseja excluir este arquivo?",
			"saving": "Salvando arquivos HDFS",
			"saveCanceled": "A operação de salvamento foi cancelada",
			"saveError": "Erro ao salvar o arquivo: {0}",
			"previewing": "Gerando visualização",
			"previewError": "Erro ao visualizar o arquivo: {0}",
			"copyPathError": "Erro ao copiar o caminho: {0}",
			"manageAccessError": "Erro inesperado ao abrir a caixa de diálogo Gerenciar Acesso: {0}"
		},
		"dist/hdfs/webhdfs": {
			"webhdfs.invalidDataStructure": "Estrutura de Dados Inválida",
			"webhdfs.missingProperties": "Não é possível criar o cliente WebHDFS devido a opções ausentes: ${0}",
			"webhdfs.undefinedArgument": "'$ {0}' é indefinido.",
			"webhdfs.httpError400": "Solicitação inválida",
			"webhdfs.httpError401": "Não autorizado",
			"webhdfs.httpError403": "Proibido",
			"webhdfs.httpError404": "Não Encontrado",
			"webhdfs.httpError500": "Erro interno do servidor",
			"webhdfs.unknownError": "Erro desconhecido",
			"webhdfs.unexpectedRedirect": "Redirecionamento inesperado"
		},
		"dist/objectExplorerNodeProvider/connection": {
			"connectionInfoUndefined": "ConnectionInfo é indefinido.",
			"connectionInfoOptionsUndefined": "ConnectionInfo.options está indefinido.",
			"connectionInfoOptionsMissingProperties": "Algumas propriedades ausentes em connectionInfo.options: {0}"
		},
		"dist/telemetry": {
			"viewKnownIssuesText": "Exibir os Problemas Conhecidos",
			"serviceCrashMessage": "O componente {0} foi encerrado inesperadamente. Reinicie o Azure Data Studio."
		},
		"dist/main": {
			"msgSampleCodeDataFrame": "Este código de exemplo carrega o arquivo em um quadro de dados e mostra os 10 primeiros resultados.",
			"notebookFileType": "Notebooks",
			"unsupportedFileType": "Somente os Notebooks .ipynb são compatíveis",
			"fileNotFound": "Não é possível localizar o arquivo especificado"
		},
		"dist/hdfs/hdfsModel": {
			"mssql.recursivePermissionOpStarted": "Aplicando alterações de permissão recursivamente em '{0}'",
			"mssql.recursivePermissionOpSucceeded": "As alterações de permissão foram aplicadas com êxito.",
			"mssql.recursivePermissionOpProgress": "Aplicando alterações de permissão para '{0}'.",
			"mssql.recursivePermissionOpError": "Erro ao aplicar as alterações de permissão: {0}"
		},
		"dist/prompts/confirm": {
			"msgYes": "Sim",
			"msgNo": "Não"
		},
		"dist/sparkFeature/dialog/dialogCommands": {
			"selectOtherServer": "Selecione outro SQL Server",
			"sparkJobSubmission_PleaseSelectSqlWithCluster": "Selecione o SQL Server com o cluster de Big Data.",
			"sparkJobSubmission_NoSqlSelected": "Nenhum SQL Server está selecionado.",
			"errorNotSqlBigDataCluster": "O servidor selecionado não pertence a um cluster de Big data do SQL Server",
			"sparkJobSubmission_GetFilePathFromSelectedNodeFailed": "Erro ao obter o caminho do arquivo: {0}"
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkJobSubmissionDialog": {
			"sparkJobSubmission_SparkJobSubmissionDialogInitializeError": "Os parâmetros de SparkJobSubmissionDialog são ilegais",
			"sparkJobSubmission_DialogTitleNewJob": "Novo Trabalho",
			"sparkJobSubmission_DialogCancelButton": "Cancelar",
			"sparkJobSubmission_DialogSubmitButton": "Enviar",
			"sparkJobSubmission_SubmitSparkJob": "Envio do Trabalho do Spark {0}:",
			"sparkJobSubmission_SubmissionStartMessage": ".......................... Início do Envio do Trabalho do Spark .........................."
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkJobSubmissionModel": {
			"sparkJobSubmission_SparkJobSubmissionModelInitializeError": "Os parâmetros para SparkJobSubmissionModel são ilegais",
			"sparkJobSubmission_submissionArgsIsInvalid": "submissionArgs é inválido. ",
			"sparkJobSubmission_LivyBatchIdIsInvalid": "livyBatchId é inválido. ",
			"sparkJobSubmission_GetApplicationIdTimeOut": "Obtenha o tempo limite da ID do aplicativo. {0}[Log]   {1}",
			"sparkJobSubmission_localFileOrFolderNotSpecified.": "A propriedade localFilePath ou hdfsFolderPath não está especificada. ",
			"sparkJobSubmission_PathNotSpecified.": "O Caminho da Propriedade não está especificado. "
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkConfigurationTab": {
			"sparkJobSubmission_GeneralTabName": "GERAL",
			"sparkJobSubmission_JobNamePlaceHolder": "Insira um nome...",
			"sparkJobSubmission_JobName": "Nome do Trabalho",
			"sparkJobSubmission_SparkCluster": "Cluster do Spark",
			"sparkJobSubmission_FilePathPlaceHolder": "Caminho para um arquivo .jar ou .py",
			"sparkJobSubmission_LocalFileDestinationHintWithPath": "O arquivo local selecionado será carregado no HDFS: {0}",
			"sparkJobSubmission_MainFilePath": "Arquivo JAR/py",
			"sparkJobSubmission_MainClass": "Classe Principal",
			"sparkJobSubmission_Arguments": "Argumentos",
			"sparkJobSubmission_ArgumentsTooltip": "Argumentos de linha de comando usados em sua classe principal. Vários argumentos devem ser divididos por espaço.",
			"sparkJobSubmission_NotSpecifyJobName": "O nome do trabalho de propriedade não está especificado.",
			"sparkJobSubmission_NotSpecifyJARPYPath": "O arquivo JAR/py de propriedade não está especificado.",
			"sparkJobSubmission_NotSpecifyMainClass": "A classe principal da propriedade não está especificada.",
			"sparkJobSubmission_HDFSFileNotExistedWithPath": "{0} não existe no cluster ou na exceção gerada. ",
			"sparkJobSubmission_HDFSFileNotExisted": "O arquivo HDFS especificado não existe. ",
			"sparkSelectLocalFile": "Selecionar",
			"sparkJobSubmission_SelectFileError": "Erro ao localizar o arquivo devido ao erro: {0}"
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkAdvancedTab": {
			"sparkJobSubmission_AdvancedTabName": "AVANÇADO",
			"sparkJobSubmission_ReferenceJarList": "Jars de Referência",
			"sparkJobSubmission_ReferenceJarListToolTip": "Jars a serem colocados no diretório de trabalho do executor. O caminho do Jar precisa ser um caminho HDFS. Vários caminhos devem ser divididos por ponto e vírgula (;)",
			"sparkJobSubmission_ReferencePyList": "Arquivos py de referência",
			"sparkJobSubmission_ReferencePyListTooltip": "Os Arquivos py a serem colocados no diretório de trabalho do executor. O caminho do arquivo precisa ser um caminho HDFS. Vários caminhos devem ser divididos por ponto e vírgula (;)",
			"sparkJobSubmission_ReferenceFilesList": "Arquivos de referência",
			"sparkJobSubmission_ReferenceFilesListTooltip": "Arquivos a serem colocados no diretório de trabalho do executor. O caminho do arquivo precisa ser um caminho HDFS. É necessário dividir vários caminhos por ponto e vírgula (;)"
		},
		"dist/objectExplorerNodeProvider/objectExplorerNodeProvider": {
			"prmptPwd": "Forneça a senha para conectar-se ao HDFS:",
			"sessionNotFound": "A sessão para o nó {0} não existe",
			"notifyError": "Erro ao notificar a alteração de nó: {0}",
			"hdfsFolder": "HDFS",
			"rootLabel": "Raiz"
		},
		"dist/objectExplorerNodeProvider/hdfsProvider": {
			"errorExpanding": "Erro: {0}",
			"errDeleteConnectionNode": "Não é possível excluir uma conexão. Somente subpastas e arquivos podem ser excluídos."
		},
		"dist/objectExplorerNodeProvider/cancelableStream": {
			"streamCanceled": "Operação de fluxo cancelada pelo usuário"
		},
		"dist/dashboard/serviceEndpoints": {
			"grafana": "Painel de Métricas",
			"kibana": "Painel de Pesquisa de Logs",
			"sparkHistory": "Painel de Gerenciamento e Monitoramento de Trabalhos do Spark",
			"yarnHistory": "Painel de Monitoramento e Diagnóstico do Spark",
			"copyText": "Copiar",
			"endpoint.appproxy": "Proxy de Aplicativo",
			"endpoint.controller": "Serviço de Gerenciamento de Cluster",
			"endpoint.gateway": "Gateway para acessar arquivos HDFS, Spark",
			"endpoint.managementproxy": "Proxy de Gerenciamento",
			"endpoint.mgmtproxy": "Proxy de Gerenciamento",
			"endpoint.sqlServerEndpoint": "Front-end da Instância Mestra do SQL Server",
			"endpoint.grafana": "Painel de Métricas",
			"endpoint.kibana": "Painel de Pesquisa de Logs",
			"endpoint.yarnHistory": "Painel de Monitoramento e Diagnóstico do Spark",
			"endpoint.sparkHistory": "Painel de Gerenciamento e Monitoramento de Trabalhos do Spark",
			"endpoint.webhdfs": "Proxy do Sistema de Arquivos HDFS",
			"endpoint.livy": "Proxy para a execução de instruções, trabalhos, aplicativos do Spark"
		},
		"dist/sqlToolsServer": {
			"serviceStartedStatusMsg": "{0} iniciado",
			"startingServiceStatusMsg": "Iniciando {0}",
			"failedToStartServiceErrorMsg": "Falha ao iniciar o {0}",
			"installingServiceChannelMsg": "Instalando {0} para {1}",
			"installingServiceStatusMsg": "Instalando {0}",
			"installedServiceChannelMsg": "Instalado {0}",
			"downloadingServiceChannelMsg": "Baixando {0}",
			"downloadingServiceSizeChannelMsg": "({0} KB)",
			"downloadingServiceStatusMsg": "Baixando {0}",
			"downloadServiceDoneChannelMsg": "Concluída a instalação de {0}"
		}
	}
}