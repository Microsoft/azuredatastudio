{
	"": [
		"--------------------------------------------------------------------------------------------",
		"Copyright (c) Microsoft Corporation. All rights reserved.",
		"Licensed under the Source EULA. See License.txt in the project root for license information.",
		"--------------------------------------------------------------------------------------------",
		"Do not edit this file. It is machine generated."
	],
	"version": "1.0.0",
	"contents": {
		"package": {
			"json.schemas.desc": "Esquemas associados a arquivos JSON no projeto atual",
			"json.schemas.url.desc": "Uma URL para um esquema ou um caminho relativo a um esquema no diretório atual",
			"json.schemas.fileMatch.desc": "Uma matriz de padrões de arquivos para fazer correspondência ao resolver arquivos JSON para esquemas.",
			"json.schemas.fileMatch.item.desc": "Um padrão de arquivos que pode conter '*' para fazer a correspondência ao resolver arquivos JSON para esquemas.",
			"json.schemas.schema.desc": "A definição de esquema para a URL fornecida. O esquema precisa ser fornecido apenas para evitar acessos à URL do esquema.",
			"json.format.enable.desc": "Habilitar/desabilitar o formatador JSON padrão (requer reinicialização)",
			"mssqlCluster.uploadFiles": "Carregar arquivos",
			"mssqlCluster.mkdir": "Novo diretório",
			"mssqlCluster.deleteFiles": "Excluir",
			"mssqlCluster.previewFile": "Visualizar",
			"mssqlCluster.saveFile": "Salvar",
			"mssqlCluster.copyPath": "Copiar Caminho",
			"mssqlCluster.manageAccess": "Gerenciar Acesso",
			"notebook.command.new": "Novo Notebook",
			"notebook.command.open": "Abrir o Notebook",
			"tab.bigDataClusterDescription": "Tarefas e informações sobre o cluster de Big Data do SQL Server",
			"title.bigDataCluster": "Cluster de Big Data do SQL Server",
			"title.submitSparkJob": "Enviar o Trabalho do Spark",
			"title.newSparkJob": "Novo Trabalho do Spark",
			"title.openSparkHistory": "Exibir o Histórico do Spark",
			"title.openYarnHistory": "Exibir o Histórico do Yarn",
			"title.tasks": "Tarefas",
			"title.installPackages": "Instalar Pacotes",
			"title.configurePython": "Configurar o Python para Notebooks",
			"title.openClusterDashboard": "Cluster\r\nPainel",
			"title.searchServers": "Pesquisar: Servidores",
			"title.clearSearchServerResult": "Pesquisar: Limpar os Resultados do Search Server",
			"title.endpoints": "Pontos de Extremidade de Serviço",
			"title.books": "Notebooks",
			"title.showLogFile": "Mostrar o Arquivo de Log",
			"mssql.disabled": "Desabilitado",
			"mssql.enabled": "Habilitado",
			"mssql.exportNotebookToSql": "Exportar o Notebook como SQL",
			"mssql.exportSqlAsNotebook": "Exportar o SQL como Notebook",
			"mssql.configuration.title": "Configuração do MSSQL",
			"mssql.query.displayBitAsNumber": "Colunas do tipo BIT devem ser exibidas como números (1 ou 0)? Se false, colunas do tipo BIT serão exibidas como 'true' ou 'false'",
			"mssql.query.maxXmlCharsToStore": "Número de caracteres XML a serem armazenados após a execução de uma consulta",
			"mssql.format.alignColumnDefinitionsInColumns": "Definições de coluna devem ser alinhadas?",
			"mssql.format.datatypeCasing": "Tipos de dados devem ser formatados como letras MAIÚSCULAS, minúsculas ou nenhum (não formatado)?",
			"mssql.format.keywordCasing": "Palavras-chave devem ser formatadas como letras MAIÚSCULAS, minúsculas ou nenhum (não formatado)?",
			"mssql.format.placeCommasBeforeNextStatement": "vírgulas devem ser colocadas no início de cada instrução em uma lista? Por exemplo, ', minhacoluna2' em vez de no final, por exemplo, 'minhacoluna1,'?",
			"mssql.format.placeSelectStatementReferencesOnNewLine": "Referências a objetos em uma instrução select devem ser divididas em linhas separadas? Por exemplo, para 'SELECT C1, C2 FROM T1', em que C1 e C2 deverão estar em linhas separadas?",
			"mssql.logDebugInfo": "[Opcional] Registre a saída da depuração no console (Exibir -> Saída) e, em seguida, selecione o canal de saída apropriado no menu suspenso",
			"mssql.tracingLevel": "[Opcional] Registre o nível para serviços de back-end. O Azure Data Studio gera um nome de arquivo sempre que é iniciado e, quando o arquivo já existe, as entradas de logs são acrescentadas a esse arquivo. Para a limpeza de arquivos de log antigos, confira as configurações logRetentionMinutes e logFilesRemovalLimit. O tracingLevel padrão não registra uma grande quantidade de log. A alteração de detalhamento pode levar ao aumento dos requisitos de log e de espaço em disco para os logs. Erro inclui Crítico, Aviso inclui Erro, informações incluem Aviso e Detalhado inclui Informações",
			"mssql.logRetentionMinutes": "O número de minutos para reter os arquivos de log dos serviços de back-end. O padrão é uma semana.",
			"mssql.logFilesRemovalLimit": "Número máximo de arquivos antigos a serem removidos na inicialização com mssql.logRetentionMinutes expirado. Os arquivos que não forem limpos devido a essa limitação serão limpos na próxima vez em que o Azure Data Studio for iniciado.",
			"mssql.intelliSense.enableIntelliSense": "O IntelliSense deve estar habilitado?",
			"mssql.intelliSense.enableErrorChecking": "A verificação de erros do IntelliSense deve estar habilitada?",
			"mssql.intelliSense.enableSuggestions": "As sugestões do IntelliSense devem ser habilitadas?",
			"mssql.intelliSense.enableQuickInfo": "As informações rápidas do IntelliSense devem estar habilitadas?",
			"mssql.intelliSense.lowerCaseSuggestions": "As sugestões do IntelliSense devem estar em letras minúsculas?",
			"mssql.query.setRowCount": "O número máximo de linhas a serem retornadas antes que o servidor pare de processar sua consulta.",
			"mssql.query.textSize": "O tamanho máximo de dados text e ntext retornados de uma instrução SELECT",
			"mssql.query.executionTimeout": "Um tempo limite de execução 0 indica uma espera ilimitada (sem tempo limite)",
			"mssql.query.noCount": "Habilitar a opção SET NOCOUNT",
			"mssql.query.noExec": "Habilitar a opção SET NOEXEC",
			"mssql.query.parseOnly": "Habilitar a opção SET PARSEONLY",
			"mssql.query.arithAbort": "Habilitar a opção SET ARITHABORT",
			"mssql.query.statisticsTime": "Habilitar a opção SET STATISTICS TIME",
			"mssql.query.statisticsIO": "Habilitar a opção SET STATISTICS IO",
			"mssql.query.xactAbortOn": "Habilitar a opção SET XACT_ABORT ON",
			"mssql.query.transactionIsolationLevel": "Habilitar a opção SET TRANSACTION ISOLATION LEVEL",
			"mssql.query.deadlockPriority": "Habilitar a opção SET DEADLOCK_PRIORITY",
			"mssql.query.lockTimeout": "Habilitar a opção SET LOCK TIMEOUT (em milissegundos)",
			"mssql.query.queryGovernorCostLimit": "Habilitar SET QUERY_GOVERNOR_COST_LIMIT",
			"mssql.query.ansiDefaults": "Habilitar SET ANSI_DEFAULTS",
			"mssql.query.quotedIdentifier": "Habilitar SET QUOTED_IDENTIFIER",
			"mssql.query.ansiNullDefaultOn": "Habilitar SET ANSI_NULL_DFLT_ON",
			"mssql.query.implicitTransactions": "Habilitar SET IMPLICIT_TRANSACTIONS",
			"mssql.query.cursorCloseOnCommit": "Habilitar SET CURSOR_CLOSE_ON_COMMIT",
			"mssql.query.ansiPadding": "Habilitar SET ANSI_PADDING",
			"mssql.query.ansiWarnings": "Habilitar SET ANSI_WARNINGS",
			"mssql.query.ansiNulls": "Habilitar SET ANSI_NULLS",
			"mssql.query.alwaysEncryptedParameterization": "Habilitar a Parametrização para Always Encrypted",
			"mssql.ignorePlatformWarning": "[Opcional] Não mostrar os avisos de plataforma sem suporte",
			"onprem.databaseProperties.recoveryModel": "Modo de Recuperação",
			"onprem.databaseProperties.lastBackupDate": "Último Backup de Banco de Dados",
			"onprem.databaseProperties.lastLogBackupDate": "Último Backup de Log",
			"onprem.databaseProperties.compatibilityLevel": "Nível de Compatibilidade",
			"onprem.databaseProperties.owner": "Proprietário",
			"onprem.serverProperties.serverVersion": "Versão",
			"onprem.serverProperties.serverEdition": "Edição",
			"onprem.serverProperties.machineName": "Nome do Computador",
			"onprem.serverProperties.osVersion": "Versão do Sistema Operacional",
			"cloud.databaseProperties.azureEdition": "Edição",
			"cloud.databaseProperties.serviceLevelObjective": "Tipo de Preço",
			"cloud.databaseProperties.compatibilityLevel": "Nível de Compatibilidade",
			"cloud.databaseProperties.owner": "Proprietário",
			"cloud.serverProperties.serverVersion": "Versão",
			"cloud.serverProperties.serverEdition": "Tipo",
			"mssql.provider.displayName": "Microsoft SQL Server",
			"mssql.connectionOptions.connectionName.displayName": "Nome (opcional)",
			"mssql.connectionOptions.connectionName.description": "Nome personalizado da conexão",
			"mssql.connectionOptions.serverName.displayName": "Servidor",
			"mssql.connectionOptions.serverName.description": "Nome da instância do SQL Server",
			"mssql.connectionOptions.databaseName.displayName": "Banco de dados",
			"mssql.connectionOptions.databaseName.description": "O nome do catálogo ou do banco de dados inicial na fonte de dados",
			"mssql.connectionOptions.authType.displayName": "Tipo de autenticação",
			"mssql.connectionOptions.authType.description": "Especifica o método de autenticação com o SQL Server",
			"mssql.connectionOptions.authType.categoryValues.sqlLogin": "Login do SQL",
			"mssql.connectionOptions.authType.categoryValues.integrated": "Autenticação do Windows",
			"mssql.connectionOptions.authType.categoryValues.azureMFA": "Azure Active Directory – Universal com suporte para MFA",
			"mssql.connectionOptions.userName.displayName": "Nome do usuário",
			"mssql.connectionOptions.userName.description": "Indica a ID de usuário a ser usada ao conectar-se à fonte de dados",
			"mssql.connectionOptions.password.displayName": "Senha",
			"mssql.connectionOptions.password.description": "Indica a senha a ser usada ao conectar-se à fonte de dados",
			"mssql.connectionOptions.applicationIntent.displayName": "Intenção do aplicativo",
			"mssql.connectionOptions.applicationIntent.description": "Declara o tipo de carga de trabalho do aplicativo ao conectar-se a um servidor",
			"mssql.connectionOptions.asynchronousProcessing.displayName": "Processamento assíncrono",
			"mssql.connectionOptions.asynchronousProcessing.description": "Quando true, permite o uso da funcionalidade assíncrona no provedor de dados do .NET Framework",
			"mssql.connectionOptions.connectTimeout.displayName": "Tempo limite de conexão",
			"mssql.connectionOptions.connectTimeout.description": "O período de tempo (em segundos) para aguardar uma conexão com o servidor antes de encerrar a tentativa e gerar um erro",
			"mssql.connectionOptions.currentLanguage.displayName": "Idioma atual",
			"mssql.connectionOptions.currentLanguage.description": "O nome do registro de idioma do SQL Server",
			"mssql.connectionOptions.columnEncryptionSetting.displayName": "Always Encrypted",
			"mssql.connectionOptions.columnEncryptionSetting.description": "Habilita ou desabilita o Always Encrypted para a conexão",
			"mssql.connectionOptions.enclaveAttestationProtocol.displayName": "Protocolo de Atestado",
			"mssql.connectionOptions.enclaveAttestationProtocol.description": "Especifica um protocolo para atestar um enclave do lado do servidor usado com o Always Encrypted com enclaves seguros",
			"mssql.connectionOptions.enclaveAttestationProtocol.categoryValues.AAS": "Atestado do Azure",
			"mssql.connectionOptions.enclaveAttestationProtocol.categoryValues.HGS": "Serviço Guardião de Host",
			"mssql.connectionOptions.enclaveAttestationUrl.displayName": "URL de Atestado de Enclave",
			"mssql.connectionOptions.enclaveAttestationUrl.description": "Especifica um ponto de extremidade para atestar um enclave do lado do servidor usado com o Always Encrypted com enclaves seguros",
			"mssql.connectionOptions.encrypt.displayName": "Criptografar",
			"mssql.connectionOptions.encrypt.description": "Quando true, o SQL Server usa a criptografia SSL para todos os dados enviados entre o cliente e o servidor quando o servidor tem um certificado instalado",
			"mssql.connectionOptions.persistSecurityInfo.displayName": "Persistir as informações de segurança",
			"mssql.connectionOptions.persistSecurityInfo.description": "Quando false, as informações confidenciais de segurança, como a senha, não são retornadas como parte da conexão",
			"mssql.connectionOptions.trustServerCertificate.displayName": "Certificado do servidor de confiança",
			"mssql.connectionOptions.trustServerCertificate.description": "Quando true (e encrypt=true), o SQL Server usa a criptografia SSL para todos os dados enviados entre o cliente e o servidor sem validar o certificado do servidor",
			"mssql.connectionOptions.attachedDBFileName.displayName": "Nome do arquivo de BD anexado",
			"mssql.connectionOptions.attachedDBFileName.description": "O nome do arquivo principal, incluindo o nome do caminho completo, de um banco de dados anexável",
			"mssql.connectionOptions.contextConnection.displayName": "Conexão de contexto",
			"mssql.connectionOptions.contextConnection.description": "Quando true, indica que a conexão deve ser do contexto do SQL Server. Disponível somente quando executado no processo do SQL Server",
			"mssql.connectionOptions.port.displayName": "Porta",
			"mssql.connectionOptions.connectRetryCount.displayName": "Contagem de nova tentativa de conexão",
			"mssql.connectionOptions.connectRetryCount.description": "Número de tentativas para restaurar a conexão",
			"mssql.connectionOptions.connectRetryInterval.displayName": "Intervalo de nova tentativa de conexão",
			"mssql.connectionOptions.connectRetryInterval.description": "Atraso entre as tentativas de restauração de conexão",
			"mssql.connectionOptions.applicationName.displayName": "Nome do aplicativo",
			"mssql.connectionOptions.applicationName.description": "O nome do aplicativo",
			"mssql.connectionOptions.workstationId.displayName": "ID da estação de trabalho",
			"mssql.connectionOptions.workstationId.description": "O nome da estação de trabalho que se conecta ao SQL Server",
			"mssql.connectionOptions.pooling.displayName": "Pooling",
			"mssql.connectionOptions.pooling.description": "Quando true, o objeto de conexão é extraído do pool apropriado ou, se necessário, é criado e adicionado ao pool apropriado",
			"mssql.connectionOptions.maxPoolSize.displayName": "Tamanho máximo do pool",
			"mssql.connectionOptions.maxPoolSize.description": "O número máximo de conexões permitidas no pool",
			"mssql.connectionOptions.minPoolSize.displayName": "Tamanho mínimo do pool",
			"mssql.connectionOptions.minPoolSize.description": "O número mínimo de conexões permitidas no pool",
			"mssql.connectionOptions.loadBalanceTimeout.displayName": "Tempo limite de balanceamento de carga",
			"mssql.connectionOptions.loadBalanceTimeout.description": "O período mínimo de tempo (em segundos) para que essa conexão exista no pool antes de ser destruída",
			"mssql.connectionOptions.replication.displayName": "Replicação",
			"mssql.connectionOptions.replication.description": "Usado pelo SQL Server na replicação",
			"mssql.connectionOptions.attachDbFilename.displayName": "Anexar o nome do arquivo de BD",
			"mssql.connectionOptions.failoverPartner.displayName": "Parceiro de failover",
			"mssql.connectionOptions.failoverPartner.description": "O nome ou o endereço de rede da instância do SQL Server que atua como um parceiro de failover",
			"mssql.connectionOptions.multiSubnetFailover.displayName": "Failover de várias sub-redes",
			"mssql.connectionOptions.multipleActiveResultSets.displayName": "Conjuntos de resultados ativos múltiplos",
			"mssql.connectionOptions.multipleActiveResultSets.description": "Quando true, conjuntos de resultados múltiplos podem ser retornados e lidos de uma conexão",
			"mssql.connectionOptions.packetSize.displayName": "Tamanho do pacote",
			"mssql.connectionOptions.packetSize.description": "Tamanho em bytes dos pacotes de rede usados para comunicar-se com uma instância do SQL Server",
			"mssql.connectionOptions.typeSystemVersion.displayName": "Versão do sistema de tipos",
			"mssql.connectionOptions.typeSystemVersion.description": "Indica qual sistema do tipo de servidor o provedor vai expor por meio do DataReader",
			"databasesListProperties.name": "Nome",
			"databasesListProperties.status": "Status",
			"databasesListProperties.size": "Tamanho (MB)",
			"databasesListProperties.lastBackup": "Último backup",
			"objectsListProperties.name": "Nome"
		},
		"dist/localizedConstants": {
			"msgMissingNodeContext": "Comando de nó chamado sem nenhum nó passado",
			"mssql.manageAccessTitle": "Gerenciar Acesso",
			"mssql.locationTitle": "Localização: ",
			"mssql.permissionsTitle": "Permissões",
			"mssql.ownerPostfix": "– Proprietário",
			"mssql.owner": "Proprietário",
			"mssql.group": "Grupo",
			"mssql.owningGroupPostfix": "– Grupo Proprietário",
			"mssql.everyone": "Todos os outros",
			"mssql.userLabel": "Usuário",
			"mssql.groupLabel": "Grupo",
			"mssql.accessHeader": "Acesso",
			"mssql.defaultHeader": "Padrão",
			"mssql.delete": "Excluir",
			"mssql.stickyHeader": "Sticky Bit",
			"mssql.inheritDefaultsLabel": "Herdar Padrões",
			"mssql.readHeader": "Ler",
			"mssql.writeHeader": "Escrever",
			"mssql.executeHeader": "Executar",
			"mssql.addUserOrGroup": "Adicionar Usuário ou Grupo",
			"mssql.enterNamePlaceholder": "Inserir o nome",
			"mssql.addLabel": "Adicionar",
			"mssql.namedUsersAndGroups": "Usuários e Grupos Nomeados",
			"mssql.defaultUserAndGroups": "Usuário e Grupos Padrão",
			"mssql.userOrGroupIcon": "Ícone do Usuário ou Grupo",
			"mssql.apply": "Aplicar",
			"mssql.applyRecursively": "Aplicar Recursivamente",
			"mssql.errorApplyingAclChanges": "Erro inesperado ao aplicar as alterações: {0}",
			"sparkJobSubmission.LocalFileDestinationHint": "O arquivo local será carregado no HDFS. ",
			"sparkJobSubmission.SubmissionEndMessage": ".......................... Término do Trabalho de Envio do Spark ............................",
			"sparkJobSubmission.PrepareUploadingFile": "Carregando o arquivo da pasta local {0} para HDFS: {1}",
			"sparkJobSubmission.UploadingFileSucceeded": "O upload do arquivo para o cluster foi bem-sucedido.",
			"sparkJobSubmission.UploadingFileFailed": "Falha ao carregar o arquivo para o cluster. {0}",
			"sparkJobSubmission.PrepareSubmitJob": "Enviando o trabalho {0}... ",
			"sparkJobSubmission.SubmitJobFinished": "O Trabalho do Spark foi enviado.",
			"sparkJobSubmission.SubmitJobFailed": "Falha no Envio do Trabalho do Spark. {0} ",
			"sparkJobSubmission.YarnUIMessage": "URL do YarnUI: {0} ",
			"sparkJobSubmission.SparkHistoryLinkMessage": "URL do Histórico do Spark: {0} ",
			"sparkJobSubmission.GetApplicationIdFailed": "Falha ao Obter a ID do Aplicativo. {0}",
			"sparkJobSubmission.LocalFileNotExisted": "O arquivo local {0} não existia. ",
			"sparkJobSubmission.NoSqlBigDataClusterFound": "Não foi encontrado nenhum cluster de Big Data do SQL Server.",
			"sparkConnectionRequired": "Conecte-se ao cluster do Spark antes de Exibir o Histórico de {0}."
		},
		"dist/objectExplorerNodeProvider/fileSources": {
			"maxSizeNotice": "AVISO: este arquivo foi truncado em {0} para visualização. ",
			"maxSizeReached": "O arquivo foi truncado em {0} para visualização."
		},
		"dist/objectExplorerNodeProvider/command": {
			"progress": "$(sync~spin) {0}...",
			"cancelTooltip": "Cancelar",
			"cancel": "Cancelar operação?",
			"mssql.searchServers": "Nomes de Servidores de Pesquisa"
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkJobSubmissionService": {
			"sparkJobSubmission.LivyNoBatchIdReturned": "Não foi retornada nenhuma ID de lote de trabalho do Spark da resposta.{0}[Erro]{1}",
			"sparkJobSubmission.LivyNoLogReturned": "Nenhum log foi retornado na resposta.{0}[Error] {1}"
		},
		"dist/sqlClusterLookUp": {
			"promptBDCUsername": "{0}Forneça o nome de usuário para se conectar ao Controlador do BDC:",
			"promptBDCPassword": "Forneça a senha para se conectar ao Controlador do BDC",
			"bdcConnectError": "Erro: {0}. ",
			"usernameAndPasswordRequired": "O nome de usuário e a senha são obrigatórios"
		},
		"dist/objectExplorerNodeProvider/hdfsCommands": {
			"allFiles": "Todos os Arquivos",
			"lblUploadFiles": "Carregar",
			"uploading": "Carregando arquivos para o HDFS",
			"uploadCanceled": "A operação de upload foi cancelada",
			"uploadError": "Erro ao carregar arquivos: {0}",
			"makingDir": "Criando diretório",
			"mkdirCanceled": "A operação foi cancelada",
			"mkDirError": "Erro ao criar o diretório: {0}",
			"enterDirName": "Insira o nome do diretório",
			"deleteError": "Erro ao excluir arquivos: {0}",
			"msgDeleteFolder": "Tem certeza de que deseja excluir esta pasta e o respectivo conteúdo?",
			"msgDeleteFile": "Tem certeza de que deseja excluir este arquivo?",
			"saving": "Salvando arquivos HDFS",
			"saveCanceled": "A operação de salvamento foi cancelada",
			"saveError": "Erro ao salvar o arquivo: {0}",
			"previewing": "Gerando visualização",
			"previewError": "Erro ao visualizar o arquivo: {0}",
			"copyPathError": "Erro ao copiar o caminho: {0}",
			"manageAccessError": "Erro inesperado ao abrir a caixa de diálogo Gerenciar Acesso: {0}"
		},
		"dist/hdfs/webhdfs": {
			"webhdfs.invalidDataStructure": "Estrutura de Dados Inválida",
			"webhdfs.missingProperties": "Não é possível criar o cliente WebHDFS devido a opções ausentes: ${0}",
			"webhdfs.undefinedArgument": "'$ {0}' é indefinido.",
			"webhdfs.httpError400": "Solicitação inválida",
			"webhdfs.httpError401": "Não autorizado",
			"webhdfs.httpError403": "Proibido",
			"webhdfs.httpError404": "Não Encontrado",
			"webhdfs.httpError500": "Erro interno do servidor",
			"webhdfs.unknownError": "Erro desconhecido",
			"webhdfs.unexpectedRedirect": "Redirecionamento inesperado"
		},
		"dist/objectExplorerNodeProvider/connection": {
			"connectionInfoUndefined": "ConnectionInfo é indefinido.",
			"connectionInfoOptionsUndefined": "ConnectionInfo.options está indefinido.",
			"connectionInfoOptionsMissingProperties": "Algumas propriedades ausentes em connectionInfo.options: {0}"
		},
		"dist/telemetry": {
			"viewKnownIssuesText": "Exibir os Problemas Conhecidos",
			"serviceCrashMessage": "O componente {0} foi encerrado inesperadamente. Reinicie o Azure Data Studio."
		},
		"dist/main": {
			"msgSampleCodeDataFrame": "Este código de exemplo carrega o arquivo em um quadro de dados e mostra os 10 primeiros resultados.",
			"mssql.errorConvertingToNotebook": "Ocorreu um erro ao converter o documento SQL em um Notebook. Erro: {0}",
			"mssql.errorConvertingToSQL": "Ocorreu um erro ao converter o documento do Notebook para SQL. Erro: {0}",
			"notebookFileType": "Notebooks",
			"unsupportedFileType": "Somente os Notebooks .ipynb são compatíveis",
			"noController": "Não foi possível localizar o ponto de extremidade do controlador para esta instância"
		},
		"dist/hdfs/hdfsModel": {
			"mssql.recursivePermissionOpStarted": "Aplicando alterações de permissão recursivamente em '{0}'",
			"mssql.recursivePermissionOpSucceeded": "As alterações de permissão foram aplicadas com êxito.",
			"mssql.recursivePermissionOpProgress": "Aplicando alterações de permissão para '{0}'.",
			"mssql.recursivePermissionOpError": "Erro ao aplicar as alterações de permissão: {0}"
		},
		"dist/prompts/confirm": {
			"msgYes": "Sim",
			"msgNo": "Não"
		},
		"dist/sparkFeature/dialog/dialogCommands": {
			"selectOtherServer": "Selecione outro SQL Server",
			"sparkJobSubmission.PleaseSelectSqlWithCluster": "Selecione o SQL Server com o cluster de Big Data.",
			"sparkJobSubmission.NoSqlSelected": "Nenhum SQL Server está selecionado.",
			"errorNotSqlBigDataCluster": "O servidor selecionado não pertence a um cluster de Big data do SQL Server",
			"sparkJobSubmission.GetFilePathFromSelectedNodeFailed": "Erro ao obter o caminho do arquivo: {0}"
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkJobSubmissionDialog": {
			"sparkJobSubmission.SparkJobSubmissionDialogInitializeError": "Os parâmetros de SparkJobSubmissionDialog são ilegais",
			"sparkJobSubmission.DialogTitleNewJob": "Novo Trabalho",
			"sparkJobSubmission.DialogCancelButton": "Cancelar",
			"sparkJobSubmission.DialogSubmitButton": "Enviar",
			"sparkJobSubmission.SubmitSparkJob": "Envio do Trabalho do Spark {0}:",
			"sparkJobSubmission.SubmissionStartMessage": ".......................... Início do Envio do Trabalho do Spark .........................."
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkJobSubmissionModel": {
			"sparkJobSubmission.SparkJobSubmissionModelInitializeError": "Os parâmetros para SparkJobSubmissionModel são ilegais",
			"sparkJobSubmission.submissionArgsIsInvalid": "submissionArgs é inválido. ",
			"sparkJobSubmission.LivyBatchIdIsInvalid": "livyBatchId é inválido. ",
			"sparkJobSubmission.GetApplicationIdTimeOut": "Obtenha o tempo limite da ID do aplicativo. {0}[Log]   {1}",
			"sparkJobSubmission.localFileOrFolderNotSpecified.": "A propriedade localFilePath ou hdfsFolderPath não está especificada. ",
			"sparkJobSubmission.PathNotSpecified.": "O Caminho da Propriedade não está especificado. "
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkConfigurationTab": {
			"sparkJobSubmission.GeneralTabName": "GERAL",
			"sparkJobSubmission.JobNamePlaceHolder": "Insira um nome...",
			"sparkJobSubmission.JobName": "Nome do Trabalho",
			"sparkJobSubmission.SparkCluster": "Cluster do Spark",
			"sparkJobSubmission.FilePathPlaceHolder": "Caminho para um arquivo .jar ou .py",
			"sparkJobSubmission.LocalFileDestinationHintWithPath": "O arquivo local selecionado será carregado no HDFS: {0}",
			"sparkJobSubmission.MainFilePath": "Arquivo JAR/py",
			"sparkJobSubmission.MainClass": "Classe Principal",
			"sparkJobSubmission.Arguments": "Argumentos",
			"sparkJobSubmission.ArgumentsTooltip": "Argumentos de linha de comando usados em sua classe principal. Vários argumentos devem ser divididos por espaço.",
			"sparkJobSubmission.NotSpecifyJobName": "O nome do trabalho de propriedade não está especificado.",
			"sparkJobSubmission.NotSpecifyJARPYPath": "O arquivo JAR/py de propriedade não está especificado.",
			"sparkJobSubmission.NotSpecifyMainClass": "A classe principal da propriedade não está especificada.",
			"sparkJobSubmission.HDFSFileNotExistedWithPath": "{0} não existe no cluster ou na exceção gerada. ",
			"sparkJobSubmission.HDFSFileNotExisted": "O arquivo HDFS especificado não existe. ",
			"sparkSelectLocalFile": "Selecionar",
			"sparkJobSubmission.SelectFileError": "Erro ao localizar o arquivo devido ao erro: {0}"
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkAdvancedTab": {
			"sparkJobSubmission.AdvancedTabName": "AVANÇADO",
			"sparkJobSubmission.ReferenceJarList": "Jars de Referência",
			"sparkJobSubmission.ReferenceJarListToolTip": "Jars a serem colocados no diretório de trabalho do executor. O caminho do Jar precisa ser um caminho HDFS. Vários caminhos devem ser divididos por ponto e vírgula (;)",
			"sparkJobSubmission.ReferencePyList": "Arquivos py de Referência",
			"sparkJobSubmission.ReferencePyListTooltip": "Os Arquivos py a serem colocados no diretório de trabalho do executor. O caminho do arquivo precisa ser um caminho HDFS. Vários caminhos devem ser divididos por ponto e vírgula (;)",
			"sparkJobSubmission.ReferenceFilesList": "Arquivos de Referência",
			"sparkJobSubmission.ReferenceFilesListTooltip": "Arquivos a serem colocados no diretório de trabalho do executor. O caminho do arquivo precisa ser um caminho HDFS. É necessário dividir vários caminhos por ponto e vírgula (;)",
			"sparkJobSubmission.driverMemory": "Memória do Driver",
			"sparkJobSubmission.driverMemoryTooltip": "Quantidade de memória a ser alocada para o driver. Especifique as unidades como parte do valor. Por exemplo, 512M ou 2G.",
			"sparkJobSubmission.driverCores": "Núcleos do Driver",
			"sparkJobSubmission.driverCoresTooltip": "Quantidade de núcleos de CPU a ser alocada para o driver.",
			"sparkJobSubmission.executorMemory": "Memória do Executor",
			"sparkJobSubmission.executorMemoryTooltip": "Quantidade de memória a ser alocada para o executor. Especifique as unidades como parte do valor. Por exemplo, 512M ou 2G.",
			"sparkJobSubmission.executorCores": "Núcleos do Executor",
			"sparkJobSubmission.executorCoresTooltip": "Quantidade de núcleos de CPU a serem alocados para o executor.",
			"sparkJobSubmission.executorCount": "Contagem do Executor",
			"sparkJobSubmission.executorCountTooltip": "Número de instâncias do executor que serão executadas.",
			"sparkJobSubmission.queueName": "Nome da Fila",
			"sparkJobSubmission.queueNameTooltip": "Nome da fila do Spark na qual executar a sessão.",
			"sparkJobSubmission.configValues": "Valores de Configuração",
			"sparkJobSubmission.configValuesTooltip": "Lista de pares nome/valor que contêm valores de configuração do Spark. Codificado como um dicionário JSON. Exemplo: '{\"name\":\"value\", \"name2\":\"value2\"}'."
		},
		"dist/objectExplorerNodeProvider/objectExplorerNodeProvider": {
			"promptUsername": "Forneça o nome de usuário para conectar-se ao HDFS:",
			"prmptPwd": "Forneça a senha para conectar-se ao HDFS:",
			"sessionNotFound": "A sessão para o nó {0} não existe",
			"notifyError": "Erro ao notificar a alteração de nó: {0}",
			"hdfsFolder": "HDFS",
			"rootLabel": "Raiz"
		},
		"dist/objectExplorerNodeProvider/hdfsProvider": {
			"errorExpanding": "Erro: {0}",
			"errDeleteConnectionNode": "Não é possível excluir uma conexão. Somente subpastas e arquivos podem ser excluídos."
		},
		"dist/objectExplorerNodeProvider/cancelableStream": {
			"streamCanceled": "Operação de fluxo cancelada pelo usuário"
		},
		"dist/dashboard/serviceEndpoints": {
			"grafana": "Painel de Métricas",
			"kibana": "Painel de Pesquisa de Logs",
			"sparkHistory": "Painel de Gerenciamento e Monitoramento de Trabalhos do Spark",
			"yarnHistory": "Painel de Monitoramento e Diagnóstico do Spark",
			"copyText": "Copiar",
			"endpoint.appproxy": "Proxy de Aplicativo",
			"endpoint.controller": "Serviço de Gerenciamento de Cluster",
			"endpoint.gateway": "Gateway para acessar arquivos HDFS, Spark",
			"endpoint.managementproxy": "Proxy de Gerenciamento",
			"endpoint.mgmtproxy": "Proxy de Gerenciamento",
			"endpoint.sqlServerEndpoint": "Front-end da Instância Mestra do SQL Server",
			"endpoint.grafana": "Painel de Métricas",
			"endpoint.kibana": "Painel de Pesquisa de Logs",
			"endpoint.yarnHistory": "Painel de Monitoramento e Diagnóstico do Spark",
			"endpoint.sparkHistory": "Painel de Gerenciamento e Monitoramento de Trabalhos do Spark",
			"endpoint.webhdfs": "Proxy do Sistema de Arquivos HDFS",
			"endpoint.livy": "Proxy para a execução de instruções, trabalhos, aplicativos do Spark"
		},
		"dist/sqlToolsServer": {
			"serviceStartedStatusMsg": "{0} iniciado",
			"startingServiceStatusMsg": "Iniciando {0}",
			"failedToStartServiceErrorMsg": "Falha ao iniciar o {0}",
			"installingServiceChannelMsg": "Instalando {0} para {1}",
			"installingServiceStatusMsg": "Instalando {0}",
			"installedServiceChannelMsg": "Instalado {0}",
			"downloadingServiceChannelMsg": "Baixando {0}",
			"downloadingServiceSizeChannelMsg": "({0} KB)",
			"downloadingServiceStatusMsg": "Baixando {0}",
			"downloadServiceDoneChannelMsg": "Concluída a instalação de {0}",
			"entryExtractedChannelMsg": "{0} extraído ({1}/{2})"
		},
		"dist/features": {
			"mssql.missingLinkedAzureAccount": "O Azure Data Studio precisa entrar em contato com o Azure Key Vault para acessar uma chave mestra de coluna para Always Encrypted, mas nenhuma conta vinculada do Azure está disponível. Adicione uma conta vinculada do Azure e tente a consulta novamente.",
			"mssql.chooseLinkedAzureAccount": "Selecione uma conta vinculada do Azure:",
			"mssql.canceledLinkedAzureAccountSelection": "O Azure Data Studio precisa entrar em contato com o Azure Key Vault para acessar uma chave mestra de coluna para Always Encrypted, mas nenhuma conta vinculada do Azure foi selecionada. Repita a consulta e selecione uma conta vinculada do Azure quando solicitado.",
			"mssql.insufficientlyPrivelagedAzureAccount": "A conta do Azure configurada para {0} não tem permissões suficientes para o Azure Key Vault para acessar uma chave mestra de coluna para Always Encrypted."
		}
	}
}