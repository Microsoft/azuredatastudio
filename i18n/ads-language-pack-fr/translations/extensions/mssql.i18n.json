{
	"": [
		"--------------------------------------------------------------------------------------------",
		"Copyright (c) Microsoft Corporation. All rights reserved.",
		"Licensed under the Source EULA. See License.txt in the project root for license information.",
		"--------------------------------------------------------------------------------------------",
		"Do not edit this file. It is machine generated."
	],
	"version": "1.0.0",
	"contents": {
		"package": {
			"json.schemas.desc": "Associer des schémas aux fichiers JSON dans le projet actuel",
			"json.schemas.url.desc": "URL de schéma ou chemin relatif d'un schéma dans le répertoire actuel",
			"json.schemas.fileMatch.desc": "Tableau de modèles de fichier à mapper durant la résolution de fichiers JSON en schémas.",
			"json.schemas.fileMatch.item.desc": "Modèle de fichier pouvant contenir '*' à mapper durant la résolution de fichiers JSON en schémas.",
			"json.schemas.schema.desc": "Définition de schéma pour l'URL indiquée. Le schéma doit être fourni uniquement pour éviter les accès à l'URL du schéma.",
			"json.format.enable.desc": "Activer/désactiver le formateur JSON par défaut (nécessite un redémarrage)",
			"mssqlCluster.uploadFiles": "Charger des fichiers",
			"mssqlCluster.mkdir": "Nouveau répertoire",
			"mssqlCluster.deleteFiles": "Supprimer",
			"mssqlCluster.previewFile": "Aperçu",
			"mssqlCluster.saveFile": "Enregistrer",
			"mssqlCluster.copyPath": "Copier le chemin",
			"mssqlCluster.manageAccess": "Gérer l'accès",
			"notebook.command.new": "Nouveau notebook",
			"notebook.command.open": "Ouvrir le notebook",
			"tab.bigDataClusterDescription": "Tâches et informations concernant votre cluster Big Data SQL Server",
			"title.bigDataCluster": "Cluster Big Data SQL Server",
			"title.submitSparkJob": "Envoyer le travail Spark",
			"title.newSparkJob": "Nouveau travail Spark",
			"title.openSparkHistory": "Voir l'historique Spark",
			"title.openYarnHistory": "Voir l'historique Yarn",
			"title.tasks": "Tâches",
			"title.installPackages": "Installer les packages",
			"title.configurePython": "Configurer Python pour Notebooks",
			"title.searchServers": "Recherche : Serveurs",
			"title.clearSearchServerResult": "Recherche : Effacer les résultats du serveur de recherche",
			"title.endpoints": "Points de terminaison de service",
			"title.books": "Notebooks",
			"title.showLogFile": "Afficher le fichier journal",
			"mssql.configuration.title": "Configuration MSSQL",
			"mssql.query.displayBitAsNumber": "Spécifie si les colonnes BIT doivent être affichées sous forme de nombre (1 ou 0). Si la valeur est false, les colonnes BIT sont affichées sous la forme 'true' ou 'false'",
			"mssql.format.alignColumnDefinitionsInColumns": "Spécifie si les définitions de colonne doivent être alignées",
			"mssql.format.datatypeCasing": "Spécifie si la mise en forme des types de données est MAJUSCULES, minuscules ou aucune (sans mise en forme)",
			"mssql.format.keywordCasing": "Spécifie si la mise en forme des mots clés est MAJUSCULES, minuscules ou aucune (sans mise en forme)",
			"mssql.format.placeCommasBeforeNextStatement": "spécifie si des virgules doivent être placées au début de chaque instruction dans une liste (par exemple : ',mycolumn2') plutôt qu'à la fin (par exemple : 'mycolumn1,')",
			"mssql.format.placeSelectStatementReferencesOnNewLine": "Spécifie si les références aux objets dans les instructions select doivent être divisées en plusieurs lignes. Par exemple, pour 'SELECT C1, C2 FROM T1', C1 et C2 sont sur des lignes distinctes",
			"mssql.logDebugInfo": "[Facultatif] Journaliser la sortie de débogage dans la console (Voir -> Sortie) et sélectionner le canal de sortie approprié dans la liste déroulante",
			"mssql.tracingLevel": "[Facultatif] Niveau de journalisation des services de back-end. Azure Data Studio génère un nom de fichier à chaque démarrage et, si le fichier existe déjà, ajoute les entrées de journal à ce fichier. Pour nettoyer les anciens fichiers journaux, consultez les paramètres logRetentionMinutes et logFilesRemovalLimit. Le niveau de suivi par défaut correspond à une faible journalisation. Si vous changez le niveau de détail, vous pouvez obtenir une journalisation massive nécessitant de l'espace disque pour les journaux. Le niveau Erreur inclut le niveau Critique, le niveau Avertissement inclut le niveau Erreur, le niveau Informations inclut le niveau Avertissement et le niveau Détail inclut le niveau Informations",
			"mssql.logRetentionMinutes": "Nombre de minutes de conservation des fichiers journaux pour les services de back-end. La durée par défaut est 1 semaine.",
			"mssql.logFilesRemovalLimit": "Nombre maximal d'anciens fichiers ayant dépassé mssql.logRetentionMinutes à supprimer au démarrage. Les fichiers qui ne sont pas nettoyés du fait de cette limitation le sont au prochain démarrage d'Azure Data Studio.",
			"ignorePlatformWarning": "[Facultatif] Ne pas afficher les avertissements de plateforme non prise en charge",
			"onprem.databaseProperties.recoveryModel": "Mode de récupération",
			"onprem.databaseProperties.lastBackupDate": "Dernière sauvegarde de base de données",
			"onprem.databaseProperties.lastLogBackupDate": "Dernière sauvegarde de journal",
			"onprem.databaseProperties.compatibilityLevel": "Niveau de compatibilité",
			"onprem.databaseProperties.owner": "Propriétaire",
			"onprem.serverProperties.serverVersion": "Version",
			"onprem.serverProperties.serverEdition": "Édition",
			"onprem.serverProperties.machineName": "Nom de l'ordinateur",
			"onprem.serverProperties.osVersion": "Version de système d'exploitation",
			"cloud.databaseProperties.azureEdition": "Édition",
			"cloud.databaseProperties.serviceLevelObjective": "Niveau tarifaire",
			"cloud.databaseProperties.compatibilityLevel": "Niveau de compatibilité",
			"cloud.databaseProperties.owner": "Propriétaire",
			"cloud.serverProperties.serverVersion": "Version",
			"cloud.serverProperties.serverEdition": "Type",
			"mssql.provider.displayName": "Microsoft SQL Server",
			"mssql.connectionOptions.connectionName.displayName": "Nom (facultatif)",
			"mssql.connectionOptions.connectionName.description": "Nom personnalisé de la connexion",
			"mssql.connectionOptions.serverName.displayName": "Serveur",
			"mssql.connectionOptions.serverName.description": "Nom de l'instance SQL Server",
			"mssql.connectionOptions.databaseName.displayName": "Base de données",
			"mssql.connectionOptions.databaseName.description": "Nom du catalogue ou de la base de données de départ dans la source de données",
			"mssql.connectionOptions.authType.displayName": "Type d'authentification",
			"mssql.connectionOptions.authType.description": "Spécifie la méthode d'authentification avec SQL Server",
			"mssql.connectionOptions.authType.categoryValues.sqlLogin": "Connexion SQL",
			"mssql.connectionOptions.authType.categoryValues.integrated": "Authentification Windows",
			"mssql.connectionOptions.authType.categoryValues.azureMFA": "Azure Active Directory - Authentification universelle avec prise en charge de MFA",
			"mssql.connectionOptions.userName.displayName": "Nom d'utilisateur",
			"mssql.connectionOptions.userName.description": "Indique l'identifiant utilisateur à utiliser pour la connexion à la source de données",
			"mssql.connectionOptions.password.displayName": "Mot de passe",
			"mssql.connectionOptions.password.description": "Indique le mot de passe à utiliser pour la connexion à la source de données",
			"mssql.connectionOptions.applicationIntent.displayName": "Intention d'application",
			"mssql.connectionOptions.applicationIntent.description": "Déclare le type de charge de travail de l'application pendant la connexion à un serveur",
			"mssql.connectionOptions.asynchronousProcessing.displayName": "Traitement asynchrone",
			"mssql.connectionOptions.asynchronousProcessing.description": "Quand la valeur est true, permet d'utiliser la fonctionnalité asynchrone dans le fournisseur de données .Net Framework",
			"mssql.connectionOptions.connectTimeout.displayName": "Délai d'expiration de la connexion",
			"mssql.connectionOptions.connectTimeout.description": "Durée d'attente (en secondes) d'une connexion au serveur avant de terminer la tentative et de générer une erreur",
			"mssql.connectionOptions.currentLanguage.displayName": "Langage actuel",
			"mssql.connectionOptions.currentLanguage.description": "Nom d'enregistrement de la langue de SQL Server",
			"mssql.connectionOptions.columnEncryptionSetting.displayName": "Chiffrement de colonne",
			"mssql.connectionOptions.columnEncryptionSetting.description": "Paramètre par défaut de chiffrement de colonne pour toutes les commandes sur la connexion",
			"mssql.connectionOptions.encrypt.displayName": "Chiffrer",
			"mssql.connectionOptions.encrypt.description": "Quand la valeur est true, SQL Server utilise le chiffrement SSL pour toutes les données envoyées entre le client et le serveur si le serveur a un certificat installé",
			"mssql.connectionOptions.persistSecurityInfo.displayName": "Conserver les informations de sécurité",
			"mssql.connectionOptions.persistSecurityInfo.description": "Quand la valeur est false, les informations de sécurité, comme le mot de passe, ne sont pas retournées dans le cadre de la connexion",
			"mssql.connectionOptions.trustServerCertificate.displayName": "Approuver le certificat de serveur",
			"mssql.connectionOptions.trustServerCertificate.description": "Quand la valeur est true (et encrypt=true), SQL Server utilise le chiffrement SSL pour toutes les données envoyées entre le client et le serveur sans valider le certificat de serveur",
			"mssql.connectionOptions.attachedDBFileName.displayName": "Nom du fichier de base de données attaché",
			"mssql.connectionOptions.attachedDBFileName.description": "Nom de fichier principal, y compris le nom de chemin complet, d'une base de données pouvant être attachée",
			"mssql.connectionOptions.contextConnection.displayName": "Connexion contextuelle",
			"mssql.connectionOptions.contextConnection.description": "Quand la valeur est true, indique que la connexion doit provenir du contexte du serveur SQL. Disponible uniquement en cas d'exécution dans le processus SQL Server",
			"mssql.connectionOptions.port.displayName": "Port",
			"mssql.connectionOptions.connectRetryCount.displayName": "Nombre de tentatives de connexion",
			"mssql.connectionOptions.connectRetryCount.description": "Nombre de tentatives de restauration de connexion",
			"mssql.connectionOptions.connectRetryInterval.displayName": "Intervalle entre les tentatives de connexion",
			"mssql.connectionOptions.connectRetryInterval.description": "Délai entre les tentatives de restauration de connexion",
			"mssql.connectionOptions.applicationName.displayName": "Nom de l'application",
			"mssql.connectionOptions.applicationName.description": "Nom de l'application",
			"mssql.connectionOptions.workstationId.displayName": "ID de station de travail",
			"mssql.connectionOptions.workstationId.description": "Nom de la station de travail se connectant à SQL Server",
			"mssql.connectionOptions.pooling.displayName": "Regroupement",
			"mssql.connectionOptions.pooling.description": "Quand la valeur est true, l'objet de connexion est tiré du pool approprié ou, si nécessaire, est créé et ajouté au pool approprié",
			"mssql.connectionOptions.maxPoolSize.displayName": "Taille maximale du pool",
			"mssql.connectionOptions.maxPoolSize.description": "Nombre maximal de connexions autorisées dans le pool",
			"mssql.connectionOptions.minPoolSize.displayName": "Taille minimale du pool",
			"mssql.connectionOptions.minPoolSize.description": "Nombre minimal de connexions autorisées dans le pool",
			"mssql.connectionOptions.loadBalanceTimeout.displayName": "Délai d'expiration de l'équilibrage de charge",
			"mssql.connectionOptions.loadBalanceTimeout.description": "Durée de vie minimale (en secondes) de cette connexion dans le pool avant d'être détruite",
			"mssql.connectionOptions.replication.displayName": "Réplication",
			"mssql.connectionOptions.replication.description": "Utilisé par SQL Server dans la réplication",
			"mssql.connectionOptions.attachDbFilename.displayName": "Attacher le nom de fichier de base de données",
			"mssql.connectionOptions.failoverPartner.displayName": "Partenaire de basculement",
			"mssql.connectionOptions.failoverPartner.description": "Nom ou adresse réseau de l'instance de SQL Server servant de partenaire de basculement",
			"mssql.connectionOptions.multiSubnetFailover.displayName": "Basculement de plusieurs sous-réseaux",
			"mssql.connectionOptions.multipleActiveResultSets.displayName": "MARS (Multiple Active Result Set)",
			"mssql.connectionOptions.multipleActiveResultSets.description": "Quand la valeur est true, plusieurs jeux de résultats peuvent être retournés et lus sur une même connexion",
			"mssql.connectionOptions.packetSize.displayName": "Taille de paquet",
			"mssql.connectionOptions.packetSize.description": "Taille en octets des paquets réseau utilisés pour communiquer avec une instance de SQL Server",
			"mssql.connectionOptions.typeSystemVersion.displayName": "Version du système de type",
			"mssql.connectionOptions.typeSystemVersion.description": "Indique le système de type serveur que le fournisseur expose par le biais de DataReader"
		},
		"dist/localizedConstants": {
			"msgMissingNodeContext": "Commande de nœud appelée sans aucun nœud",
			"mssql.manageAccessTitle": "Gérer l'accès",
			"mssql.locationTitle": "Emplacement : ",
			"mssql.permissionsTitle": "Autorisations",
			"mssql.ownerPostfix": " - Propriétaire",
			"mssql.owningGroupPostfix": " - Groupe propriétaire",
			"mssql.everyone": "Tous les autres",
			"mssql.userLabel": "Utilisateur",
			"mssql.groupLabel": "Groupe",
			"mssql.accessHeader": "Accès",
			"mssql.defaultHeader": "Par défaut",
			"mssql.delete": "Supprimer",
			"mssql.stickyHeader": "Épinglé",
			"mssql.inheritDefaultsLabel": "Hériter les valeurs par défaut",
			"mssql.readHeader": "Lire",
			"mssql.writeHeader": "Écrire",
			"mssql.executeHeader": "Exécuter",
			"mssql.addUserOrGroup": "Ajouter un utilisateur ou un groupe",
			"mssql.enterNamePlaceholder": "Entrer un nom",
			"mssql.addLabel": "Ajouter",
			"mssql.namedUsersAndGroups": "Utilisateurs et groupes nommés",
			"mssql.apply": "Appliquer",
			"mssql.applyRecursively": "Appliquer de façon récursive",
			"mssql.errorApplyingAclChanges": "Une erreur inattendue s'est produite pendant l'application des changements : {0}",
			"sparkJobSubmission_LocalFileDestinationHint": "Le fichier local est chargé dans HDFS. ",
			"sparkJobSubmission_SubmissionEndMessage": ".......................... Fin d'envoi du travail Spark ............................",
			"sparkJobSubmission_PrepareUploadingFile": "Chargement du fichier du {0} local vers le dossier HDFS : {1}",
			"sparkJobSubmission_UploadingFileSucceeded": "Le fichier a été chargé dans le cluster.",
			"sparkJobSubmission_UploadingFileFailed": "Le chargement du fichier dans le cluster a échoué. {0}",
			"sparkJobSubmission_PrepareSubmitJob": "Envoi du travail {0}... ",
			"sparkJobSubmission_SubmitJobFinished": "Le travail Spark a été envoyé.",
			"sparkJobSubmission_SubmitJobFailed": "L'envoi du travail Spark a échoué. {0} ",
			"sparkJobSubmission_YarnUIMessage": "URL de YarnUI : {0} ",
			"sparkJobSubmission_SparkHistoryLinkMessage": "URL de l'historique Spark : {0} ",
			"sparkJobSubmission_GetApplicationIdFailed": "L'obtention de l'ID d'application a échoué. {0}",
			"sparkJobSubmission_LocalFileNotExisted": "Le fichier local {0} n'existe pas. ",
			"sparkJobSubmission_NoSqlBigDataClusterFound": "Aucun cluster Big Data SQL Server."
		},
		"dist/objectExplorerNodeProvider/fileSources": {
			"maxSizeNotice": "REMARQUE : Ce fichier a été tronqué au niveau de {0} pour l'aperçu. ",
			"maxSizeReached": "Le fichier a été tronqué au niveau de {0} pour l'aperçu."
		},
		"dist/objectExplorerNodeProvider/command": {
			"progress": "$(sync~spin) {0}...",
			"cancelTooltip": "Annuler",
			"cancel": "Annuler l'opération ?",
			"mssql.searchServers": "Rechercher dans les noms de serveur"
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkJobSubmissionService": {
			"sparkJobSubmission_LivyNoBatchIdReturned": "Aucun ID de lot de travail Spark dans la réponse.{0}[Erreur] {1}",
			"sparkJobSubmission_LivyNoLogReturned": "Aucun journal dans la réponse.{0}[Erreur] {1}"
		},
		"dist/objectExplorerNodeProvider/hdfsCommands": {
			"allFiles": "Tous les fichiers",
			"lblUploadFiles": "Charger",
			"uploading": "Chargement des fichiers dans HDFS",
			"uploadCanceled": "L'opération de chargement a été annulée",
			"uploadError": "Erreur de chargement des fichiers : {0}",
			"makingDir": "Création du répertoire",
			"mkdirCanceled": "L'opération a été annulée",
			"mkDirError": "Erreur de création du répertoire : {0}",
			"enterDirName": "Entrer le nom du répertoire",
			"deleteError": "Erreur de suppression des fichiers : {0}",
			"msgDeleteFolder": "Voulez-vous vraiment supprimer ce dossier et son contenu ?",
			"msgDeleteFile": "Voulez-vous vraiment supprimer ce fichier ?",
			"saving": "Enregistrement des fichiers HDFS",
			"saveCanceled": "L'opération d'enregistrement a été annulée",
			"saveError": "Erreur d'enregistrement du fichier : {0}",
			"previewing": "Génération de l'aperçu",
			"previewError": "Erreur d'affichage de l'aperçu du fichier : {0}",
			"copyPathError": "Erreur de copie du chemin : {0}",
			"manageAccessError": "Une erreur inattendue s'est produite à l'ouverture de la boîte de dialogue Gérer l'accès : {0}"
		},
		"dist/hdfs/webhdfs": {
			"webhdfs.invalidDataStructure": "Structure de données non valide",
			"webhdfs.missingProperties": "Impossible de créer le client WebHDFS en raison d'options manquantes : ${0}",
			"webhdfs.undefinedArgument": "'${0}' n'est pas défini.",
			"webhdfs.httpError400": "Demande incorrecte",
			"webhdfs.httpError401": "Non autorisé",
			"webhdfs.httpError403": "Interdit",
			"webhdfs.httpError404": "Introuvable",
			"webhdfs.httpError500": "Erreur de serveur interne",
			"webhdfs.unknownError": "Erreur inconnue",
			"webhdfs.unexpectedRedirect": "Redirection inattendue"
		},
		"dist/objectExplorerNodeProvider/connection": {
			"connectionInfoUndefined": "ConnectionInfo n'est pas défini.",
			"connectionInfoOptionsUndefined": "ConnectionInfo.options n'est pas défini.",
			"connectionInfoOptionsMissingProperties": "Des propriétés sont manquantes dans connectionInfo.options : {0}"
		},
		"dist/telemetry": {
			"viewKnownIssuesText": "Voir les problèmes connus",
			"serviceCrashMessage": "Le composant {0} a quitté de manière inattendue. Redémarrez Azure Data Studio."
		},
		"dist/main": {
			"msgSampleCodeDataFrame": "Cet exemple de code charge le fichier dans un cadre de données et affiche les 10 premiers résultats.",
			"notebookFileType": "Notebooks",
			"unsupportedFileType": "Seuls les notebooks .ipynb sont pris en charge",
			"fileNotFound": "Fichier spécifié introuvable"
		},
		"dist/hdfs/hdfsModel": {
			"mssql.recursivePermissionOpStarted": "Application des changements d'autorisation de manière récursive sous '{0}'",
			"mssql.recursivePermissionOpSucceeded": "Les changements d'autorisation ont été appliqués.",
			"mssql.recursivePermissionOpProgress": "Application des changements d'autorisation à '{0}'.",
			"mssql.recursivePermissionOpError": "Erreur d'application des changements d'autorisation : {0}"
		},
		"dist/prompts/confirm": {
			"msgYes": "Oui",
			"msgNo": "Non"
		},
		"dist/sparkFeature/dialog/dialogCommands": {
			"selectOtherServer": "Sélectionner un autre serveur SQL",
			"sparkJobSubmission_PleaseSelectSqlWithCluster": "Sélectionnez SQL Server avec le cluster Big Data.",
			"sparkJobSubmission_NoSqlSelected": "Aucun serveur SQL sélectionné.",
			"errorNotSqlBigDataCluster": "Le serveur sélectionné n'appartient pas à un cluster Big Data SQL Server",
			"sparkJobSubmission_GetFilePathFromSelectedNodeFailed": "Erreur d'obtention du chemin de fichier : {0}"
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkJobSubmissionDialog": {
			"sparkJobSubmission_SparkJobSubmissionDialogInitializeError": "Les paramètres de SparkJobSubmissionDialog ne sont pas autorisés",
			"sparkJobSubmission_DialogTitleNewJob": "Nouveau travail",
			"sparkJobSubmission_DialogCancelButton": "Annuler",
			"sparkJobSubmission_DialogSubmitButton": "Envoyer",
			"sparkJobSubmission_SubmitSparkJob": "Envoi du travail Spark {0} :",
			"sparkJobSubmission_SubmissionStartMessage": ".......................... Début de l'envoi du travail Spark .........................."
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkJobSubmissionModel": {
			"sparkJobSubmission_SparkJobSubmissionModelInitializeError": "Les paramètres de SparkJobSubmissionModel ne sont pas autorisés",
			"sparkJobSubmission_submissionArgsIsInvalid": "soumissionArgs n'est pas valide. ",
			"sparkJobSubmission_LivyBatchIdIsInvalid": "livyBatchId n'est pas valide. ",
			"sparkJobSubmission_GetApplicationIdTimeOut": "Le délai d'obtention de l'ID d'application a expiré. {0}[Journal]   {1}",
			"sparkJobSubmission_localFileOrFolderNotSpecified.": "La propriété localeFilePath ou hdfsFolderPath n'est pas spécifiée. ",
			"sparkJobSubmission_PathNotSpecified.": "Le chemin de propriété n'est pas spécifié. "
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkConfigurationTab": {
			"sparkJobSubmission_GeneralTabName": "GÉNÉRAL",
			"sparkJobSubmission_JobNamePlaceHolder": "Entrer un nom...",
			"sparkJobSubmission_JobName": "Nom du travail",
			"sparkJobSubmission_SparkCluster": "Cluster Spark",
			"sparkJobSubmission_FilePathPlaceHolder": "Chemin d'un fichier .jar ou .py",
			"sparkJobSubmission_LocalFileDestinationHintWithPath": "Le fichier local sélectionné est chargé dans HDFS : {0}",
			"sparkJobSubmission_MainFilePath": "Fichier JAR/py",
			"sparkJobSubmission_MainClass": "Classe principale",
			"sparkJobSubmission_Arguments": "Arguments",
			"sparkJobSubmission_ArgumentsTooltip": "Arguments de ligne de commande utilisés dans votre classe principale, plusieurs arguments doivent être séparés par un espace.",
			"sparkJobSubmission_NotSpecifyJobName": "Le nom de travail de la propriété n'est pas spécifié.",
			"sparkJobSubmission_NotSpecifyJARPYPath": "Le fichier JAR/py de propriétés n'est pas spécifié.",
			"sparkJobSubmission_NotSpecifyMainClass": "La classe principale de la propriété n'est pas spécifiée.",
			"sparkJobSubmission_HDFSFileNotExistedWithPath": "{0} n'existe pas dans le cluster ou une exception est levée. ",
			"sparkJobSubmission_HDFSFileNotExisted": "Le fichier HDFS spécifié n'existe pas. ",
			"sparkSelectLocalFile": "Sélectionner",
			"sparkJobSubmission_SelectFileError": "Erreur de localisation du fichier en raison de l'erreur : {0}"
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkAdvancedTab": {
			"sparkJobSubmission_AdvancedTabName": "AVANCÉ",
			"sparkJobSubmission_ReferenceJarList": "Fichiers JAR de référence",
			"sparkJobSubmission_ReferenceJarListToolTip": "Fichiers JAR à placer dans le répertoire de travail de l'exécuteur. Le chemin de fichier JAR doit être un chemin HDFS. Plusieurs chemins doivent être séparés par un point-virgule (;)",
			"sparkJobSubmission_ReferencePyList": "Fichiers py de référence",
			"sparkJobSubmission_ReferencePyListTooltip": "Fichiers py à placer dans le répertoire de travail de l'exécuteur. Le chemin de fichier doit être un chemin HDFS. Plusieurs chemins doivent être séparés par un point-virgule (;)",
			"sparkJobSubmission_ReferenceFilesList": "Fichiers de référence",
			"sparkJobSubmission_ReferenceFilesListTooltip": "Fichiers à placer dans le répertoire de travail de l'exécuteur. Le chemin de fichier doit être un chemin HDFS. Plusieurs chemins doivent être séparés par un point-virgule (;)"
		},
		"dist/objectExplorerNodeProvider/objectExplorerNodeProvider": {
			"prmptPwd": "Fournissez le mot de passe de connexion à HDFS :",
			"sessionNotFound": "La session du nœud {0} n'existe pas",
			"notifyError": "Erreur de notification du changement de nœud : {0}",
			"hdfsFolder": "HDFS",
			"rootLabel": "Racine"
		},
		"dist/objectExplorerNodeProvider/hdfsProvider": {
			"errorExpanding": "Erreur : {0}",
			"errDeleteConnectionNode": "Impossible de supprimer une connexion. Seuls les sous-dossiers et les fichiers peuvent être supprimés."
		},
		"dist/objectExplorerNodeProvider/cancelableStream": {
			"streamCanceled": "Opération de flux annulée par l'utilisateur"
		},
		"dist/dashboard/serviceEndpoints": {
			"grafana": "Tableau de bord des métriques",
			"kibana": "Tableau de bord de recherche dans les journaux",
			"sparkHistory": "Tableau de bord de gestion et de supervision de travaux Spark",
			"yarnHistory": "Tableau de bord de diagnostic et de supervision Spark",
			"copyText": "Copier",
			"endpoint.appproxy": "Proxy d'application",
			"endpoint.controller": "Service de gestion de cluster",
			"endpoint.gateway": "Passerelle d'accès aux fichiers HDFS, à Spark",
			"endpoint.managementproxy": "Proxy de gestion",
			"endpoint.mgmtproxy": "Proxy de gestion",
			"endpoint.sqlServerEndpoint": "Front-end de l'instance maître SQL Server",
			"endpoint.grafana": "Tableau de bord des métriques",
			"endpoint.kibana": "Tableau de bord de recherche dans les journaux",
			"endpoint.yarnHistory": "Tableau de bord de diagnostic et de supervision Spark",
			"endpoint.sparkHistory": "Tableau de bord de gestion et de supervision de travaux Spark",
			"endpoint.webhdfs": "Proxy du système de fichiers HDFS",
			"endpoint.livy": "Proxy pour exécuter les instructions, travaux, applications Spark"
		},
		"dist/sqlToolsServer": {
			"serviceStartedStatusMsg": "{0} démarré",
			"startingServiceStatusMsg": "Démarrage de {0}",
			"failedToStartServiceErrorMsg": "Échec du démarrage de {0}",
			"installingServiceChannelMsg": "Installation de {0} sur {1}",
			"installingServiceStatusMsg": "Installation de {0}",
			"installedServiceChannelMsg": "{0} installé",
			"downloadingServiceChannelMsg": "Téléchargement de {0}",
			"downloadingServiceSizeChannelMsg": "({0} Ko)",
			"downloadingServiceStatusMsg": "Téléchargement de {0}",
			"downloadServiceDoneChannelMsg": "Installation de {0} effectuée"
		}
	}
}