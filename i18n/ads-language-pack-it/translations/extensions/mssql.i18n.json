{
	"": [
		"--------------------------------------------------------------------------------------------",
		"Copyright (c) Microsoft Corporation. All rights reserved.",
		"Licensed under the Source EULA. See License.txt in the project root for license information.",
		"--------------------------------------------------------------------------------------------",
		"Do not edit this file. It is machine generated."
	],
	"version": "1.0.0",
	"contents": {
		"package": {
			"json.schemas.desc": "Associa schemi a file JSON nel progetto corrente",
			"json.schemas.url.desc": "URL di uno schema o percorso relativo di uno schema nella directory corrente",
			"json.schemas.fileMatch.desc": "Matrice di criteri dei file da usare per la ricerca durante la risoluzione di file JSON in schemi.",
			"json.schemas.fileMatch.item.desc": "Criteri dei file che possono contenere '*' da usare per la ricerca durante la risoluzione di file JSON in schemi.",
			"json.schemas.schema.desc": "Definizione dello schema per l'URL specificato. È necessario specificare lo schema per evitare accessi all'URL dello schema.",
			"json.format.enable.desc": "Abilita/Disabilita il formattatore JSON predefinito (richiede il riavvio)",
			"mssqlCluster.uploadFiles": "Carica file",
			"mssqlCluster.mkdir": "Nuova directory",
			"mssqlCluster.deleteFiles": "Elimina",
			"mssqlCluster.previewFile": "Anteprima",
			"mssqlCluster.saveFile": "Salva",
			"mssqlCluster.copyPath": "Copia percorso",
			"mssqlCluster.manageAccess": "Gestisci accesso",
			"notebook.command.new": "Nuovo notebook",
			"notebook.command.open": "Apri notebook",
			"tab.bigDataClusterDescription": "Attività e informazioni sul cluster Big Data di SQL Server",
			"title.bigDataCluster": "Cluster Big Data di SQL Server",
			"title.submitSparkJob": "Invia processo Spark",
			"title.newSparkJob": "Nuovo processo Spark",
			"title.openSparkHistory": "Visualizza cronologia di Spark",
			"title.openYarnHistory": "Visualizza cronologia di YARN",
			"title.tasks": "Attività",
			"title.installPackages": "Installa pacchetti",
			"title.configurePython": "Configura Python per Notebooks",
			"title.searchServers": "Ricerca: Server",
			"title.clearSearchServerResult": "Ricerca: Cancella risultati del server di ricerca",
			"title.endpoints": "Endpoint servizio",
			"title.books": "Notebooks",
			"title.showLogFile": "Mostra file di log",
			"mssql.configuration.title": "Configurazione di MSSQL",
			"mssql.query.displayBitAsNumber": "Consente di indicare se le colonne di tipo BIT devono essere visualizzate come numeri (1 o 0). Se è 'false', verranno visualizzate come 'true' o 'false'",
			"mssql.format.alignColumnDefinitionsInColumns": "Consente di indicare se le definizioni di colonna devono essere allineate",
			"mssql.format.datatypeCasing": "Consente di indicare se ai tipi di dati deve essere applicata la formattazione in lettere MAIUSCOLE o minuscole oppure se non deve essere applicata alcuna formattazione",
			"mssql.format.keywordCasing": "Consente di indicare se alle parole chiave deve essere applicata la formattazione in lettere MAIUSCOLE o minuscole oppure se non deve essere applicata alcuna formattazione",
			"mssql.format.placeCommasBeforeNextStatement": "Consente di indicare se le virgole devono essere inserite all'inizio di ogni istruzione in un elenco, ad esempio ', mycolumn2', anziché alla fine, ad esempio 'mycolumn1,'?",
			"mssql.format.placeSelectStatementReferencesOnNewLine": "Consente di indicare se i riferimenti agli oggetti in istruzioni select devono essere suddivisi su righe diverse. Ad esempio per 'SELECT C1, C2 FROM T1' sia C1 che C2 saranno su righe diverse",
			"mssql.logDebugInfo": "[Facoltativa] Registrare l'output di debug nella console (Visualizza -> Output), quindi selezionare il canale di output appropriato dall'elenco a discesa",
			"mssql.tracingLevel": "[Facoltativa] Livello di registrazione per i servizi back-end. Azure Data Studio genera un nome file a ogni avvio e, se il file esiste già, le voci del log vengono accodate a tale file. Per la pulizia dei file di log meno recenti, vedere le impostazioni logRetentionMinutes e logFilesRemovalLimit. Con l'impostazione predefinita di tracingLevel, la quantità di dati registrata non è eccessiva. Se si cambia il livello di dettaglio, la registrazione potrebbe diventare eccessiva e richiedere un notevole spazio su disco per i log. Il livello Error include quello Critical, il livello Warning include quello Error, il livello Information include quello Warning e il livello Verbose include quello Information",
			"mssql.logRetentionMinutes": "Numero di minuti per la conservazione dei file di log per i servizi di back-end. L'impostazione predefinita è 1 settimana.",
			"mssql.logFilesRemovalLimit": "Numero massimo di file meno recenti da rimuovere all'avvio per cui è scaduto il tempo impostato con mssql.logRetentionMinutes. I file che non vengono rimossi a causa di questa limitazione verranno rimossi al successivo avvio di Azure Data Studio.",
			"ignorePlatformWarning": "[Facoltativa] Non visualizzare avvisi su piattaforme non supportate",
			"onprem.databaseProperties.recoveryModel": "Modello di recupero",
			"onprem.databaseProperties.lastBackupDate": "Ultimo backup del database",
			"onprem.databaseProperties.lastLogBackupDate": "Ultimo backup del log",
			"onprem.databaseProperties.compatibilityLevel": "Livello di compatibilità",
			"onprem.databaseProperties.owner": "Proprietario",
			"onprem.serverProperties.serverVersion": "Versione",
			"onprem.serverProperties.serverEdition": "Edizione",
			"onprem.serverProperties.machineName": "Nome del computer",
			"onprem.serverProperties.osVersion": "Versione del sistema operativo",
			"cloud.databaseProperties.azureEdition": "Edizione",
			"cloud.databaseProperties.serviceLevelObjective": "Piano tariffario",
			"cloud.databaseProperties.compatibilityLevel": "Livello di compatibilità",
			"cloud.databaseProperties.owner": "Proprietario",
			"cloud.serverProperties.serverVersion": "Versione",
			"cloud.serverProperties.serverEdition": "Tipo",
			"mssql.provider.displayName": "Microsoft SQL Server",
			"mssql.connectionOptions.connectionName.displayName": "Nome (facoltativo)",
			"mssql.connectionOptions.connectionName.description": "Nome personalizzato della connessione",
			"mssql.connectionOptions.serverName.displayName": "Server",
			"mssql.connectionOptions.serverName.description": "Nome dell'istanza di SQL Server",
			"mssql.connectionOptions.databaseName.displayName": "Database",
			"mssql.connectionOptions.databaseName.description": "Nome del database o del catalogo iniziale nell'origine dati",
			"mssql.connectionOptions.authType.displayName": "Tipo di autenticazione",
			"mssql.connectionOptions.authType.description": "Specifica il metodo di autenticazione con SQL Server",
			"mssql.connectionOptions.authType.categoryValues.sqlLogin": "Account di accesso SQL",
			"mssql.connectionOptions.authType.categoryValues.integrated": "Autenticazione di Windows",
			"mssql.connectionOptions.authType.categoryValues.azureMFA": "Azure Active Directory - Universale con supporto MFA",
			"mssql.connectionOptions.userName.displayName": "Nome utente",
			"mssql.connectionOptions.userName.description": "Indica l'ID utente da usare per la connessione all'origine dati",
			"mssql.connectionOptions.password.displayName": "Password",
			"mssql.connectionOptions.password.description": "Indica la password da usare per la connessione all'origine dati",
			"mssql.connectionOptions.applicationIntent.displayName": "Finalità dell'applicazione",
			"mssql.connectionOptions.applicationIntent.description": "Dichiara il tipo di carico di lavoro dell'applicazione durante la connessione a un server",
			"mssql.connectionOptions.asynchronousProcessing.displayName": "Elaborazione asincrona",
			"mssql.connectionOptions.asynchronousProcessing.description": "Se è true, consente l'utilizzo della funzionalità asincrona nel provider di dati .NET Framework.",
			"mssql.connectionOptions.connectTimeout.displayName": "Timeout di connessione",
			"mssql.connectionOptions.connectTimeout.description": "Intervallo di tempo (in secondi) in cui attendere la connessione al server prima di interrompere il tentativo e generare un errore",
			"mssql.connectionOptions.currentLanguage.displayName": "Lingua corrente",
			"mssql.connectionOptions.currentLanguage.description": "Nome del record di lingua di SQL Server",
			"mssql.connectionOptions.columnEncryptionSetting.displayName": "Crittografia di colonna",
			"mssql.connectionOptions.columnEncryptionSetting.description": "Impostazione di crittografia di colonna predefinita per tutti i comandi della connessione",
			"mssql.connectionOptions.encrypt.displayName": "Crittografa",
			"mssql.connectionOptions.encrypt.description": "Se è true, SQL Server usa la crittografia SSL per tutti i dati scambiati tra il client e il server, se nel server è installato un certificato",
			"mssql.connectionOptions.persistSecurityInfo.displayName": "Salva in modo permanente le informazioni di sicurezza",
			"mssql.connectionOptions.persistSecurityInfo.description": "Se è false, le informazioni sensibili dal punto di vista della sicurezza, come la password, non vengono restituite nell'ambito della connessione",
			"mssql.connectionOptions.trustServerCertificate.displayName": "Considera attendibile il certificato del server",
			"mssql.connectionOptions.trustServerCertificate.description": "Se è true (ed encrypt=true), SQL Server usa la crittografia SSL per tutti i dati inviati tra il client e il server senza convalidare il certificato del server",
			"mssql.connectionOptions.attachedDBFileName.displayName": "Nome file del database collegato",
			"mssql.connectionOptions.attachedDBFileName.description": "Nome del file primario, incluso il nome del percorso completo, di un database collegabile",
			"mssql.connectionOptions.contextConnection.displayName": "Connessione al contesto",
			"mssql.connectionOptions.contextConnection.description": "Se è true, indica che la connessione deve provenire dal contesto SQL Server. Disponibile solo quando è in esecuzione nel processo SQL Server",
			"mssql.connectionOptions.port.displayName": "Porta",
			"mssql.connectionOptions.connectRetryCount.displayName": "Conteggio tentativi di connessione",
			"mssql.connectionOptions.connectRetryCount.description": "Numero di tentativi di ripristino della connessione",
			"mssql.connectionOptions.connectRetryInterval.displayName": "Intervallo tentativi di connessione",
			"mssql.connectionOptions.connectRetryInterval.description": "Ritardo tra tentativi di ripristino della connessione",
			"mssql.connectionOptions.applicationName.displayName": "Nome dell'applicazione",
			"mssql.connectionOptions.applicationName.description": "Nome dell'applicazione",
			"mssql.connectionOptions.workstationId.displayName": "ID workstation",
			"mssql.connectionOptions.workstationId.description": "Nome della workstation che si connette a SQL Server",
			"mssql.connectionOptions.pooling.displayName": "Pooling",
			"mssql.connectionOptions.pooling.description": "Se è true, l'oggetto connessione viene prelevato dal pool appropriato oppure, se necessario, viene creato e aggiunto al pool appropriato",
			"mssql.connectionOptions.maxPoolSize.displayName": "Dimensioni massime del pool",
			"mssql.connectionOptions.maxPoolSize.description": "Numero massimo di connessioni consentite nel pool",
			"mssql.connectionOptions.minPoolSize.displayName": "Dimensioni minime del pool",
			"mssql.connectionOptions.minPoolSize.description": "Numero minimo di connessioni consentite nel pool",
			"mssql.connectionOptions.loadBalanceTimeout.displayName": "Timeout durante il bilanciamento del carico",
			"mssql.connectionOptions.loadBalanceTimeout.description": "Tempo minimo (in secondi) in cui la connessione rimane attiva nel pool prima di essere eliminata definitivamente",
			"mssql.connectionOptions.replication.displayName": "Replica",
			"mssql.connectionOptions.replication.description": "Usato da SQL Server nella replica",
			"mssql.connectionOptions.attachDbFilename.displayName": "Collega nome file del database",
			"mssql.connectionOptions.failoverPartner.displayName": "Partner di failover",
			"mssql.connectionOptions.failoverPartner.description": "Nome o indirizzo di rete dell'istanza di SQL Server che funge da partner di failover",
			"mssql.connectionOptions.multiSubnetFailover.displayName": "Failover su più subnet",
			"mssql.connectionOptions.multipleActiveResultSets.displayName": "Multiple Active Result Set",
			"mssql.connectionOptions.multipleActiveResultSets.description": "Se è true, possono essere restituiti e letti più set di risultati da un'unica connessione",
			"mssql.connectionOptions.packetSize.displayName": "Dimensioni del pacchetto",
			"mssql.connectionOptions.packetSize.description": "Dimensioni in byte dei pacchetti di rete usati per comunicare con un'istanza di SQL Server",
			"mssql.connectionOptions.typeSystemVersion.displayName": "Versione del sistema di tipi",
			"mssql.connectionOptions.typeSystemVersion.description": "Indica il sistema di tipi di server esposto dal provider tramite l'oggetto DataReader"
		},
		"dist/localizedConstants": {
			"msgMissingNodeContext": "Il comando di Node è stato chiamato senza passare alcun nodo",
			"mssql.manageAccessTitle": "Gestisci accesso",
			"mssql.locationTitle": "Percorso : ",
			"mssql.permissionsTitle": "Autorizzazioni",
			"mssql.ownerPostfix": " - Proprietario",
			"mssql.owningGroupPostfix": " - Gruppo proprietario",
			"mssql.everyone": "Tutti gli altri",
			"mssql.userLabel": "Utente",
			"mssql.groupLabel": "Gruppo",
			"mssql.accessHeader": "Accesso",
			"mssql.defaultHeader": "Predefinito",
			"mssql.delete": "Elimina",
			"mssql.stickyHeader": "Permanente",
			"mssql.inheritDefaultsLabel": "Impostazioni predefinite di ereditarietà",
			"mssql.readHeader": "Lettura",
			"mssql.writeHeader": "Scrittura",
			"mssql.executeHeader": "Esecuzione",
			"mssql.addUserOrGroup": "Aggiungi utente o gruppo",
			"mssql.enterNamePlaceholder": "Immettere il nome",
			"mssql.addLabel": "Aggiungi",
			"mssql.namedUsersAndGroups": "Utenti e gruppi non anonimi",
			"mssql.apply": "Applica",
			"mssql.applyRecursively": "Applica in modo ricorsivo",
			"mssql.errorApplyingAclChanges": "Si è verificato un errore imprevisto durante l'applicazione delle modifiche: {0}",
			"sparkJobSubmission_LocalFileDestinationHint": "Il file locale verrà caricato in HDFS. ",
			"sparkJobSubmission_SubmissionEndMessage": ".......................... Invia processo Spark - Fine ..........................",
			"sparkJobSubmission_PrepareUploadingFile": "Caricamento del file dalla cartella locale {0} alla cartella HDFS: {1}",
			"sparkJobSubmission_UploadingFileSucceeded": "Il caricamento del file nel cluster è riuscito.",
			"sparkJobSubmission_UploadingFileFailed": "Il caricamento del file nel cluster non è riuscito. {0}",
			"sparkJobSubmission_PrepareSubmitJob": "Invio del processo {0}... ",
			"sparkJobSubmission_SubmitJobFinished": "Il processo Spark è stato inviato.",
			"sparkJobSubmission_SubmitJobFailed": "L'invio del processo Spark non è riuscito. {0} ",
			"sparkJobSubmission_YarnUIMessage": "URL dell'interfaccia utente di YARN: {0} ",
			"sparkJobSubmission_SparkHistoryLinkMessage": "URL della cronologia di Spark: {0} ",
			"sparkJobSubmission_GetApplicationIdFailed": "Il recupero dell'ID applicazione non è riuscito. {0}",
			"sparkJobSubmission_LocalFileNotExisted": "Il file locale {0} non esiste. ",
			"sparkJobSubmission_NoSqlBigDataClusterFound": "Non è stato trovato alcun cluster Big Data di SQL Server."
		},
		"dist/objectExplorerNodeProvider/fileSources": {
			"maxSizeNotice": "AVVISO: questo file è stato troncato alla posizione {0} per l'anteprima. ",
			"maxSizeReached": "Il file è stato troncato alla posizione {0} per l'anteprima."
		},
		"dist/objectExplorerNodeProvider/command": {
			"progress": "$(sync~spin) {0}...",
			"cancelTooltip": "Annulla",
			"cancel": "Annullare l'operazione?",
			"mssql.searchServers": "Nomi dei server di ricerca"
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkJobSubmissionService": {
			"sparkJobSubmission_LivyNoBatchIdReturned": "La risposta non ha restituito alcun ID batch di processo Spark.{0}[Errore] {1}",
			"sparkJobSubmission_LivyNoLogReturned": "Nella risposta non è stato restituito alcun log.{0}[Errore] {1}"
		},
		"dist/objectExplorerNodeProvider/hdfsCommands": {
			"allFiles": "Tutti i file",
			"lblUploadFiles": "Carica",
			"uploading": "Caricamento di file in HDFS",
			"uploadCanceled": "L'operazione di caricamento è stata annullata",
			"uploadError": "Si è verificato un errore durante il caricamento dei file: {0}",
			"makingDir": "Creazione della directory",
			"mkdirCanceled": "L'operazione è stata annullata",
			"mkDirError": "Si è verificato un errore durante la creazione della directory: {0}",
			"enterDirName": "Immettere il nome della directory",
			"deleteError": "Si è verificato un errore durante l'eliminazione dei file: {0}",
			"msgDeleteFolder": "Eliminare questa cartella e il relativo contenuto?",
			"msgDeleteFile": "Eliminare questo file?",
			"saving": "Salvataggio dei file HDFS",
			"saveCanceled": "L'operazione di salvataggio è stata annullata",
			"saveError": "Si è verificato un errore durante il salvataggio del file: {0}",
			"previewing": "Generazione dell'anteprima",
			"previewError": "Si è verificato un errore durante l'anteprima del file: {0}",
			"copyPathError": "Si è verificato un errore durante la copia del percorso: {0}",
			"manageAccessError": "Si è verificato un errore imprevisto durante l'apertura della finestra Gestisci accesso: {0}"
		},
		"dist/hdfs/webhdfs": {
			"webhdfs.invalidDataStructure": "Struttura dei dati non valida",
			"webhdfs.missingProperties": "Non è possibile creare il client WebHDFS a causa di opzioni mancanti: ${0}",
			"webhdfs.undefinedArgument": "'${0}' non è definito.",
			"webhdfs.httpError400": "Richiesta non valida",
			"webhdfs.httpError401": "Non autorizzato",
			"webhdfs.httpError403": "Accesso negato",
			"webhdfs.httpError404": "Non trovato",
			"webhdfs.httpError500": "Errore interno del server",
			"webhdfs.unknownError": "Errore sconosciuto",
			"webhdfs.unexpectedRedirect": "Reindirizzamento imprevisto"
		},
		"dist/objectExplorerNodeProvider/connection": {
			"connectionInfoUndefined": "ConnectionInfo non è definito.",
			"connectionInfoOptionsUndefined": "ConnectionInfo.options non è definito.",
			"connectionInfoOptionsMissingProperties": "In connectionInfo.options mancano alcune proprietà: {0}"
		},
		"dist/telemetry": {
			"viewKnownIssuesText": "Visualizza problemi noti",
			"serviceCrashMessage": "Il componente {0} è stato chiuso in modo imprevisto. Riavviare Azure Data Studio."
		},
		"dist/main": {
			"msgSampleCodeDataFrame": "Questo codice di esempio consente di caricare il file in un frame di dati e visualizzare i primi 10 risultati.",
			"notebookFileType": "Notebooks",
			"unsupportedFileType": "Sono supportati solo notebook con estensione ipynb",
			"fileNotFound": "Non è possibile trovare il file specificato"
		},
		"dist/hdfs/hdfsModel": {
			"mssql.recursivePermissionOpStarted": "Applicazione delle modifiche delle autorizzazioni in modo ricorsivo in '{0}'",
			"mssql.recursivePermissionOpSucceeded": "Le modifiche delle autorizzazioni sono state applicate.",
			"mssql.recursivePermissionOpProgress": "Applicazione delle modifiche delle autorizzazioni a '{0}'.",
			"mssql.recursivePermissionOpError": "Si è verificato un errore durante l'applicazione delle modifiche delle autorizzazioni: {0}"
		},
		"dist/prompts/confirm": {
			"msgYes": "Sì",
			"msgNo": "No"
		},
		"dist/sparkFeature/dialog/dialogCommands": {
			"selectOtherServer": "Seleziona un'altra istanza di SQL Server",
			"sparkJobSubmission_PleaseSelectSqlWithCluster": "Selezionare SQL Server con il cluster Big Data.",
			"sparkJobSubmission_NoSqlSelected": "Non è stata selezionata alcuna istanza di SQL Server.",
			"errorNotSqlBigDataCluster": "Il server selezionato non appartiene a un cluster Big Data di SQL Server",
			"sparkJobSubmission_GetFilePathFromSelectedNodeFailed": "Si è verificato un errore durante il recupero del percorso del file: {0}"
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkJobSubmissionDialog": {
			"sparkJobSubmission_SparkJobSubmissionDialogInitializeError": "I parametri per SparkJobSubmissionDialog non sono validi",
			"sparkJobSubmission_DialogTitleNewJob": "Nuovo processo",
			"sparkJobSubmission_DialogCancelButton": "Annulla",
			"sparkJobSubmission_DialogSubmitButton": "Invia",
			"sparkJobSubmission_SubmitSparkJob": "Invio processo Spark {0}:",
			"sparkJobSubmission_SubmissionStartMessage": ".......................... Invia processo Spark - Inizio .........................."
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkJobSubmissionModel": {
			"sparkJobSubmission_SparkJobSubmissionModelInitializeError": "I parametri per SparkJobSubmissionModel non sono validi",
			"sparkJobSubmission_submissionArgsIsInvalid": "submissionArgs non è valido. ",
			"sparkJobSubmission_LivyBatchIdIsInvalid": "livyBatchId non è valido. ",
			"sparkJobSubmission_GetApplicationIdTimeOut": "Timeout durante il recupero dell'ID applicazione. {0}[Log]   {1}",
			"sparkJobSubmission_localFileOrFolderNotSpecified.": "La proprietà localFilePath o hdfsFolderPath non è specificata. ",
			"sparkJobSubmission_PathNotSpecified.": "Il percorso proprietà non è specificato. "
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkConfigurationTab": {
			"sparkJobSubmission_GeneralTabName": "GENERALE",
			"sparkJobSubmission_JobNamePlaceHolder": "Immetti un nome...",
			"sparkJobSubmission_JobName": "Nome del processo",
			"sparkJobSubmission_SparkCluster": "Cluster Spark",
			"sparkJobSubmission_FilePathPlaceHolder": "Percorso di un file con estensione jar o py",
			"sparkJobSubmission_LocalFileDestinationHintWithPath": "Il file locale selezionato verrà caricato in HDFS: {0}",
			"sparkJobSubmission_MainFilePath": "File JAR/py",
			"sparkJobSubmission_MainClass": "Classe principale",
			"sparkJobSubmission_Arguments": "Argomenti",
			"sparkJobSubmission_ArgumentsTooltip": "Argomenti della riga di comando usati nella classe principale. Separare più argomenti con uno spazio.",
			"sparkJobSubmission_NotSpecifyJobName": "Il nome del processo della proprietà non è specificato.",
			"sparkJobSubmission_NotSpecifyJARPYPath": "Il file JAR/py della proprietà non è specificato.",
			"sparkJobSubmission_NotSpecifyMainClass": "La classe principale della proprietà non è specificata.",
			"sparkJobSubmission_HDFSFileNotExistedWithPath": "{0} non esiste nel cluster oppure è stata generata un'eccezione. ",
			"sparkJobSubmission_HDFSFileNotExisted": "Il file HDFS specificato non esiste. ",
			"sparkSelectLocalFile": "Seleziona",
			"sparkJobSubmission_SelectFileError": "Si è verificato un errore durante l'individuazione del file. Errore: {0}"
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkAdvancedTab": {
			"sparkJobSubmission_AdvancedTabName": "AVANZATE",
			"sparkJobSubmission_ReferenceJarList": "File JAR di riferimento",
			"sparkJobSubmission_ReferenceJarListToolTip": "File con estensione jar da inserire nella directory di lavoro dell'executor. Il percorso dei file deve essere di tipo HDFS. Separare più percorsi con punti e virgola (;)",
			"sparkJobSubmission_ReferencePyList": "File di riferimento py",
			"sparkJobSubmission_ReferencePyListTooltip": "File con estensione py da inserire nella directory di lavoro dell'executor. Il percorso dei file deve essere di tipo HDFS. Separare più percorsi con punti e virgola (;)",
			"sparkJobSubmission_ReferenceFilesList": "File di riferimento",
			"sparkJobSubmission_ReferenceFilesListTooltip": "File da inserire nella directory di lavoro dell'executor. Il percorso dei file deve essere di tipo HDFS. Separare più percorsi con punti e virgola (;)"
		},
		"dist/objectExplorerNodeProvider/objectExplorerNodeProvider": {
			"prmptPwd": "Specificare la password per la connessione a HDFS:",
			"sessionNotFound": "La sessione per il nodo {0} non esiste",
			"notifyError": "Si è verificato un errore durante la notifica della modifica del nodo: {0}",
			"hdfsFolder": "HDFS",
			"rootLabel": "Radice"
		},
		"dist/objectExplorerNodeProvider/hdfsProvider": {
			"errorExpanding": "Errore: {0}",
			"errDeleteConnectionNode": "Non è possibile eliminare una connessione. È possibile eliminare solo sottocartelle e file."
		},
		"dist/objectExplorerNodeProvider/cancelableStream": {
			"streamCanceled": "Operazione di flusso annullata dall'utente"
		},
		"dist/dashboard/serviceEndpoints": {
			"grafana": "Dashboard di metriche",
			"kibana": "Dashboard di ricerca log",
			"sparkHistory": "Dashboard di gestione processi e monitoraggio di Spark",
			"yarnHistory": "Dashboard di diagnostica e monitoraggio di Spark",
			"copyText": "Copia",
			"endpoint.appproxy": "Proxy dell'applicazione",
			"endpoint.controller": "Servizio di gestione cluster",
			"endpoint.gateway": "Gateway per l'accesso ai file HDFS, Spark",
			"endpoint.managementproxy": "Proxy di gestione",
			"endpoint.mgmtproxy": "Proxy di gestione",
			"endpoint.sqlServerEndpoint": "Front-end dell'istanza master di SQL Server",
			"endpoint.grafana": "Dashboard di metriche",
			"endpoint.kibana": "Dashboard di ricerca log",
			"endpoint.yarnHistory": "Dashboard di diagnostica e monitoraggio di Spark",
			"endpoint.sparkHistory": "Dashboard di gestione processi e monitoraggio di Spark",
			"endpoint.webhdfs": "Proxy del file system HDFS",
			"endpoint.livy": "Proxy per l'esecuzione di istruzioni, processi e applicazioni Spark"
		},
		"dist/sqlToolsServer": {
			"serviceStartedStatusMsg": "{0} avviato",
			"startingServiceStatusMsg": "Avvio di {0}",
			"failedToStartServiceErrorMsg": "Non è stato possibile avviare {0}",
			"installingServiceChannelMsg": "Installazione di {0} in {1}",
			"installingServiceStatusMsg": "Installazione di {0}",
			"installedServiceChannelMsg": "{0} installato",
			"downloadingServiceChannelMsg": "Download di {0}",
			"downloadingServiceSizeChannelMsg": "({0} KB)",
			"downloadingServiceStatusMsg": "Download di {0}",
			"downloadServiceDoneChannelMsg": "Installazione di {0} completata"
		}
	}
}