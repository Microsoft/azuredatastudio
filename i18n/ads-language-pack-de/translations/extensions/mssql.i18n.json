{
	"": [
		"--------------------------------------------------------------------------------------------",
		"Copyright (c) Microsoft Corporation. All rights reserved.",
		"Licensed under the Source EULA. See License.txt in the project root for license information.",
		"--------------------------------------------------------------------------------------------",
		"Do not edit this file. It is machine generated."
	],
	"version": "1.0.0",
	"contents": {
		"package": {
			"json.schemas.desc": "Hiermit werden Schemas zu JSON-Dateien im aktuellen Projekt zugeordnet.",
			"json.schemas.url.desc": "Eine URL zu einem Schema oder ein relativer Pfad zu einem Schema im aktuellen Verzeichnis",
			"json.schemas.fileMatch.desc": "Ein Array aus Dateimustern zum Abgleich beim Auflösen von JSON-Dateien in Schemas",
			"json.schemas.fileMatch.item.desc": "Ein Dateimuster, das \"*\" enthalten kann, zum Abgleich beim Auflösen von JSON-Dateien in Schemas",
			"json.schemas.schema.desc": "Die Schemadefinition für die angegebene URL. Das Schema muss nur angegeben werden, um Zugriffe auf die Schema-URL zu vermeiden.",
			"json.format.enable.desc": "Standard-JSON-Formatierer aktivieren/deaktivieren (Neustart erforderlich)",
			"mssqlCluster.uploadFiles": "Dateien hochladen",
			"mssqlCluster.mkdir": "Neues Verzeichnis",
			"mssqlCluster.deleteFiles": "Löschen",
			"mssqlCluster.previewFile": "Vorschau",
			"mssqlCluster.saveFile": "Speichern",
			"mssqlCluster.copyPath": "Pfad kopieren",
			"mssqlCluster.manageAccess": "Zugriff verwalten",
			"notebook.command.new": "Neues Notebook",
			"notebook.command.open": "Notebook öffnen",
			"tab.bigDataClusterDescription": "Aufgaben und Informationen zu Ihrem SQL Server-Big Data-Cluster",
			"title.bigDataCluster": "SQL Server-Big Data-Cluster",
			"title.submitSparkJob": "Spark-Auftrag übermitteln",
			"title.newSparkJob": "Neuer Spark-Auftrag",
			"title.openSparkHistory": "Spark-Verlauf anzeigen",
			"title.openYarnHistory": "Yarn-Verlauf anzeigen",
			"title.tasks": "Aufgaben",
			"title.installPackages": "Pakete installieren",
			"title.configurePython": "Python für Notebooks konfigurieren",
			"title.searchServers": "Suche: Server",
			"title.clearSearchServerResult": "Suche: Suchserverergebnisse löschen",
			"title.endpoints": "Dienstendpunkte",
			"title.books": "Notebooks",
			"title.showLogFile": "Protokolldatei anzeigen",
			"mssql.configuration.title": "MSSQL-Konfiguration",
			"mssql.query.displayBitAsNumber": "BIT-Spalten als Zahlen (1 oder 0) anzeigen? Bei Festlegung auf FALSE werden BIT-Spalten als TRUE oder FALSE angezeigt.",
			"mssql.format.alignColumnDefinitionsInColumns": "Sollen Spaltendefinitionen ausgerichtet werden?",
			"mssql.format.datatypeCasing": "Gibt an, ob Datentypen in Großbuchstaben, Kleinbuchstaben oder gar nicht formatiert werden sollen.",
			"mssql.format.keywordCasing": "Gibt an, ob Schlüsselwörter in Großbuchstaben, Kleinbuchstaben oder gar nicht formatiert werden sollen.",
			"mssql.format.placeCommasBeforeNextStatement": "Gibt an, dass Kommas in einer Liste am Anfang der einzelnen Anweisungen (z. B. \", mycolumn2\") und nicht am Ende platziert werden sollen: \"mycolumn1,\"",
			"mssql.format.placeSelectStatementReferencesOnNewLine": "Sollen Verweise auf Objekte in einer SELECT-Anweisung in separaten Zeilen angezeigt werden? Beispielsweise werden bei \"SELECT C1, C2 FROM T1\" C1 und C2 jeweils in separaten Zeilen angezeigt.",
			"mssql.logDebugInfo": "[Optional] Protokollieren Sie die Debugausgabe in der Konsole (Ansicht > Ausgabe), und wählen Sie dann in der Dropdownliste den geeigneten Ausgabekanal aus.",
			"mssql.tracingLevel": "[Optional] Protokolliergrad für Back-End-Dienste. Azure Data Studio generiert bei jedem Start einen Dateinamen, und falls die Datei bereits vorhanden ist, werden die Protokolleinträge an diese Datei angehängt. Zur Bereinigung alter Protokolldateien können die Einstellungen \"logRetentionMinutes\" und \"logFilesRemovalLimit\" verwendet werden. Bei Verwendung des Standardwerts für \"tracingLevel\" werden nur wenige Informationen protokolliert. Eine Änderung der Ausführlichkeit kann zu einem umfangreichen Protokollierungsaufkommen und einem hohen Speicherplatzbedarf für die Protokolle führen. \"Error\" umfasst kritische Meldungen, \"Warning\" umfasst alle Daten aus \"Error\" sowie Warnmeldungen, \"Information\" umfasst alle Daten aus \"Warning\" sowie Informationsmeldungen, \"Verbose\" umfasst ausführliche Informationen.",
			"mssql.logRetentionMinutes": "Anzahl von Minuten, für die Protokolldateien für Back-End-Dienste aufbewahrt werden sollen. Der Standardwert ist 1 Woche.",
			"mssql.logFilesRemovalLimit": "Die maximale Anzahl alter Dateien, die beim Start entfernt werden sollen, bei denen der mssql.logRetentionMinutes-Wert abgelaufen ist. Dateien, die aufgrund dieser Einschränkung nicht bereinigt werden, werden beim nächsten Start von Azure Data Studio bereinigt.",
			"ignorePlatformWarning": "[Optional] Keine Anzeige von Warnungen zu nicht unterstützten Plattformen.",
			"onprem.databaseProperties.recoveryModel": "Wiederherstellungsmodell",
			"onprem.databaseProperties.lastBackupDate": "Letzte Datenbanksicherung",
			"onprem.databaseProperties.lastLogBackupDate": "Letzte Protokollsicherung",
			"onprem.databaseProperties.compatibilityLevel": "Kompatibilitätsgrad",
			"onprem.databaseProperties.owner": "Besitzer",
			"onprem.serverProperties.serverVersion": "Version",
			"onprem.serverProperties.serverEdition": "Edition",
			"onprem.serverProperties.machineName": "Computername",
			"onprem.serverProperties.osVersion": "Betriebssystemversion",
			"cloud.databaseProperties.azureEdition": "Edition",
			"cloud.databaseProperties.serviceLevelObjective": "Tarif",
			"cloud.databaseProperties.compatibilityLevel": "Kompatibilitätsgrad",
			"cloud.databaseProperties.owner": "Besitzer",
			"cloud.serverProperties.serverVersion": "Version",
			"cloud.serverProperties.serverEdition": "Typ",
			"mssql.provider.displayName": "Microsoft SQL Server",
			"mssql.connectionOptions.connectionName.displayName": "Name (optional)",
			"mssql.connectionOptions.connectionName.description": "Benutzerdefinierter Name der Verbindung",
			"mssql.connectionOptions.serverName.displayName": "Server",
			"mssql.connectionOptions.serverName.description": "Name der SQL Server-Instanz",
			"mssql.connectionOptions.databaseName.displayName": "Datenbank",
			"mssql.connectionOptions.databaseName.description": "Der Name des anfänglichen Katalogs oder der ersten Datenbank in der Datenquelle",
			"mssql.connectionOptions.authType.displayName": "Authentifizierungstyp",
			"mssql.connectionOptions.authType.description": "Gibt die Methode für die Authentifizierung bei SQL Server an.",
			"mssql.connectionOptions.authType.categoryValues.sqlLogin": "SQL-Anmeldung",
			"mssql.connectionOptions.authType.categoryValues.integrated": "Windows-Authentifizierung",
			"mssql.connectionOptions.authType.categoryValues.azureMFA": "Azure Active Directory: universell mit MFA-Unterstützung",
			"mssql.connectionOptions.userName.displayName": "Benutzername",
			"mssql.connectionOptions.userName.description": "Gibt die Benutzer-ID an, die beim Herstellen einer Verbindung mit der Datenquelle verwendet werden soll.",
			"mssql.connectionOptions.password.displayName": "Kennwort",
			"mssql.connectionOptions.password.description": "Gibt das Kennwort an, das beim Herstellen einer Verbindung mit der Datenquelle verwendet werden soll.",
			"mssql.connectionOptions.applicationIntent.displayName": "Anwendungszweck",
			"mssql.connectionOptions.applicationIntent.description": "Deklariert den Anwendungsauslastungstyp beim Herstellen einer Verbindung mit einem Server.",
			"mssql.connectionOptions.asynchronousProcessing.displayName": "Asynchrone Verarbeitung",
			"mssql.connectionOptions.asynchronousProcessing.description": "Bei Festlegung auf TRUE wird die Verwendung der asynchronen Verarbeitung im .NET Framework-Datenanbieter ermöglicht.",
			"mssql.connectionOptions.connectTimeout.displayName": "Verbindungstimeout",
			"mssql.connectionOptions.connectTimeout.description": "Die Zeitspanne (in Sekunden), die auf eine Verbindung mit dem Server gewartet wird, bevor der Versuch beendet und ein Fehler generiert wird.",
			"mssql.connectionOptions.currentLanguage.displayName": "Aktuelle Sprache",
			"mssql.connectionOptions.currentLanguage.description": "Der Datensatzname der SQL Server-Sprache",
			"mssql.connectionOptions.columnEncryptionSetting.displayName": "Spaltenverschlüsselung",
			"mssql.connectionOptions.columnEncryptionSetting.description": "Die Standardeinstellung für die Spaltenverschlüsselung für alle Befehle in der Verbindung",
			"mssql.connectionOptions.encrypt.displayName": "Verschlüsseln",
			"mssql.connectionOptions.encrypt.description": "Bei Festlegung auf TRUE verwendet SQL Server die SSL-Verschlüsselung für alle zwischen Client und Server gesendeten Daten, sofern auf dem Server ein Zertifikat installiert ist.",
			"mssql.connectionOptions.persistSecurityInfo.displayName": "Sicherheitsinformationen dauerhaft speichern",
			"mssql.connectionOptions.persistSecurityInfo.description": "Bei Festlegung auf FALSE werden sicherheitsrelevante Informationen, z. B. das Kennwort, nicht als Teil der Verbindung zurückgegeben.",
			"mssql.connectionOptions.trustServerCertificate.displayName": "Serverzertifikat vertrauen",
			"mssql.connectionOptions.trustServerCertificate.description": "Bei Festlegung auf TRUE (und encrypt=true) verwendet SQL Server die SSL-Verschlüsselung für alle zwischen Client und Server gesendeten Daten, ohne das Serverzertifikat zu überprüfen.",
			"mssql.connectionOptions.attachedDBFileName.displayName": "Dateiname der angefügten Datenbank",
			"mssql.connectionOptions.attachedDBFileName.description": "Der Name der primären Datei einer anfügbaren Datenbank, einschließlich des vollständigen Pfadnamens.",
			"mssql.connectionOptions.contextConnection.displayName": "Kontextverbindung",
			"mssql.connectionOptions.contextConnection.description": "Bei Festlegung auf TRUE muss die Verbindung aus dem SQL-Serverkontext stammen. Nur verfügbar bei Ausführung im SQL Server-Prozess.",
			"mssql.connectionOptions.port.displayName": "Port",
			"mssql.connectionOptions.connectRetryCount.displayName": "Anzahl der Verbindungswiederholungen",
			"mssql.connectionOptions.connectRetryCount.description": "Anzahl der Versuche zur Verbindungswiederherstellung",
			"mssql.connectionOptions.connectRetryInterval.displayName": "Intervall für Verbindungswiederholung",
			"mssql.connectionOptions.connectRetryInterval.description": "Verzögerung zwischen Versuchen zur Verbindungswiederherstellung",
			"mssql.connectionOptions.applicationName.displayName": "Anwendungsname",
			"mssql.connectionOptions.applicationName.description": "Der Name der Anwendung",
			"mssql.connectionOptions.workstationId.displayName": "Arbeitsstations-ID",
			"mssql.connectionOptions.workstationId.description": "Der Name der Arbeitsstation, die eine Verbindung mit SQL Server herstellt",
			"mssql.connectionOptions.pooling.displayName": "Pooling",
			"mssql.connectionOptions.pooling.description": "Bei Festlegung auf TRUE wird das Verbindungsobjekt aus dem geeigneten Pool abgerufen oder bei Bedarf erstellt und dem geeigneten Pool hinzugefügt.",
			"mssql.connectionOptions.maxPoolSize.displayName": "Maximale Poolgröße",
			"mssql.connectionOptions.maxPoolSize.description": "Die maximal zulässige Anzahl von Verbindungen im Pool",
			"mssql.connectionOptions.minPoolSize.displayName": "Minimale Poolgröße",
			"mssql.connectionOptions.minPoolSize.description": "Die mindestens erforderliche Anzahl von Verbindungen im Pool",
			"mssql.connectionOptions.loadBalanceTimeout.displayName": "Timeout für Lastenausgleich",
			"mssql.connectionOptions.loadBalanceTimeout.description": "Die Mindestzeitspanne (in Sekunden), für die diese Verbindung im Pool verbleiben soll, bevor sie zerstört wird",
			"mssql.connectionOptions.replication.displayName": "Replikation",
			"mssql.connectionOptions.replication.description": "Wird von SQL Server bei der Replikation verwendet.",
			"mssql.connectionOptions.attachDbFilename.displayName": "Dateiname der anzufügenden Datenbank",
			"mssql.connectionOptions.failoverPartner.displayName": "Failoverpartner",
			"mssql.connectionOptions.failoverPartner.description": "Der Name oder die Netzwerkadresse der SQL Server-Instanz, die als Failoverpartner fungiert",
			"mssql.connectionOptions.multiSubnetFailover.displayName": "Multisubnetzfailover",
			"mssql.connectionOptions.multipleActiveResultSets.displayName": "Mehrere aktive Resultsets",
			"mssql.connectionOptions.multipleActiveResultSets.description": "Bei Festlegung auf TRUE können mehrere Resultsets zurückgegeben und aus einer Verbindung gelesen werden.",
			"mssql.connectionOptions.packetSize.displayName": "Paketgröße",
			"mssql.connectionOptions.packetSize.description": "Größe der Netzwerkpakete (in Byte), die bei der Kommunikation mit einer Instanz von SQL Server verwendet werden",
			"mssql.connectionOptions.typeSystemVersion.displayName": "Typsystemversion",
			"mssql.connectionOptions.typeSystemVersion.description": "Gibt an, welches Servertypsystem der Anbieter über den DataReader verfügbar macht."
		},
		"dist/localizedConstants": {
			"msgMissingNodeContext": "Es wurde ein Knotenbefehl aufgerufen, ohne dass ein Knoten übergeben wurde.",
			"mssql.manageAccessTitle": "Zugriff verwalten",
			"mssql.locationTitle": "Speicherort: ",
			"mssql.permissionsTitle": "Berechtigungen",
			"mssql.ownerPostfix": "– Besitzer",
			"mssql.owningGroupPostfix": "– Besitzergruppe",
			"mssql.everyone": "Beliebige andere Person",
			"mssql.userLabel": "Benutzer",
			"mssql.groupLabel": "Gruppe",
			"mssql.accessHeader": "Zugriff",
			"mssql.defaultHeader": "Standard",
			"mssql.delete": "Löschen",
			"mssql.stickyHeader": "Fixiert",
			"mssql.inheritDefaultsLabel": "Standardwerte erben",
			"mssql.readHeader": "Lesen",
			"mssql.writeHeader": "Schreiben",
			"mssql.executeHeader": "Ausführen",
			"mssql.addUserOrGroup": "Benutzer oder Gruppe hinzufügen",
			"mssql.enterNamePlaceholder": "Namen eingeben",
			"mssql.addLabel": "Hinzufügen",
			"mssql.namedUsersAndGroups": "Benannte Benutzer und Gruppen",
			"mssql.apply": "Anwenden",
			"mssql.applyRecursively": "Rekursiv anwenden",
			"mssql.errorApplyingAclChanges": "Unerwarteter Fehler beim Anwenden von Änderungen: {0}",
			"sparkJobSubmission_LocalFileDestinationHint": "Lokale Datei wird in HDFS hochgeladen. ",
			"sparkJobSubmission_SubmissionEndMessage": ".......................... Ende der Spark-Auftragsübermittlung ............................",
			"sparkJobSubmission_PrepareUploadingFile": "Die Datei wird aus dem lokalen Ordner \"{0}\" in den HDFS-Ordner hochgeladen: {1}",
			"sparkJobSubmission_UploadingFileSucceeded": "Die Datei wurde erfolgreich in den Cluster hochgeladen.",
			"sparkJobSubmission_UploadingFileFailed": "Fehler beim Hochladen der Datei in den Cluster. {0}",
			"sparkJobSubmission_PrepareSubmitJob": "Der Auftrag \"{0}\" wird übermittelt... ",
			"sparkJobSubmission_SubmitJobFinished": "Der Spark-Auftrag wurde übermittelt.",
			"sparkJobSubmission_SubmitJobFailed": "Fehler bei der Spark-Auftragsübermittlung. {0} ",
			"sparkJobSubmission_YarnUIMessage": "YarnUI-URL: {0} ",
			"sparkJobSubmission_SparkHistoryLinkMessage": "Spark-Verlaufs-URL: {0} ",
			"sparkJobSubmission_GetApplicationIdFailed": "Fehler beim Abrufen der Anwendungs-ID. {0}",
			"sparkJobSubmission_LocalFileNotExisted": "Die lokale Datei \"{0}\" ist nicht vorhanden. ",
			"sparkJobSubmission_NoSqlBigDataClusterFound": "Es wurde kein SQL Server-Big Data-Cluster gefunden."
		},
		"dist/objectExplorerNodeProvider/fileSources": {
			"maxSizeNotice": "HINWEIS: Diese Datei wurde zur Vorschau bei \"{0}\" abgeschnitten. ",
			"maxSizeReached": "Die Datei wurde zur Vorschau bei \"{0}\" abgeschnitten."
		},
		"dist/objectExplorerNodeProvider/command": {
			"progress": "$(sync~spin) {0}...",
			"cancelTooltip": "Abbrechen",
			"cancel": "Vorgang abbrechen?",
			"mssql.searchServers": "Servernamen suchen"
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkJobSubmissionService": {
			"sparkJobSubmission_LivyNoBatchIdReturned": "In der Antwort wurde keine Batch-ID für Spark-Aufträge zurückgegeben.{0}[Fehler] {1}",
			"sparkJobSubmission_LivyNoLogReturned": "Innerhalb der Antwort wird kein Protokoll zurückgegeben.{0}[Fehler] {1}"
		},
		"dist/objectExplorerNodeProvider/hdfsCommands": {
			"allFiles": "Alle Dateien",
			"lblUploadFiles": "Hochladen",
			"uploading": "Dateien werden in HDFS hochgeladen",
			"uploadCanceled": "Der Uploadvorgang wurde abgebrochen.",
			"uploadError": "Fehler beim Hochladen von Dateien: {0}",
			"makingDir": "Das Verzeichnis wird erstellt.",
			"mkdirCanceled": "Der Vorgang wurde abgebrochen.",
			"mkDirError": "Fehler beim Erstellen des Verzeichnisses: {0}",
			"enterDirName": "Verzeichnisnamen eingeben",
			"deleteError": "Fehler beim Löschen von Dateien: {0}",
			"msgDeleteFolder": "Möchten Sie diesen Ordner und den zugehörigen Inhalt löschen?",
			"msgDeleteFile": "Möchten Sie diese Datei löschen?",
			"saving": "Die HDFS-Dateien werden gespeichert.",
			"saveCanceled": "Der Speichervorgang wurde abgebrochen.",
			"saveError": "Fehler beim Speichern der Datei: {0}",
			"previewing": "Die Vorschau wird generiert.",
			"previewError": "Fehler bei der Vorschau der Datei: {0}",
			"copyPathError": "Fehler beim Kopieren des Pfads: {0}",
			"manageAccessError": "Unerwarteter Fehler beim Öffnen des Dialogfelds \"Zugriff verwalten\": {0}"
		},
		"dist/hdfs/webhdfs": {
			"webhdfs.invalidDataStructure": "Ungültige Datenstruktur.",
			"webhdfs.missingProperties": "Der WebHDFS-Client kann aufgrund von fehlenden Optionen nicht erstellt werden: ${0}",
			"webhdfs.undefinedArgument": "\"${0}\" ist nicht definiert.",
			"webhdfs.httpError400": "Fehlerhafte Anforderung.",
			"webhdfs.httpError401": "Nicht autorisiert",
			"webhdfs.httpError403": "Unzulässig",
			"webhdfs.httpError404": "Nicht gefunden",
			"webhdfs.httpError500": "Interner Serverfehler",
			"webhdfs.unknownError": "Unbekannter Fehler",
			"webhdfs.unexpectedRedirect": "Unerwartete Umleitung"
		},
		"dist/objectExplorerNodeProvider/connection": {
			"connectionInfoUndefined": "\"ConnectionInfo\" ist nicht definiert.",
			"connectionInfoOptionsUndefined": "\"ConnectionInfo.options\" ist nicht definiert.",
			"connectionInfoOptionsMissingProperties": "In \"connectionInfo.options\" fehlen einige Eigenschaften: {0}"
		},
		"dist/telemetry": {
			"viewKnownIssuesText": "Bekannte Probleme anzeigen",
			"serviceCrashMessage": "Die Komponente \"{0}\" wurde unerwartet beendet. Starten Sie Azure Data Studio neu."
		},
		"dist/main": {
			"msgSampleCodeDataFrame": "Dieser Beispielcode lädt die Datei in einen Datenrahmen und zeigt die ersten 10 Ergebnisse an.",
			"notebookFileType": "Notebooks",
			"unsupportedFileType": "Es werden nur IPYNB-Notebooks unterstützt.",
			"fileNotFound": "Die angegebene Datei wurde nicht gefunden."
		},
		"dist/hdfs/hdfsModel": {
			"mssql.recursivePermissionOpStarted": "Die Berechtigungsänderungen werden unter \"{0}\" rekursiv angewendet.",
			"mssql.recursivePermissionOpSucceeded": "Die Berechtigungsänderungen wurden erfolgreich angewendet.",
			"mssql.recursivePermissionOpProgress": "Die Berechtigungsänderungen werden auf \"{0}\" angewendet.",
			"mssql.recursivePermissionOpError": "Fehler beim Anwenden von Berechtigungsänderungen: {0}"
		},
		"dist/prompts/confirm": {
			"msgYes": "Ja",
			"msgNo": "Nein"
		},
		"dist/sparkFeature/dialog/dialogCommands": {
			"selectOtherServer": "Andere SQL Server-Instanz auswählen",
			"sparkJobSubmission_PleaseSelectSqlWithCluster": "Wählen Sie SQL Server mit Big Data-Cluster aus.",
			"sparkJobSubmission_NoSqlSelected": "Es ist keine SQL Server-Instanz ausgewählt.",
			"errorNotSqlBigDataCluster": "Der ausgewählte Server gehört nicht zu einem SQL Server-Big Data-Cluster.",
			"sparkJobSubmission_GetFilePathFromSelectedNodeFailed": "Fehler beim Abrufen des Dateipfads: {0}"
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkJobSubmissionDialog": {
			"sparkJobSubmission_SparkJobSubmissionDialogInitializeError": "Die Parameter für \"SparkJobSubmissionDialog\" sind ungültig.",
			"sparkJobSubmission_DialogTitleNewJob": "Neuer Auftrag",
			"sparkJobSubmission_DialogCancelButton": "Abbrechen",
			"sparkJobSubmission_DialogSubmitButton": "Übermitteln",
			"sparkJobSubmission_SubmitSparkJob": "{0} Spark-Auftragsübermittlung:",
			"sparkJobSubmission_SubmissionStartMessage": ".......................... Start der Spark-Auftragsübermittlung .........................."
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkJobSubmissionModel": {
			"sparkJobSubmission_SparkJobSubmissionModelInitializeError": "Die Parameter für \"SparkJobSubmissionModel\" sind ungültig.",
			"sparkJobSubmission_submissionArgsIsInvalid": "\"submissionArgs\" ist ungültig. ",
			"sparkJobSubmission_LivyBatchIdIsInvalid": "\"livyBatchId\" ist ungültig. ",
			"sparkJobSubmission_GetApplicationIdTimeOut": "Timeout beim Abrufen der Anwendungs-ID. {0}[Protokoll]   {1}",
			"sparkJobSubmission_localFileOrFolderNotSpecified.": "Die localFilePath- oder hdfsFolderPath-Eigenschaft wurde nicht angegeben. ",
			"sparkJobSubmission_PathNotSpecified.": "Der Eigenschaftspfad wurde nicht angegeben. "
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkConfigurationTab": {
			"sparkJobSubmission_GeneralTabName": "ALLGEMEIN",
			"sparkJobSubmission_JobNamePlaceHolder": "Namen eingeben...",
			"sparkJobSubmission_JobName": "Auftragsname",
			"sparkJobSubmission_SparkCluster": "Spark-Cluster",
			"sparkJobSubmission_FilePathPlaceHolder": "Pfad zu einer JAR- oder PY-Datei",
			"sparkJobSubmission_LocalFileDestinationHintWithPath": "Die ausgewählte lokale Datei wird in HDFS hochgeladen: {0}",
			"sparkJobSubmission_MainFilePath": "JAR-/PY-Datei",
			"sparkJobSubmission_MainClass": "Hauptklasse",
			"sparkJobSubmission_Arguments": "Argumente",
			"sparkJobSubmission_ArgumentsTooltip": "Befehlszeilenargumente, die in Ihrer Hauptklasse verwendet werden. Mehrere Argumente müssen durch Leerzeichen voneinander getrennt werden.",
			"sparkJobSubmission_NotSpecifyJobName": "Der Eigenschaftsauftragsname wurde nicht angegeben.",
			"sparkJobSubmission_NotSpecifyJARPYPath": "Die JAR-/PY-Eigenschaftsdatei wurde nicht angegeben.",
			"sparkJobSubmission_NotSpecifyMainClass": "Die Hauptklasse der Eigenschaft wurde nicht angegeben.",
			"sparkJobSubmission_HDFSFileNotExistedWithPath": "\"{0}\" ist nicht im Cluster vorhanden, oder es wurde eine Ausnahme ausgelöst. ",
			"sparkJobSubmission_HDFSFileNotExisted": "Die angegebene HDFS-Datei ist nicht vorhanden. ",
			"sparkSelectLocalFile": "Auswählen",
			"sparkJobSubmission_SelectFileError": "Fehler beim Suchen der Datei: {0}"
		},
		"dist/sparkFeature/dialog/sparkJobSubmission/sparkAdvancedTab": {
			"sparkJobSubmission_AdvancedTabName": "ERWEITERT",
			"sparkJobSubmission_ReferenceJarList": "JAR-Referenzdateien",
			"sparkJobSubmission_ReferenceJarListToolTip": "JAR-Dateien, die im Executor-Arbeitsverzeichnis platziert werden sollen. Der JAR-Pfad muss ein HDFS-Pfad sein. Mehrere Pfade müssen durch ein Semikolon (;) voneinander getrennt werden.",
			"sparkJobSubmission_ReferencePyList": "PY-Referenzdateien",
			"sparkJobSubmission_ReferencePyListTooltip": "PY-Dateien, die im Executor-Arbeitsverzeichnis platziert werden sollen. Der Dateipfad muss ein HDFS-Pfad sein. Mehrere Pfade müssen durch ein Semikolon (;) voneinander getrennt werden.",
			"sparkJobSubmission_ReferenceFilesList": "Referenzdateien",
			"sparkJobSubmission_ReferenceFilesListTooltip": "Dateien, die im Executor-Arbeitsverzeichnis platziert werden sollen. Der Dateipfad muss ein HDFS-Pfad sein. Mehrere Pfade müssen durch ein Semikolon (;) voneinander getrennt werden."
		},
		"dist/objectExplorerNodeProvider/objectExplorerNodeProvider": {
			"prmptPwd": "Geben Sie das Kennwort für die Verbindung mit HDFS an:",
			"sessionNotFound": "Die Sitzung für den Knoten \"{0}\" ist nicht vorhanden.",
			"notifyError": "Fehler bei Benachrichtigung über Knotenänderung: {0}",
			"hdfsFolder": "HDFS",
			"rootLabel": "Stamm"
		},
		"dist/objectExplorerNodeProvider/hdfsProvider": {
			"errorExpanding": "Fehler: {0}",
			"errDeleteConnectionNode": "Eine Verbindung kann nicht gelöscht werden. Nur Unterordner und Dateien können gelöscht werden."
		},
		"dist/objectExplorerNodeProvider/cancelableStream": {
			"streamCanceled": "Der Streamvorgang wurde vom Benutzer abgebrochen."
		},
		"dist/dashboard/serviceEndpoints": {
			"grafana": "Metrikdashboard",
			"kibana": "Dashboard für Protokollsuche",
			"sparkHistory": "Dashboard zum Verwalten und Überwachen von Spark-Aufträgen",
			"yarnHistory": "Dashboard zur Spark-Diagnose und -Überwachung",
			"copyText": "Kopieren",
			"endpoint.appproxy": "Anwendungsproxy",
			"endpoint.controller": "Clusterverwaltungsdienst",
			"endpoint.gateway": "Gateway für den Zugriff auf HDFS-Dateien, Spark",
			"endpoint.managementproxy": "Verwaltungsproxy",
			"endpoint.mgmtproxy": "Verwaltungsproxy",
			"endpoint.sqlServerEndpoint": "Front-End der SQL Server-Masterinstanz",
			"endpoint.grafana": "Metrikdashboard",
			"endpoint.kibana": "Dashboard für Protokollsuche",
			"endpoint.yarnHistory": "Dashboard zur Spark-Diagnose und -Überwachung",
			"endpoint.sparkHistory": "Dashboard zum Verwalten und Überwachen von Spark-Aufträgen",
			"endpoint.webhdfs": "HDFS-Dateisystemproxy",
			"endpoint.livy": "Proxy zum Ausführen von Spark-Anweisungen, -Aufträgen und -Anwendungen"
		},
		"dist/sqlToolsServer": {
			"serviceStartedStatusMsg": "\"{0}\" wurde gestartet.",
			"startingServiceStatusMsg": "\"{0}\" wird gestartet.",
			"failedToStartServiceErrorMsg": "Fehler beim Starten von \"{0}\".",
			"installingServiceChannelMsg": "\"{0}\" wird in \"{1}\" installiert.",
			"installingServiceStatusMsg": "\"{0}\" wird installiert.",
			"installedServiceChannelMsg": "\"{0}\" wurde installiert.",
			"downloadingServiceChannelMsg": "\"{0}\" wird heruntergeladen.",
			"downloadingServiceSizeChannelMsg": "({0} KB)",
			"downloadingServiceStatusMsg": "\"{0}\" wird heruntergeladen.",
			"downloadServiceDoneChannelMsg": "Die Installation von {0} wurde abgeschlossen."
		}
	}
}