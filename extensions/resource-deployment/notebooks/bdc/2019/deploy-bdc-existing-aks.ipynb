{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python",
            "version": "3.7.3",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "![Microsoft](https://raw.githubusercontent.com/microsoft/azuredatastudio/master/src/sql/media/microsoft-small-logo.png)\n",
                " \n",
                "## Deploy SQL Server 2019 Big Data Cluster on an existing Azure Kubernetes Service (AKS) cluster\n",
                " \n",
                "This notebook walks through the process of deploying a <a href=\"https://docs.microsoft.com/sql/big-data-cluster/big-data-cluster-overview?view=sqlallproducts-allversions\">SQL Server 2019 Big Data Cluster</a> on an existing AKS cluster.\n",
                " \n",
                "* Follow the instructions in the **Prerequisites** cell to install the tools if not already installed.\n",
                "* Make sure you have the target cluster set as the current context in your kubectl config file.\n",
                "        The config file would typically be under C:\\Users\\(userid)\\.kube on Windows, and under ~/.kube/ for macOS and Linux for a default installation.\n",
                "        In the kubectl config file, look for \"current-context\" and ensure it is set to the AKS cluster that the SQL Server 2019 Big Data Cluster will be deployed to.\n",
                "* The **Required information** cell will prompt you for password that will be used to access the cluster controller, SQL Server, and Knox.\n",
                "* The values in the **Default settings** cell can be changed as appropriate.\n",
                "\n",
                "<span style=\"color:red\"><font size=\"3\">Please press the \"Run Cells\" button to run the notebook</font></span>"
            ],
            "metadata": {
                "azdata_cell_guid": "82e60c1a-7acf-47ee-877f-9e85e92e11da"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Prerequisites** \n",
                "Ensure the following tools are installed and added to PATH before proceeding.\n",
                " \n",
                "|Tools|Description|Installation|\n",
                "|---|---|---|\n",
                "|kubectl | Command-line tool for monitoring the underlying Kuberentes cluster | [Installation](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-binary-using-native-package-management) |\n",
                "|azdata | Command-line tool for installing and managing a Big Data Cluster |[Installation](https://docs.microsoft.com/en-us/sql/big-data-cluster/deploy-install-azdata?view=sqlallproducts-allversions) |"
            ],
            "metadata": {
                "azdata_cell_guid": "714582b9-10ee-409e-ab12-15a4825c9471"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Check dependencies**"
            ],
            "metadata": {
                "azdata_cell_guid": "e3dd8e75-e15f-44b4-81fc-1f54d6f0b1e2"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import pandas,sys,os,json,html,getpass,time\r\n",
                "pandas_version = pandas.__version__.split('.')\r\n",
                "pandas_major = int(pandas_version[0])\r\n",
                "pandas_minor = int(pandas_version[1])\r\n",
                "pandas_patch = int(pandas_version[2])\r\n",
                "if not (pandas_major > 0 or (pandas_major == 0 and pandas_minor > 24) or (pandas_major == 0 and pandas_minor == 24 and pandas_patch >= 2)):\r\n",
                "    sys.exit('Please upgrade the Notebook dependency before you can proceed, you can do it by running the \"Reinstall Notebook dependencies\" command in command palette (View menu -> Command Paletteâ€¦).')\r\n",
                "def run_command(command):\r\n",
                "    print(\"Executing: \" + command)\r\n",
                "    !{command}\r\n",
                "    if _exit_code != 0:\r\n",
                "        sys.exit(f'Command execution failed with exit code: {str(_exit_code)}.\\n\\t{command}\\n')\r\n",
                "    print(f'Successfully executed: {command}')\r\n",
                "\r\n",
                "run_command('kubectl version --client=true')\r\n",
                "run_command('azdata --version')"
            ],
            "metadata": {
                "azdata_cell_guid": "d973d5b4-7f0a-4a9d-b204-a16480f3940d"
            },
            "outputs": [],
            "execution_count": 1
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Required information**"
            ],
            "metadata": {
                "azdata_cell_guid": "0bb02e76-fee8-4dbc-a75b-d5b9d1b187d0"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "invoked_by_wizard = \"AZDATA_NB_VAR_BDC_ADMIN_PASSWORD\" in os.environ\n",
                "if not invoked_by_wizard:\n",
                "    mssql_password = getpass.getpass(prompt = 'SQL Server 2019 Big Data Cluster controller password')\n",
                "    if mssql_password == \"\":\n",
                "        sys.exit(f'Password is required.')\n",
                "    confirm_password = getpass.getpass(prompt = 'Confirm password')\n",
                "    if mssql_password != confirm_password:\n",
                "        sys.exit(f'Passwords do not match.')\n",
                "\n",
                "print('You can also use the controller password to access Knox and SQL Server.')"
            ],
            "metadata": {
                "azdata_cell_guid": "e7e10828-6cae-45af-8c2f-1484b6d4f9ac"
            },
            "outputs": [],
            "execution_count": 3
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Default settings**"
            ],
            "metadata": {
                "azdata_cell_guid": "c009edfe-b964-4b28-beeb-02a2c65f9918"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "if invoked_by_wizard:\n",
                "    mssql_cluster_name = os.environ[\"AZDATA_NB_VAR_BDC_CLUSTER_NAME\"]\n",
                "    mssql_controller_username = os.environ[\"AZDATA_NB_VAR_BDC_CONTROLLER_USERNAME\"]\n",
                "    mssql_password = os.environ[\"AZDATA_NB_VAR_BDC_ADMIN_PASSWORD\"]\n",
                "    mssql_source_profile = os.environ[\"AZDATA_NB_VAR_BDC_DEPLOYMENT_PROFILE\"]\n",
                "    mssql_kube_config_path = os.environ[\"AZDATA_NB_VAR_BDC_KUBECONFIG_PATH\"]\n",
                "    mssql_cluster_context = os.environ[\"AZDATA_NB_VAR_BDC_CLUSTER_CONTEXT\"]\n",
                "    mssql_sqlserver_scale = int(os.environ[\"AZDATA_NB_VAR_BDC_SQLSERVER_SCALE\"])\n",
                "    mssql_compute_scale = int(os.environ[\"AZDATA_NB_VAR_BDC_COMPUTEPOOL_SCALE\"])\n",
                "    mssql_data_scale = int(os.environ[\"AZDATA_NB_VAR_BDC_DATAPOOL_SCALE\"])\n",
                "    mssql_hdfs_scale = int(os.environ[\"AZDATA_NB_VAR_BDC_HDFSPOOL_SCALE\"])\n",
                "    mssql_spark_scale = int(os.environ[\"AZDATA_NB_VAR_BDC_SPARKPOOL_SCALE\"])\n",
                "    mssql_name_node_scale = int(os.environ[\"AZDATA_NB_VAR_BDC_NAMENODE_SCALE\"])\n",
                "    mssql_include_spark = os.environ[\"AZDATA_NB_VAR_BDC_INCLUDESPARK\"] == \"true\"\n",
                "    mssql_controller_port = int(os.environ[\"AZDATA_NB_VAR_BDC_CONTROLLER_PORT\"])\n",
                "    mssql_sqlserver_port = int(os.environ[\"AZDATA_NB_VAR_BDC_SQL_PORT\"])\n",
                "    mssql_gateway_port = int(os.environ[\"AZDATA_NB_VAR_BDC_GATEWAY_PORT\"])\n",
                "    mssql_readable_secondary_port = os.environ[\"AZDATA_NB_VAR_BDC_READABLE_SECONDARY_PORT\"]\n",
                "    mssql_controller_data_storage_class = os.environ[\"AZDATA_NB_VAR_BDC_CONTROLLER_DATA_STORAGE_CLASS\"]\n",
                "    mssql_controller_data_size = int(os.environ[\"AZDATA_NB_VAR_BDC_CONTROLLER_DATA_STORAGE_SIZE\"])\n",
                "    mssql_controller_logs_storage_class = os.environ[\"AZDATA_NB_VAR_BDC_CONTROLLER_LOGS_STORAGE_CLASS\"]\n",
                "    mssql_controller_logs_size = int(os.environ[\"AZDATA_NB_VAR_BDC_CONTROLLER_LOGS_STORAGE_SIZE\"])\n",
                "    mssql_datapool_data_storage_class = os.environ[\"AZDATA_NB_VAR_BDC_DATA_DATA_STORAGE_CLASS\"]\n",
                "    mssql_datapool_data_size = int(os.environ[\"AZDATA_NB_VAR_BDC_DATA_DATA_STORAGE_SIZE\"])\n",
                "    mssql_datapool_logs_storage_class = os.environ[\"AZDATA_NB_VAR_BDC_DATA_LOGS_STORAGE_CLASS\"]\n",
                "    mssql_datapool_logs_size = int(os.environ[\"AZDATA_NB_VAR_BDC_DATA_LOGS_STORAGE_SIZE\"])\n",
                "    mssql_hdfs_data_storage_class = os.environ[\"AZDATA_NB_VAR_BDC_HDFS_DATA_STORAGE_CLASS\"]\n",
                "    mssql_hdfs_data_size = int(os.environ[\"AZDATA_NB_VAR_BDC_HDFS_DATA_STORAGE_SIZE\"])\n",
                "    mssql_hdfs_logs_storage_class = os.environ[\"AZDATA_NB_VAR_BDC_HDFS_LOGS_STORAGE_CLASS\"]\n",
                "    mssql_hdfs_logs_size = int(os.environ[\"AZDATA_NB_VAR_BDC_HDFS_LOGS_STORAGE_SIZE\"])\n",
                "    mssql_sql_data_storage_class = os.environ[\"AZDATA_NB_VAR_BDC_SQL_DATA_STORAGE_CLASS\"]\n",
                "    mssql_sql_data_size = int(os.environ[\"AZDATA_NB_VAR_BDC_SQL_DATA_STORAGE_SIZE\"])\n",
                "    mssql_sql_logs_storage_class = os.environ[\"AZDATA_NB_VAR_BDC_SQL_LOGS_STORAGE_CLASS\"]\n",
                "    mssql_sql_logs_size = int(os.environ[\"AZDATA_NB_VAR_BDC_SQL_LOGS_STORAGE_SIZE\"])\n",
                "    mssql_hadr_enabled = os.environ[\"AZDATA_NB_VAR_BDC_ENABLE_HADR\"] == \"true\"\n",
                "    os.environ[\"KUBECONFIG\"] = mssql_kube_config_path\n",
                "else:\n",
                "    mssql_source_profile = 'aks-dev-test'\n",
                "    mssql_cluster_name = 'mssql-cluster'\n",
                "    mssql_controller_username = 'admin'\n",
                "\n",
                "mssql_target_profile = 'ads-bdc-custom-profile'\n",
                "print(f'SQL Server Big Data Cluster name: {mssql_cluster_name}')\n",
                "print(f'SQL Server Big Data Cluster controller username: {mssql_controller_username}')\n",
                "print(f'Deployment source profile: {mssql_source_profile}')\n",
                "print(f'Deployment profile: {mssql_target_profile}')\n",
                "if invoked_by_wizard:\n",
                "    print(f'kube config path: {mssql_kube_config_path}')\n",
                "    print(f'Cluster context: {mssql_cluster_context}')\n",
                "    print(f'SQL Server Master scale: {mssql_sqlserver_scale}')\n",
                "    print(f'Enable Availability Groups: {mssql_hadr_enabled}')\n",
                "    print(f'Compute pool scale: {mssql_compute_scale}')\n",
                "    print(f'HDFS pool scale: {mssql_hdfs_scale}')\n",
                "    print(f'Include Spark in HDFS pool: {mssql_include_spark}')\n",
                "    print(f'Data pool scale: {mssql_data_scale}')\n",
                "    print(f'Spark pool scale: {mssql_spark_scale}')\n",
                "    print(f'HDFS name node scale: {mssql_name_node_scale}')\n",
                "    print(f'Controller port: {mssql_controller_port}')\n",
                "    print(f'SQL Server port: {mssql_sqlserver_port}')\n",
                "    print(f'Gateway port: {mssql_gateway_port}')\n",
                "    if mssql_readable_secondary_port != '':\n",
                "        print(f'Readable secondary port: {mssql_readable_secondary_port}')\n",
                "    print(f'Controller data storage class name: {mssql_controller_data_storage_class}')\n",
                "    print(f'Controller logs storage class name: {mssql_controller_logs_storage_class}')\n",
                "    print(f'Controller data storage size(GB): {mssql_controller_data_size}')\n",
                "    print(f'Controller logs storage size(GB): {mssql_controller_logs_size}')\n",
                "    print(f'Data pool data storage class name: {mssql_datapool_data_storage_class}')\n",
                "    print(f'Data pool logs storage class name: {mssql_datapool_logs_storage_class}')\n",
                "    print(f'Data pool data storage size(GB): {mssql_datapool_data_size}')\n",
                "    print(f'Data pool logs storage size(GB): {mssql_datapool_logs_size}')\n",
                "    print(f'HDFS data storage class name: {mssql_hdfs_data_storage_class}')\n",
                "    print(f'HDFS logs storage class name: {mssql_hdfs_logs_storage_class}')\n",
                "    print(f'HDFS data storage size(GB): {mssql_hdfs_data_size}')\n",
                "    print(f'HDFS logs storage size(GB): {mssql_hdfs_logs_size}')\n",
                "    print(f'SQL Server Master data storage class name: {mssql_sql_data_storage_class}')\n",
                "    print(f'SQL Server Master logs storage class name: {mssql_sql_logs_storage_class}')\n",
                "    print(f'SQL Server Master data storage size(GB): {mssql_sql_data_size}')\n",
                "    print(f'SQL Server Master logs storage size(GB): {mssql_sql_logs_size}')"
            ],
            "metadata": {
                "azdata_cell_guid": "cc8e7295-b338-4af5-9471-8a5912675665"
            },
            "outputs": [],
            "execution_count": 4
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Set and show current context**"
            ],
            "metadata": {
                "azdata_cell_guid": "127c8042-181f-4862-a390-96e59c181d09"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "if invoked_by_wizard:\n",
                "    run_command(f'kubectl config use-context {mssql_cluster_context}')\n",
                "run_command('kubectl config current-context')"
            ],
            "metadata": {
                "azdata_cell_guid": "7d1a03d4-1df8-48eb-bff0-0042603b95b1"
            },
            "outputs": [],
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Create a deployment configuration file**"
            ],
            "metadata": {
                "azdata_cell_guid": "138536c3-1db6-428f-9e5c-8269a02fb52e"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "os.environ[\"ACCEPT_EULA\"] = 'yes'\n",
                "run_command(f'azdata bdc config init --source {mssql_source_profile} --target {mssql_target_profile} --force')\n",
                "if invoked_by_wizard:\n",
                "    run_command(f'azdata bdc config replace -c {mssql_target_profile}/bdc.json -j \"$.spec.resources.gateway.spec.endpoints[?(@.name==\"\"Knox\"\")].port={mssql_gateway_port}\"')\n",
                "    run_command(f'azdata bdc config replace -c {mssql_target_profile}/control.json -j \"$.spec.endpoints[?(@.name==\"\"Controller\"\")].port={mssql_controller_port}\"')\n",
                "    run_command(f'azdata bdc config replace -c {mssql_target_profile}/control.json -j $.spec.storage.data.className={mssql_controller_data_storage_class}')\n",
                "    run_command(f'azdata bdc config replace -c {mssql_target_profile}/control.json -j $.spec.storage.data.size={mssql_controller_data_size}Gi')\n",
                "    run_command(f'azdata bdc config replace -c {mssql_target_profile}/control.json -j $.spec.storage.logs.className={mssql_controller_logs_storage_class}')\n",
                "    run_command(f'azdata bdc config replace -c {mssql_target_profile}/control.json -j $.spec.storage.logs.size={mssql_controller_logs_size}Gi')\n",
                "    bdcPatch = {\n",
                "        \"patch\":[\n",
                "            {\n",
                "                \"op\": \"replace\",\n",
                "                \"path\": \"spec.resources.master.spec\",\n",
                "                \"value\": {\n",
                "                    \"type\": \"Master\",\n",
                "                    \"replicas\": mssql_sqlserver_scale,\n",
                "                    \"endpoints\": [\n",
                "                        {\n",
                "                            \"name\": \"Master\",\n",
                "                            \"serviceType\": \"LoadBalancer\",\n",
                "                            \"port\": mssql_sqlserver_port\n",
                "                        }\n",
                "                    ],\n",
                "                    \"settings\": {\n",
                "                        \"sql\": {\n",
                "                            \"hadr.enabled\": mssql_hadr_enabled\n",
                "                            }\n",
                "                        },\n",
                "                    \"storage\": {\n",
                "                    \"data\": {\n",
                "                        \"size\": f'{mssql_sql_data_size}Gi',\n",
                "                        \"className\": mssql_sql_data_storage_class,\n",
                "                        \"accessMode\": \"ReadWriteOnce\"\n",
                "                        },\n",
                "                    \"logs\": {\n",
                "                        \"size\": f'{mssql_sql_logs_size}Gi',\n",
                "                        \"className\": mssql_sql_logs_storage_class,\n",
                "                        \"accessMode\": \"ReadWriteOnce\"\n",
                "                        }\n",
                "                    }\n",
                "                }\n",
                "            }, {\n",
                "                \"op\": \"replace\",\n",
                "                \"path\": \"metadata.name\",\n",
                "                \"value\": mssql_cluster_name\n",
                "            }, {\n",
                "                \"op\": \"replace\",\n",
                "                \"path\": \"spec.resources.sparkhead.spec\",\n",
                "                \"value\": {\n",
                "                    \"replicas\": mssql_spark_scale\n",
                "                }\n",
                "            }, {\n",
                "                \"op\": \"replace\",\n",
                "                \"path\": \"spec.resources.compute-0.spec\",\n",
                "                \"value\": {\n",
                "                    \"replicas\": mssql_name_node_scale\n",
                "                }\n",
                "            }, {\n",
                "                \"op\": \"replace\",\n",
                "                \"path\": \"spec.resources.nmnode-0.spec\",\n",
                "                \"value\": {\n",
                "                    \"type\": \"Compute\",\n",
                "                    \"replicas\": mssql_compute_scale\n",
                "                }\n",
                "            }, {\n",
                "                \"op\": \"replace\",\n",
                "                \"path\": \"spec.resources.storage-0.spec\",\n",
                "                \"value\": {\n",
                "                    \"type\": \"Storage\",\n",
                "                    \"replicas\": mssql_hdfs_scale,\n",
                "                    \"settings\": {\n",
                "                        \"spark\": {\n",
                "                            \"includeSpark\": mssql_include_spark\n",
                "                        }\n",
                "                    },\n",
                "                    \"storage\": {\n",
                "                        \"data\": {\n",
                "                            \"size\": f'{mssql_hdfs_data_size}Gi',\n",
                "                            \"className\": mssql_hdfs_data_storage_class,\n",
                "                            \"accessMode\": \"ReadWriteOnce\"\n",
                "                        },\n",
                "                        \"logs\": {\n",
                "                            \"size\": f'{mssql_hdfs_logs_size}Gi',\n",
                "                            \"className\": mssql_hdfs_logs_storage_class,\n",
                "                            \"accessMode\": \"ReadWriteOnce\"\n",
                "                        }\n",
                "                    }\n",
                "                }\n",
                "            },{\n",
                "                \"op\": \"replace\",\n",
                "                \"path\": \"spec.resources.data-0.spec\",\n",
                "                \"value\": {\n",
                "                    \"type\": \"Data\",\n",
                "                    \"replicas\": mssql_data_scale,\n",
                "                    \"storage\": {\n",
                "                        \"data\": {\n",
                "                            \"size\": f'{mssql_datapool_data_size}Gi',\n",
                "                            \"className\": mssql_datapool_data_storage_class,\n",
                "                            \"accessMode\": \"ReadWriteOnce\"\n",
                "                        },\n",
                "                        \"logs\": {\n",
                "                            \"size\": f'{mssql_datapool_logs_size}Gi',\n",
                "                            \"className\": mssql_datapool_logs_storage_class,\n",
                "                            \"accessMode\": \"ReadWriteOnce\"\n",
                "                        }\n",
                "                    }\n",
                "                }\n",
                "            }\n",
                "        ]\n",
                "    }\n",
                "    if mssql_spark_scale > 0:\n",
                "        bdcPatch['patch'].append({\n",
                "            \"op\": \"add\",\n",
                "            \"path\": \"spec.resources.spark-0\",\n",
                "            \"value\": {\n",
                "                \"metadata\": {\n",
                "                    \"kind\": \"Pool\",\n",
                "                    \"name\": \"default\"\n",
                "                    },\n",
                "                \"spec\": {\n",
                "                    \"type\": \"Spark\",\n",
                "                    \"replicas\": mssql_spark_scale\n",
                "                    }\n",
                "                }\n",
                "            })\n",
                "        bdcPatch['patch'].append({\n",
                "            \"op\": \"add\",\n",
                "            \"path\": \"spec.services.spark.resources/-\",\n",
                "            \"value\": \"spark-0\"\n",
                "            })\n",
                "        bdcPatch['patch'].append({\n",
                "            \"op\": \"add\",\n",
                "            \"path\": \"spec.services.hdfs.resources/-\",\n",
                "            \"value\": \"spark-0\"\n",
                "            })    \n",
                "    if mssql_hadr_enabled:\n",
                "        bdcPatch['patch'][0]['value']['endpoints'].append({\n",
                "            \"name\": \"MasterSecondary\",\n",
                "            \"dnsName\": \"\",\n",
                "            \"serviceType\": \"NodePort\",\n",
                "            \"port\": int(mssql_readable_secondary_port)})\n",
                "    with open(f'{mssql_target_profile}/patch.json', \"w\") as write_file:\n",
                "        json.dump(bdcPatch, write_file)\n",
                "    run_command(f'azdata bdc config patch -c {mssql_target_profile}/bdc.json --patch-file {mssql_target_profile}/patch.json')\n",
                "else:\n",
                "    run_command(f'azdata bdc config replace -c {mssql_target_profile}/bdc.json -j metadata.name={mssql_cluster_name}')"
            ],
            "metadata": {
                "azdata_cell_guid": "2ff82c8a-4bce-449c-9d91-3ac7dd272021"
            },
            "outputs": [],
            "execution_count": 6
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Create SQL Server 2019 Big Data Cluster**"
            ],
            "metadata": {
                "azdata_cell_guid": "efe78cd3-ed73-4c9b-b586-fdd6c07dd37f"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "print (f'Creating SQL Server 2019 Big Data Cluster: {mssql_cluster_name} using configuration {mssql_target_profile}')\n",
                "os.environ[\"CONTROLLER_USERNAME\"] = mssql_controller_username\n",
                "os.environ[\"CONTROLLER_PASSWORD\"] = mssql_password\n",
                "os.environ[\"MSSQL_SA_PASSWORD\"] = mssql_password\n",
                "os.environ[\"KNOX_PASSWORD\"] = mssql_password\n",
                "run_command(f'azdata bdc create -c {mssql_target_profile}')"
            ],
            "metadata": {
                "azdata_cell_guid": "373947a1-90b9-49ee-86f4-17a4c7d4ca76"
            },
            "outputs": [],
            "execution_count": 7
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Login to SQL Server 2019 Big Data Cluster**"
            ],
            "metadata": {
                "azdata_cell_guid": "4e026d39-12d4-4c80-8e30-de2b782f2110"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "run_command(f'azdata login --cluster-name {mssql_cluster_name}')"
            ],
            "metadata": {
                "azdata_cell_guid": "79adda27-371d-4dcb-b867-db025f8162a5"
            },
            "outputs": [],
            "execution_count": 8
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Show SQL Server 2019 Big Data Cluster endpoints**"
            ],
            "metadata": {
                "azdata_cell_guid": "c1921288-ad11-40d8-9aea-127a722b3df8"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from IPython.display import *\n",
                "pandas.set_option('display.max_colwidth', -1)\n",
                "cmd = f'azdata bdc endpoint list'\n",
                "cmdOutput = !{cmd}\n",
                "endpoints = json.loads(''.join(cmdOutput))\n",
                "endpointsDataFrame = pandas.DataFrame(endpoints)\n",
                "endpointsDataFrame.columns = [' '.join(word[0].upper() + word[1:] for word in columnName.split()) for columnName in endpoints[0].keys()]\n",
                "display(HTML(endpointsDataFrame.to_html(index=False, render_links=True)))"
            ],
            "metadata": {
                "azdata_cell_guid": "a2202494-fd6c-4534-987d-15c403a5096f"
            },
            "outputs": [],
            "execution_count": 9
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Connect to SQL Server Master instance in Azure Data Studio**\r\n",
                "Click the link below to connect to the SQL Server Master instance of the SQL Server 2019 Big Data Cluster."
            ],
            "metadata": {
                "azdata_cell_guid": "621863a2-aa61-46f4-a9d0-717f41c009ee"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "sqlEndpoints = [x for x in endpoints if x['name'] == 'sql-server-master']\r\n",
                "if sqlEndpoints and len(sqlEndpoints) == 1:\r\n",
                "    connectionParameter = '{\"serverName\":\"' + sqlEndpoints[0]['endpoint'] + '\",\"providerName\":\"MSSQL\",\"authenticationType\":\"SqlLogin\",\"userName\":\"sa\",\"password\":' + json.dumps(mssql_password) + '}'\r\n",
                "    display(HTML('<br/><a href=\"command:azdata.connect?' + html.escape(connectionParameter)+'\"><font size=\"3\">Click here to connect to SQL Server Master instance</font></a><br/>'))\r\n",
                "else:\r\n",
                "    sys.exit('Could not find the SQL Server Master instance endpoint')"
            ],
            "metadata": {
                "azdata_cell_guid": "48342355-9d2b-4fa6-b1aa-3bc77d434dfa"
            },
            "outputs": [],
            "execution_count": 10
        }
    ]
}