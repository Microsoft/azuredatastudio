{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python",
            "version": "3.7.3",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "![Microsoft](https://raw.githubusercontent.com/microsoft/azuredatastudio/master/src/sql/media/microsoft-small-logo.png)\n",
                " \n",
                "## Create Azure Kubernetes Service cluster and deploy SQL Server 2019 Big Data Cluster\n",
                " \n",
                "This notebook walks through the process of creating a new Azure Kubernetes Service cluster first, and then deploys a <a href=\"https://docs.microsoft.com/sql/big-data-cluster/big-data-cluster-overview?view=sqlallproducts-allversions\">SQL Server 2019 Big Data Cluster</a> on the newly created AKS cluster.\n",
                " \n",
                "* Follow the instructions in the **Prerequisites** cell to install the tools if not already installed.\n",
                "* The **Required information** cell will prompt you for a password that will be used to access the cluster controller, SQL Server, and Knox.\n",
                "* The values in the **Azure settings** and **Default settings** cell can be changed as appropriate.\n",
                "\n",
                "<span style=\"color:red\"><font size=\"3\">Please press the \"Run Cells\" button to run the notebook</font></span>"
            ],
            "metadata": {
                "azdata_cell_guid": "4f6bc3bc-3592-420a-b534-384011189005"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Prerequisites**\n",
                "Ensure the following tools are installed and added to PATH before proceeding.\n",
                "\n",
                "|Tools|Description|Installation|\n",
                "|---|---|---|\n",
                "|Azure CLI |Command-line tool for managing Azure services. Used to create AKS cluster | [Installation](https://docs.microsoft.com/cli/azure/install-azure-cli?view=azure-cli-latest) |\n",
                "|kubectl | Command-line tool for monitoring the underlying Kuberentes cluster | [Installation](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-binary-using-native-package-management) |\n",
                "|azdata | Command-line tool for installing and managing a Big Data Cluster |[Installation](https://docs.microsoft.com/en-us/sql/big-data-cluster/deploy-install-azdata?view=sqlallproducts-allversions) |"
            ],
            "metadata": {
                "azdata_cell_guid": "d949980e-ad3f-4d02-ae84-7e4fbb19a087"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Check dependencies**"
            ],
            "metadata": {
                "azdata_cell_guid": "a56d3413-a730-4997-b5c2-c8abd972757e"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import pandas,sys,os,json,html,getpass,time\r\n",
                "pandas_version = pandas.__version__.split('.')\r\n",
                "pandas_major = int(pandas_version[0])\r\n",
                "pandas_minor = int(pandas_version[1])\r\n",
                "pandas_patch = int(pandas_version[2])\r\n",
                "if not (pandas_major > 0 or (pandas_major == 0 and pandas_minor > 24) or (pandas_major == 0 and pandas_minor == 24 and pandas_patch >= 2)):\r\n",
                "    sys.exit('Please upgrade the Notebook dependency before you can proceed, you can do it by running the \"Reinstall Notebook dependencies\" command in command palette (View menu -> Command Paletteâ€¦).')\r\n",
                "def run_command(command):\r\n",
                "    print(\"Executing: \" + command)\r\n",
                "    !{command}\r\n",
                "    if _exit_code != 0:\r\n",
                "        sys.exit(f'Command execution failed with exit code: {str(_exit_code)}.\\n\\t{command}\\n')\r\n",
                "    print(f'Successfully executed: {command}')\r\n",
                "\r\n",
                "run_command('kubectl version --client=true')\r\n",
                "run_command('azdata --version')\r\n",
                "run_command('az --version')"
            ],
            "metadata": {
                "azdata_cell_guid": "326645cf-022a-47f2-8aff-37de71da8955"
            },
            "outputs": [],
            "execution_count": 1
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Required information**"
            ],
            "metadata": {
                "azdata_cell_guid": "720c200c-322a-49dd-9aa3-8bf7946aa251"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "invoked_by_wizard = \"AZDATA_NB_VAR_BDC_ADMIN_PASSWORD\" in os.environ\n",
                "if not invoked_by_wizard:\n",
                "    mssql_password = getpass.getpass(prompt = 'SQL Server 2019 Big Data Cluster controller password')\n",
                "    if mssql_password == \"\":\n",
                "        sys.exit(f'Password is required.')\n",
                "    confirm_password = getpass.getpass(prompt = 'Confirm password')\n",
                "    if mssql_password != confirm_password:\n",
                "        sys.exit(f'Passwords do not match.')\n",
                "\n",
                "print('You can also use the controller password to access Knox and SQL Server.')"
            ],
            "metadata": {
                "azdata_cell_guid": "17e5d087-7128-4d02-8c16-fe1ddee675e5"
            },
            "outputs": [],
            "execution_count": 2
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Azure settings**\n",
                "*Subscription ID*: visit <a href=\"https://portal.azure.com/#blade/Microsoft_Azure_Billing/SubscriptionsBlade\">here</a> to find out the subscriptions you can use, if you leave it unspecified, the default subscription will be used.\n",
                "\n",
                "*VM Size*: visit <a href=\"https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes\">here</a> to find out the available VM sizes you could use. \n",
                " \n",
                "*Region*: visit <a href=\"https://azure.microsoft.com/en-us/global-infrastructure/services/?products=kubernetes-service\">here</a> to find out the Azure regions where the Azure Kubernettes Service is available."
            ],
            "metadata": {
                "azdata_cell_guid": "ce8e932c-b196-4d4a-85b5-521327b2f67d"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "if not invoked_by_wizard:\n",
                "    azure_subscription_id = \"\"\n",
                "    azure_vm_size = \"Standard_E4s_v3\"\n",
                "    azure_region = \"eastus\"\n",
                "    azure_vm_count = int(5)\n",
                "print('Azure settings cell executed successfully')"
            ],
            "metadata": {
                "azdata_cell_guid": "94ecb550-bb2a-4122-9d8d-f10729339f57"
            },
            "outputs": [],
            "execution_count": 3
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Default settings**"
            ],
            "metadata": {
                "azdata_cell_guid": "4945bace-a50a-4e58-b55c-e9736106f805"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "if invoked_by_wizard:\n",
                "    mssql_cluster_name = os.environ[\"AZDATA_NB_VAR_BDC_CLUSTER_NAME\"]\n",
                "    mssql_controller_username = os.environ[\"AZDATA_NB_VAR_BDC_CONTROLLER_USERNAME\"]\n",
                "    mssql_password = os.environ[\"AZDATA_NB_VAR_BDC_ADMIN_PASSWORD\"]\n",
                "    azure_subscription_id = os.environ[\"AZDATA_NB_VAR_BDC_AZURE_SUBSCRIPTION\"]\n",
                "    azure_vm_size = os.environ[\"AZDATA_NB_VAR_BDC_AZURE_VM_SIZE\"]\n",
                "    azure_region = os.environ[\"AZDATA_NB_VAR_BDC_AZURE_REGION\"]\n",
                "    azure_vm_count = int(os.environ[\"AZDATA_NB_VAR_BDC_VM_COUNT\"])\n",
                "    azure_resource_group = os.environ[\"AZDATA_NB_VAR_BDC_RESOURCEGROUP_NAME\"]\n",
                "    aks_cluster_name = os.environ[\"AZDATA_NB_VAR_BDC_AKS_NAME\"]\n",
                "    mssql_source_profile = os.environ[\"AZDATA_NB_VAR_BDC_DEPLOYMENT_PROFILE\"]\n",
                "    mssql_sqlserver_scale = int(os.environ[\"AZDATA_NB_VAR_BDC_SQLSERVER_SCALE\"])\n",
                "    mssql_compute_scale = int(os.environ[\"AZDATA_NB_VAR_BDC_COMPUTEPOOL_SCALE\"])\n",
                "    mssql_data_scale = int(os.environ[\"AZDATA_NB_VAR_BDC_DATAPOOL_SCALE\"])\n",
                "    mssql_hdfs_scale = int(os.environ[\"AZDATA_NB_VAR_BDC_HDFSPOOL_SCALE\"])\n",
                "    mssql_spark_scale = int(os.environ[\"AZDATA_NB_VAR_BDC_SPARKPOOL_SCALE\"])\n",
                "    mssql_name_node_scale = int(os.environ[\"AZDATA_NB_VAR_BDC_NAMENODE_SCALE\"])\n",
                "    mssql_include_spark = os.environ[\"AZDATA_NB_VAR_BDC_INCLUDESPARK\"] == \"true\"\n",
                "    mssql_controller_port = int(os.environ[\"AZDATA_NB_VAR_BDC_CONTROLLER_PORT\"])\n",
                "    mssql_sqlserver_port = int(os.environ[\"AZDATA_NB_VAR_BDC_SQL_PORT\"])\n",
                "    mssql_gateway_port = int(os.environ[\"AZDATA_NB_VAR_BDC_GATEWAY_PORT\"])\n",
                "    mssql_readable_secondary_port = os.environ[\"AZDATA_NB_VAR_BDC_READABLE_SECONDARY_PORT\"]\n",
                "    mssql_controller_data_storage_class = os.environ[\"AZDATA_NB_VAR_BDC_CONTROLLER_DATA_STORAGE_CLASS\"]\n",
                "    mssql_controller_data_size = int(os.environ[\"AZDATA_NB_VAR_BDC_CONTROLLER_DATA_STORAGE_SIZE\"])\n",
                "    mssql_controller_logs_storage_class = os.environ[\"AZDATA_NB_VAR_BDC_CONTROLLER_LOGS_STORAGE_CLASS\"]\n",
                "    mssql_controller_logs_size = int(os.environ[\"AZDATA_NB_VAR_BDC_CONTROLLER_LOGS_STORAGE_SIZE\"])\n",
                "    mssql_datapool_data_storage_class = os.environ[\"AZDATA_NB_VAR_BDC_DATA_DATA_STORAGE_CLASS\"]\n",
                "    mssql_datapool_data_size = int(os.environ[\"AZDATA_NB_VAR_BDC_DATA_DATA_STORAGE_SIZE\"])\n",
                "    mssql_datapool_logs_storage_class = os.environ[\"AZDATA_NB_VAR_BDC_DATA_LOGS_STORAGE_CLASS\"]\n",
                "    mssql_datapool_logs_size = int(os.environ[\"AZDATA_NB_VAR_BDC_DATA_LOGS_STORAGE_SIZE\"])\n",
                "    mssql_hdfs_data_storage_class = os.environ[\"AZDATA_NB_VAR_BDC_HDFS_DATA_STORAGE_CLASS\"]\n",
                "    mssql_hdfs_data_size = int(os.environ[\"AZDATA_NB_VAR_BDC_HDFS_DATA_STORAGE_SIZE\"])\n",
                "    mssql_hdfs_logs_storage_class = os.environ[\"AZDATA_NB_VAR_BDC_HDFS_LOGS_STORAGE_CLASS\"]\n",
                "    mssql_hdfs_logs_size = int(os.environ[\"AZDATA_NB_VAR_BDC_HDFS_LOGS_STORAGE_SIZE\"])\n",
                "    mssql_sql_data_storage_class = os.environ[\"AZDATA_NB_VAR_BDC_SQL_DATA_STORAGE_CLASS\"]\n",
                "    mssql_sql_data_size = int(os.environ[\"AZDATA_NB_VAR_BDC_SQL_DATA_STORAGE_SIZE\"])\n",
                "    mssql_sql_logs_storage_class = os.environ[\"AZDATA_NB_VAR_BDC_SQL_LOGS_STORAGE_CLASS\"]\n",
                "    mssql_sql_logs_size = int(os.environ[\"AZDATA_NB_VAR_BDC_SQL_LOGS_STORAGE_SIZE\"])\n",
                "    mssql_hadr_enabled = os.environ[\"AZDATA_NB_VAR_BDC_ENABLE_HADR\"] == \"true\"\n",
                "else:\n",
                "    mssql_cluster_name = 'mssql-cluster'\n",
                "    mssql_controller_username = 'admin'\n",
                "    azure_resource_group = mssql_cluster_name + '-' + time.strftime(\"%Y%m%d%H%M%S\", time.localtime())\n",
                "    aks_cluster_name = azure_resource_group\n",
                "    mssql_source_profile = 'aks-dev-test'\n",
                "mssql_target_profile = 'ads-bdc-custom-profile'\n",
                "print(f'Azure subscription: {azure_subscription_id}')\n",
                "print(f'Azure VM size: {azure_vm_size}')\n",
                "print(f'Azure VM count: {azure_vm_count}')\n",
                "print(f'Azure region: {azure_region}')\n",
                "print(f'Azure resource group: {azure_resource_group}')\n",
                "print(f'AKS cluster name: {aks_cluster_name}')\n",
                "print(f'SQL Server Big Data Cluster name: {mssql_cluster_name}')\n",
                "print(f'SQL Server Big Data Cluster controller username: {mssql_controller_username}')\n",
                "print(f'Deployment source profile: {mssql_source_profile}')\n",
                "print(f'Deployment profile: {mssql_target_profile}')\n",
                "if invoked_by_wizard:\n",
                "    print(f'SQL Server Master scale: {mssql_sqlserver_scale}')\n",
                "    print(f'Compute pool scale: {mssql_compute_scale}')\n",
                "    print(f'HDFS pool scale: {mssql_hdfs_scale}')\n",
                "    print(f'Include Spark in HDFS pool: {mssql_include_spark}')\n",
                "    print(f'Data pool scale: {mssql_data_scale}')\n",
                "    print(f'Spark pool scale: {mssql_spark_scale}')\n",
                "    print(f'HDFS name node scale: {mssql_name_node_scale}')\n",
                "    print(f'Controller port: {mssql_controller_port}')\n",
                "    print(f'SQL Server port: {mssql_sqlserver_port}')\n",
                "    print(f'Gateway port: {mssql_gateway_port}')\n",
                "    if mssql_readable_secondary_port != '':\n",
                "        print(f'Readable secondary port: {mssql_readable_secondary_port}')\n",
                "    print(f'Controller data storage class name: {mssql_controller_data_storage_class}')\n",
                "    print(f'Controller logs storage class name: {mssql_controller_logs_storage_class}')\n",
                "    print(f'Controller data storage size(GB): {mssql_controller_data_size}')\n",
                "    print(f'Controller logs storage size(GB): {mssql_controller_logs_size}')\n",
                "    print(f'Data pool data storage class name: {mssql_datapool_data_storage_class}')\n",
                "    print(f'Data pool logs storage class name: {mssql_datapool_logs_storage_class}')\n",
                "    print(f'Data pool data storage size(GB): {mssql_datapool_data_size}')\n",
                "    print(f'Data pool logs storage size(GB): {mssql_datapool_logs_size}')\n",
                "    print(f'HDFS data storage class name: {mssql_hdfs_data_storage_class}')\n",
                "    print(f'HDFS logs storage class name: {mssql_hdfs_logs_storage_class}')\n",
                "    print(f'HDFS data storage size(GB): {mssql_hdfs_data_size}')\n",
                "    print(f'HDFS logs storage size(GB): {mssql_hdfs_logs_size}')\n",
                "    print(f'SQL Server Master data storage class name: {mssql_sql_data_storage_class}')\n",
                "    print(f'SQL Server Master logs storage class name: {mssql_sql_logs_storage_class}')\n",
                "    print(f'SQL Server Master data storage size(GB): {mssql_sql_data_size}')\n",
                "    print(f'SQL Server Master logs storage size(GB): {mssql_sql_logs_size}')"
            ],
            "metadata": {
                "azdata_cell_guid": "0c3c064a-0e4c-4a58-b9ce-6bed862e8246"
            },
            "outputs": [],
            "execution_count": 4
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Login to Azure**\n",
                "\n",
                "This will open a web browser window to enable credentials to be entered. If this cells is hanging forever, it might be because your Web browser windows is waiting for you to enter your Azure credentials!\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "baddf2d9-93ee-4c42-aaf1-b42116bb1912"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "run_command(f'az login')"
            ],
            "metadata": {
                "azdata_cell_guid": "8f1404a6-216d-49fb-b6ad-81beeea50083"
            },
            "outputs": [],
            "execution_count": 5
        },
        {
            "cell_type": "markdown",
            "source": [
                "\n",
                "### **Set active Azure subscription**"
            ],
            "metadata": {
                "azdata_cell_guid": "230dc0f1-bf6e-474a-bfaa-aae6f8aad12e"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "if azure_subscription_id != \"\":\n",
                "    run_command(f'az account set --subscription {azure_subscription_id}')\n",
                "else:\n",
                "    print('Using the default Azure subscription', {azure_subscription_id})\n",
                "run_command(f'az account show')"
            ],
            "metadata": {
                "azdata_cell_guid": "ab230931-2e99-483b-a229-3847684a8c1c"
            },
            "outputs": [],
            "execution_count": 6
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Create Azure resource group**"
            ],
            "metadata": {
                "azdata_cell_guid": "d51db914-f484-489f-990d-72edb3065068"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "run_command(f'az group create --name {azure_resource_group} --location {azure_region}')"
            ],
            "metadata": {
                "azdata_cell_guid": "7c53eb23-c327-41bf-8936-bd34a02ebdd5"
            },
            "outputs": [],
            "execution_count": 7
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Create AKS cluster**"
            ],
            "metadata": {
                "azdata_cell_guid": "818eb705-71e2-4013-8420-44886a5468b2"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "run_command(f'az aks create --name {aks_cluster_name} --resource-group {azure_resource_group} --generate-ssh-keys --node-vm-size {azure_vm_size} --node-count {azure_vm_count}')"
            ],
            "metadata": {
                "azdata_cell_guid": "3cea1da0-0c18-4030-a5aa-79bc98a5a14d"
            },
            "outputs": [],
            "execution_count": 8
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Set the new AKS cluster as current context**"
            ],
            "metadata": {
                "azdata_cell_guid": "5ade8453-5e71-478f-b6b6-83c55626243d"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "run_command(f'az aks get-credentials --resource-group {azure_resource_group} --name {aks_cluster_name} --admin --overwrite-existing')"
            ],
            "metadata": {
                "azdata_cell_guid": "9ccb9adf-1cf6-4dcb-8bd9-7ae9a85c2437"
            },
            "outputs": [],
            "execution_count": 9
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Create a deployment configuration file**"
            ],
            "metadata": {
                "azdata_cell_guid": "57eb69fb-c68f-4ba8-818d-ffbaa0bc7aec"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "os.environ[\"ACCEPT_EULA\"] = 'yes'\n",
                "run_command(f'azdata bdc config init --source {mssql_source_profile} --target {mssql_target_profile} --force')\n",
                "if invoked_by_wizard:\n",
                "    run_command(f'azdata bdc config replace -c {mssql_target_profile}/bdc.json -j \"$.spec.resources.gateway.spec.endpoints[?(@.name==\"\"Knox\"\")].port={mssql_gateway_port}\"')\n",
                "    run_command(f'azdata bdc config replace -c {mssql_target_profile}/control.json -j \"$.spec.endpoints[?(@.name==\"\"Controller\"\")].port={mssql_controller_port}\"')\n",
                "    run_command(f'azdata bdc config replace -c {mssql_target_profile}/control.json -j $.spec.storage.data.className={mssql_controller_data_storage_class}')\n",
                "    run_command(f'azdata bdc config replace -c {mssql_target_profile}/control.json -j $.spec.storage.data.size={mssql_controller_data_size}Gi')\n",
                "    run_command(f'azdata bdc config replace -c {mssql_target_profile}/control.json -j $.spec.storage.logs.className={mssql_controller_logs_storage_class}')\n",
                "    run_command(f'azdata bdc config replace -c {mssql_target_profile}/control.json -j $.spec.storage.logs.size={mssql_controller_logs_size}Gi')\n",
                "    bdcPatch = {\n",
                "        \"patch\":[\n",
                "            {\n",
                "                \"op\": \"replace\",\n",
                "                \"path\": \"spec.resources.master.spec\",\n",
                "                \"value\": {\n",
                "                    \"type\": \"Master\",\n",
                "                    \"replicas\": mssql_sqlserver_scale,\n",
                "                    \"endpoints\": [\n",
                "                        {\n",
                "                            \"name\": \"Master\",\n",
                "                            \"serviceType\": \"LoadBalancer\",\n",
                "                            \"port\": mssql_sqlserver_port\n",
                "                        }\n",
                "                    ],\n",
                "                    \"settings\": {\n",
                "                        \"sql\": {\n",
                "                            \"hadr.enabled\": mssql_hadr_enabled\n",
                "                            }\n",
                "                        },\n",
                "                    \"storage\": {\n",
                "                    \"data\": {\n",
                "                        \"size\": f'{mssql_sql_data_size}Gi',\n",
                "                        \"className\": mssql_sql_data_storage_class,\n",
                "                        \"accessMode\": \"ReadWriteOnce\"\n",
                "                        },\n",
                "                    \"logs\": {\n",
                "                        \"size\": f'{mssql_sql_logs_size}Gi',\n",
                "                        \"className\": mssql_sql_logs_storage_class,\n",
                "                        \"accessMode\": \"ReadWriteOnce\"\n",
                "                        }\n",
                "                    }\n",
                "                }\n",
                "            }, {\n",
                "                \"op\": \"replace\",\n",
                "                \"path\": \"metadata.name\",\n",
                "                \"value\": mssql_cluster_name\n",
                "            }, {\n",
                "                \"op\": \"replace\",\n",
                "                \"path\": \"spec.resources.sparkhead.spec\",\n",
                "                \"value\": {\n",
                "                    \"replicas\": mssql_spark_scale\n",
                "                }\n",
                "            }, {\n",
                "                \"op\": \"replace\",\n",
                "                \"path\": \"spec.resources.compute-0.spec\",\n",
                "                \"value\": {\n",
                "                    \"replicas\": mssql_name_node_scale\n",
                "                }\n",
                "            }, {\n",
                "                \"op\": \"replace\",\n",
                "                \"path\": \"spec.resources.nmnode-0.spec\",\n",
                "                \"value\": {\n",
                "                    \"type\": \"Compute\",\n",
                "                    \"replicas\": mssql_compute_scale\n",
                "                }\n",
                "            }, {\n",
                "                \"op\": \"replace\",\n",
                "                \"path\": \"spec.resources.storage-0.spec\",\n",
                "                \"value\": {\n",
                "                    \"type\": \"Storage\",\n",
                "                    \"replicas\": mssql_hdfs_scale,\n",
                "                    \"settings\": {\n",
                "                        \"spark\": {\n",
                "                            \"includeSpark\": mssql_include_spark\n",
                "                        }\n",
                "                    },\n",
                "                    \"storage\": {\n",
                "                        \"data\": {\n",
                "                            \"size\": f'{mssql_hdfs_data_size}Gi',\n",
                "                            \"className\": mssql_hdfs_data_storage_class,\n",
                "                            \"accessMode\": \"ReadWriteOnce\"\n",
                "                        },\n",
                "                        \"logs\": {\n",
                "                            \"size\": f'{mssql_hdfs_logs_size}Gi',\n",
                "                            \"className\": mssql_hdfs_logs_storage_class,\n",
                "                            \"accessMode\": \"ReadWriteOnce\"\n",
                "                        }\n",
                "                    }\n",
                "                }\n",
                "            },{\n",
                "                \"op\": \"replace\",\n",
                "                \"path\": \"spec.resources.data-0.spec\",\n",
                "                \"value\": {\n",
                "                    \"type\": \"Data\",\n",
                "                    \"replicas\": mssql_data_scale,\n",
                "                    \"storage\": {\n",
                "                        \"data\": {\n",
                "                            \"size\": f'{mssql_datapool_data_size}Gi',\n",
                "                            \"className\": mssql_datapool_data_storage_class,\n",
                "                            \"accessMode\": \"ReadWriteOnce\"\n",
                "                        },\n",
                "                        \"logs\": {\n",
                "                            \"size\": f'{mssql_datapool_logs_size}Gi',\n",
                "                            \"className\": mssql_datapool_logs_storage_class,\n",
                "                            \"accessMode\": \"ReadWriteOnce\"\n",
                "                        }\n",
                "                    }\n",
                "                }\n",
                "            }\n",
                "        ]\n",
                "    }\n",
                "    if mssql_spark_scale > 0:\n",
                "        bdcPatch['patch'].append({\n",
                "            \"op\": \"add\",\n",
                "            \"path\": \"spec.resources.spark-0\",\n",
                "            \"value\": {\n",
                "                \"metadata\": {\n",
                "                    \"kind\": \"Pool\",\n",
                "                    \"name\": \"default\"\n",
                "                    },\n",
                "                \"spec\": {\n",
                "                    \"type\": \"Spark\",\n",
                "                    \"replicas\": mssql_spark_scale\n",
                "                    }\n",
                "                }\n",
                "            })\n",
                "        bdcPatch['patch'].append({\n",
                "            \"op\": \"add\",\n",
                "            \"path\": \"spec.services.spark.resources/-\",\n",
                "            \"value\": \"spark-0\"\n",
                "            })\n",
                "        bdcPatch['patch'].append({\n",
                "            \"op\": \"add\",\n",
                "            \"path\": \"spec.services.hdfs.resources/-\",\n",
                "            \"value\": \"spark-0\"\n",
                "            })    \n",
                "    if mssql_hadr_enabled:\n",
                "        bdcPatch['patch'][0]['value']['endpoints'].append({\n",
                "            \"name\": \"MasterSecondary\",\n",
                "            \"dnsName\": \"\",\n",
                "            \"serviceType\": \"NodePort\",\n",
                "            \"port\": int(mssql_readable_secondary_port)})\n",
                "    with open(f'{mssql_target_profile}/patch.json', \"w\") as write_file:\n",
                "        json.dump(bdcPatch, write_file)\n",
                "    run_command(f'azdata bdc config patch -c {mssql_target_profile}/bdc.json --patch-file {mssql_target_profile}/patch.json')\n",
                "else:\n",
                "    run_command(f'azdata bdc config replace -c {mssql_target_profile}/bdc.json -j metadata.name={mssql_cluster_name}')"
            ],
            "metadata": {
                "azdata_cell_guid": "3fd73c04-8a79-4d08-9049-1dad30265558"
            },
            "outputs": [],
            "execution_count": 10
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Create SQL Server 2019 Big Data Cluster**"
            ],
            "metadata": {
                "azdata_cell_guid": "6e82fad8-0fd0-4952-87ce-3fea1edd98cb"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "print (f'Creating SQL Server 2019 Big Data Cluster: {mssql_cluster_name} using configuration {mssql_target_profile}')\n",
                "os.environ[\"CONTROLLER_USERNAME\"] = mssql_controller_username\n",
                "os.environ[\"CONTROLLER_PASSWORD\"] = mssql_password\n",
                "os.environ[\"MSSQL_SA_PASSWORD\"] = mssql_password\n",
                "os.environ[\"KNOX_PASSWORD\"] = mssql_password\n",
                "run_command(f'azdata bdc create -c {mssql_target_profile}')"
            ],
            "metadata": {
                "azdata_cell_guid": "c43ea026-ca5e-4e2a-8602-fcc786354168"
            },
            "outputs": [],
            "execution_count": 11
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Login to SQL Server 2019 Big Data Cluster**"
            ],
            "metadata": {
                "azdata_cell_guid": "9c5428f4-08b9-4799-a35d-867c91dc29fb"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "run_command(f'azdata login --cluster-name {mssql_cluster_name}')"
            ],
            "metadata": {
                "azdata_cell_guid": "5120c387-1088-435b-856e-e59f147c45a2"
            },
            "outputs": [],
            "execution_count": 12
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Show SQL Server 2019 Big Data Cluster endpoints**"
            ],
            "metadata": {
                "azdata_cell_guid": "97974eda-e108-4c21-a58e-c6bb58f14ef1"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from IPython.display import *\n",
                "pandas.set_option('display.max_colwidth', -1)\n",
                "cmd = f'azdata bdc endpoint list'\n",
                "cmdOutput = !{cmd}\n",
                "endpoints = json.loads(''.join(cmdOutput))\n",
                "endpointsDataFrame = pandas.DataFrame(endpoints)\n",
                "endpointsDataFrame.columns = [' '.join(word[0].upper() + word[1:] for word in columnName.split()) for columnName in endpoints[0].keys()]\n",
                "display(HTML(endpointsDataFrame.to_html(index=False, render_links=True)))"
            ],
            "metadata": {
                "azdata_cell_guid": "9a5d0aef-a8da-4845-b470-d714435f0304"
            },
            "outputs": [],
            "execution_count": 13
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Connect to SQL Server Master instance in Azure Data Studio**\r\n",
                "Click the link below to connect to the SQL Server Master instance of the SQL Server 2019 Big Data Cluster."
            ],
            "metadata": {
                "azdata_cell_guid": "4a49b629-bd7a-43ba-bf18-6cdc0737b0f9"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "sqlEndpoints = [x for x in endpoints if x['name'] == 'sql-server-master']\r\n",
                "if sqlEndpoints and len(sqlEndpoints) == 1:\r\n",
                "    connectionParameter = '{\"serverName\":\"' + sqlEndpoints[0]['endpoint'] + '\",\"providerName\":\"MSSQL\",\"authenticationType\":\"SqlLogin\",\"userName\":\"sa\",\"password\":' + json.dumps(mssql_password) + '}'\r\n",
                "    display(HTML('<br/><a href=\"command:azdata.connect?' + html.escape(connectionParameter)+'\"><font size=\"3\">Click here to connect to SQL Server Master instance</font></a><br/>'))\r\n",
                "else:\r\n",
                "    sys.exit('Could not find the SQL Server Master instance endpoint.')"
            ],
            "metadata": {
                "azdata_cell_guid": "1c9d1f2c-62ba-4070-920a-d30b67bdcc7c"
            },
            "outputs": [],
            "execution_count": 14
        }
    ]
}